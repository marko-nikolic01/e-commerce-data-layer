Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:35:02 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:35:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:35:02 INFO ResourceUtils: ==============================================================
25/03/30 12:35:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:35:02 INFO ResourceUtils: ==============================================================
25/03/30 12:35:02 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:35:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:35:02 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:35:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:35:02 INFO SecurityManager: Changing view acls to: root
25/03/30 12:35:02 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:35:02 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:35:02 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:35:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:35:02 INFO Utils: Successfully started service 'sparkDriver' on port 35759.
25/03/30 12:35:02 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:35:02 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:35:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:35:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:35:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:35:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83f65d9f-9b65-450a-bc0b-e9a29a2ebdd7
25/03/30 12:35:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:35:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:35:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:35:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:35:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 27 ms (0 ms spent in bootstraps)
25/03/30 12:35:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330123503-0003
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123503-0003/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:35:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123503-0003/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123503-0003/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:35:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123503-0003/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123503-0003/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:35:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123503-0003/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:35:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45753.
25/03/30 12:35:03 INFO NettyBlockTransferService: Server created on a339e4692fd5:45753
25/03/30 12:35:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:35:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 45753, None)
25/03/30 12:35:03 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:45753 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 45753, None)
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123503-0003/2 is now RUNNING
25/03/30 12:35:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 45753, None)
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123503-0003/1 is now RUNNING
25/03/30 12:35:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 45753, None)
25/03/30 12:35:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123503-0003/0 is now RUNNING
25/03/30 12:35:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:35:04 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:35:04 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:35:05 INFO InMemoryFileIndex: It took 87 ms to list leaf files for 1 paths.
25/03/30 12:35:05 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 4 paths.
25/03/30 12:35:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:47196) with ID 2,  ResourceProfileId 0
25/03/30 12:35:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:49260) with ID 0,  ResourceProfileId 0
25/03/30 12:35:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:47558) with ID 1,  ResourceProfileId 0
25/03/30 12:35:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:34455 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 34455, None)
25/03/30 12:35:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:46753 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 46753, None)
25/03/30 12:35:06 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:42631 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 42631, None)
25/03/30 12:35:07 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:35:07 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:35:07 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:35:08 INFO CodeGenerator: Code generated in 163.421088 ms
25/03/30 12:35:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:35:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:35:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:45753 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:35:08 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:35:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:35:08 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:35:08 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:35:08 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:35:08 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:35:08 INFO DAGScheduler: Missing parents: List()
25/03/30 12:35:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:35:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:35:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:35:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:45753 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:35:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:35:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:35:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:35:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 0, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:35:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:46753 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:35:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:46753 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:35:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1703 ms on 172.18.0.7 (executor 0) (1/1)
25/03/30 12:35:10 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:35:10 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.783 s
25/03/30 12:35:10 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:35:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:35:10 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.818836 s
25/03/30 12:35:10 INFO CodeGenerator: Code generated in 11.163725 ms
25/03/30 12:35:10 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:35:10 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:35:10 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:35:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:35:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:35:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:45753 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:35:10 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:35:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:35:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:35:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:35:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:35:10 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:35:10 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:35:10 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:35:10 INFO metastore: Connected to metastore.
25/03/30 12:35:11 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3c30fdd2-d388-4d95-9d92-babb7a975c4e, clientType=HIVECLI]
25/03/30 12:35:11 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:35:11 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:35:11 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:35:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:35:11 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:35:11 INFO metastore: Connected to metastore.
25/03/30 12:35:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:35:11 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:35:11 INFO metastore: Connected to metastore.
25/03/30 12:35:11 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 1 paths.
25/03/30 12:35:11 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:35:11 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:35:11 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:35:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:35:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:35:11 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:35:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:35:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:35:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:35:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:35:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:35:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:35:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:35:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:45753 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:35:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.7:46753 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:35:11 INFO CodeGenerator: Code generated in 21.801975 ms
25/03/30 12:35:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:35:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 365.2 MiB)
25/03/30 12:35:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:45753 (size: 33.6 KiB, free: 366.2 MiB)
25/03/30 12:35:11 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:35:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4544447 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:35:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:35:11 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 7 output partitions
25/03/30 12:35:11 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:35:11 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:35:11 INFO DAGScheduler: Missing parents: List()
25/03/30 12:35:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:35:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:35:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:35:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:45753 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:35:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:35:11 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
25/03/30 12:35:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
25/03/30 12:35:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 1, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.4, executor 2, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.7, executor 0, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.9, executor 1, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.4, executor 2, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.7, executor 0, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.9, executor 1, partition 6, ANY, 4947 bytes) taskResourceAssignments Map()
25/03/30 12:35:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:46753 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:35:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:46753 (size: 33.6 KiB, free: 366.2 MiB)
25/03/30 12:35:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:34455 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:35:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:42631 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:35:12 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 843 ms on 172.18.0.7 (executor 0) (1/7)
25/03/30 12:35:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 844 ms on 172.18.0.7 (executor 0) (2/7)
25/03/30 12:35:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:34455 (size: 33.6 KiB, free: 366.3 MiB)
25/03/30 12:35:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:42631 (size: 33.6 KiB, free: 366.3 MiB)
25/03/30 12:35:14 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2538 ms on 172.18.0.4 (executor 2) (3/7)
25/03/30 12:35:14 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2540 ms on 172.18.0.4 (executor 2) (4/7)
25/03/30 12:35:14 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 2578 ms on 172.18.0.9 (executor 1) (5/7)
25/03/30 12:35:14 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2579 ms on 172.18.0.9 (executor 1) (6/7)
25/03/30 12:35:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2818 ms on 172.18.0.9 (executor 1) (7/7)
25/03/30 12:35:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:35:14 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.823 s
25/03/30 12:35:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:35:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:35:14 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.827349 s
25/03/30 12:35:14 INFO CodeGenerator: Code generated in 6.812227 ms
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:45753 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:35:14 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:35:14 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:35:14 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:35:14 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:35:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:35:14 INFO CodeGenerator: Code generated in 29.878369 ms
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:45753 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:35:14 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:35:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:35:14 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:35:14 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:35:14 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:35:14 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:35:14 INFO DAGScheduler: Missing parents: List()
25/03/30 12:35:14 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:35:14 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:45753 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:35:14 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:35:14 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:35:14 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:35:14 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (172.18.0.9, executor 1, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:35:14 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9) (172.18.0.7, executor 0, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:35:14 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 10) (172.18.0.4, executor 2, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:35:14 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 11) (172.18.0.9, executor 1, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:46753 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:34455 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:35:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:42631 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:46753 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:42631 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:34455 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:46753 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 618 ms on 172.18.0.7 (executor 0) (1/4)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:34455 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:42631 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:35:15 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 10) in 742 ms on 172.18.0.4 (executor 2) (2/4)
25/03/30 12:35:15 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 11) in 760 ms on 172.18.0.9 (executor 1) (3/4)
25/03/30 12:35:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 762 ms on 172.18.0.9 (executor 1) (4/4)
25/03/30 12:35:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:35:15 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.779 s
25/03/30 12:35:15 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:35:15 INFO DAGScheduler: running: Set()
25/03/30 12:35:15 INFO DAGScheduler: waiting: Set()
25/03/30 12:35:15 INFO DAGScheduler: failed: Set()
25/03/30 12:35:15 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:35:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:35:15 INFO CodeGenerator: Code generated in 16.355043 ms
25/03/30 12:35:15 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:35:15 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:35:15 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:35:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:35:15 INFO DAGScheduler: Missing parents: List()
25/03/30 12:35:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:35:15 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:35:15 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 363.4 MiB)
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:45753 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:35:15 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:35:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:35:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:35:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12) (172.18.0.4, executor 2, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:35:15 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.4:34455 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:35:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.4:47196
25/03/30 12:35:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 534 ms on 172.18.0.4 (executor 2) (1/1)
25/03/30 12:35:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:35:16 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.552 s
25/03/30 12:35:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:35:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:35:16 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.561486 s
25/03/30 12:35:16 INFO FileFormatWriter: Start to commit write Job 94dfd319-fd85-4677-8538-8b614693e1c5.
25/03/30 12:35:16 INFO FileFormatWriter: Write Job 94dfd319-fd85-4677-8538-8b614693e1c5 committed. Elapsed time: 53 ms.
25/03/30 12:35:16 INFO FileFormatWriter: Finished processing stats for write job 94dfd319-fd85-4677-8538-8b614693e1c5.
25/03/30 12:35:16 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 1 paths.
25/03/30 12:35:16 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:35:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:35:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:35:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:35:16 INFO MemoryStore: MemoryStore cleared
25/03/30 12:35:16 INFO BlockManager: BlockManager stopped
25/03/30 12:35:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:35:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:35:16 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:35:16 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:35:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab89abb-738a-4061-be01-77cb2ca202ce
25/03/30 12:35:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab89abb-738a-4061-be01-77cb2ca202ce/pyspark-8abc2c81-af30-4d3c-a741-477fcf5dae0b
25/03/30 12:35:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-0bc673a0-94a2-4347-a1c2-28e04d4a7c47
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:36:02 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:36:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:36:02 INFO ResourceUtils: ==============================================================
25/03/30 12:36:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:36:02 INFO ResourceUtils: ==============================================================
25/03/30 12:36:02 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:36:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:36:02 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:36:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:36:02 INFO SecurityManager: Changing view acls to: root
25/03/30 12:36:02 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:36:02 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:36:02 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:36:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:36:02 INFO Utils: Successfully started service 'sparkDriver' on port 39861.
25/03/30 12:36:02 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:36:02 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:36:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:36:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:36:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:36:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21c89493-2b67-44b1-9a48-afb292a6dfe9
25/03/30 12:36:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:36:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:36:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:36:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:36:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 24 ms (0 ms spent in bootstraps)
25/03/30 12:36:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330123603-0004
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123603-0004/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:36:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123603-0004/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123603-0004/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:36:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123603-0004/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123603-0004/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:36:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123603-0004/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:36:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46651.
25/03/30 12:36:03 INFO NettyBlockTransferService: Server created on a339e4692fd5:46651
25/03/30 12:36:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:36:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 46651, None)
25/03/30 12:36:03 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:46651 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 46651, None)
25/03/30 12:36:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 46651, None)
25/03/30 12:36:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 46651, None)
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123603-0004/1 is now RUNNING
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123603-0004/0 is now RUNNING
25/03/30 12:36:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123603-0004/2 is now RUNNING
25/03/30 12:36:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:36:03 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:36:03 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:36:05 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.
25/03/30 12:36:05 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 4 paths.
25/03/30 12:36:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:42736) with ID 2,  ResourceProfileId 0
25/03/30 12:36:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:59880) with ID 0,  ResourceProfileId 0
25/03/30 12:36:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:54388) with ID 1,  ResourceProfileId 0
25/03/30 12:36:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:38991 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 38991, None)
25/03/30 12:36:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:40731 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 40731, None)
25/03/30 12:36:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38627 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 38627, None)
25/03/30 12:36:07 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:36:07 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:36:07 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:36:07 INFO CodeGenerator: Code generated in 141.471047 ms
25/03/30 12:36:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:36:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:36:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:46651 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:36:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:36:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:36:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:36:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:36:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:36:07 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:36:07 INFO DAGScheduler: Missing parents: List()
25/03/30 12:36:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:36:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:36:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:36:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:46651 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:36:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:36:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:36:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:36:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 0, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:36:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:40731 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:36:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:40731 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:36:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1505 ms on 172.18.0.7 (executor 0) (1/1)
25/03/30 12:36:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:36:09 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.575 s
25/03/30 12:36:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:36:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:36:09 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.609134 s
25/03/30 12:36:09 INFO CodeGenerator: Code generated in 8.46959 ms
25/03/30 12:36:09 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:36:09 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:36:09 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:36:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:36:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:36:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:46651 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:36:09 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:36:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:36:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:36:09 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:36:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:36:09 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:36:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:36:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:36:09 INFO metastore: Connected to metastore.
25/03/30 12:36:10 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=fe1a7f6d-b26f-4862-b9a0-38ef324873ce, clientType=HIVECLI]
25/03/30 12:36:10 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:36:10 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:36:10 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:36:10 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:36:10 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:36:10 INFO metastore: Connected to metastore.
25/03/30 12:36:10 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:36:10 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:36:10 INFO metastore: Connected to metastore.
25/03/30 12:36:10 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
25/03/30 12:36:10 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:36:10 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:36:10 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:36:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:36:10 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:36:10 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:36:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:36:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:36:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:36:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:36:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:36:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:36:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:36:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:46651 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.7:40731 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:36:10 INFO CodeGenerator: Code generated in 19.272243 ms
25/03/30 12:36:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:36:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.2 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:46651 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:36:10 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:36:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4894006 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:36:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:36:10 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 7 output partitions
25/03/30 12:36:10 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:36:10 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:36:10 INFO DAGScheduler: Missing parents: List()
25/03/30 12:36:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:36:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:36:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:46651 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:36:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:36:10 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
25/03/30 12:36:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
25/03/30 12:36:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 2, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.9, executor 1, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.7, executor 0, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.4, executor 2, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.9, executor 1, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.7, executor 0, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.4, executor 2, partition 6, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:40731 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:40731 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:38627 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:36:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:38991 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:36:11 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 709 ms on 172.18.0.7 (executor 0) (1/7)
25/03/30 12:36:11 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 728 ms on 172.18.0.7 (executor 0) (2/7)
25/03/30 12:36:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:38991 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:36:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:38627 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:36:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2203 ms on 172.18.0.4 (executor 2) (3/7)
25/03/30 12:36:12 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 2203 ms on 172.18.0.4 (executor 2) (4/7)
25/03/30 12:36:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2251 ms on 172.18.0.9 (executor 1) (5/7)
25/03/30 12:36:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2252 ms on 172.18.0.9 (executor 1) (6/7)
25/03/30 12:36:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2416 ms on 172.18.0.4 (executor 2) (7/7)
25/03/30 12:36:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:36:13 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.422 s
25/03/30 12:36:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:36:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:36:13 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.426403 s
25/03/30 12:36:13 INFO CodeGenerator: Code generated in 8.140087 ms
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:46651 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:36:13 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:36:13 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:36:13 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:36:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:36:13 INFO CodeGenerator: Code generated in 24.033838 ms
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:46651 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:36:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:36:13 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:36:13 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:36:13 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:36:13 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:36:13 INFO DAGScheduler: Missing parents: List()
25/03/30 12:36:13 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:36:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:46651 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:36:13 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:36:13 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:36:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:36:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 8) (172.18.0.4, executor 2, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:36:13 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 9) (172.18.0.9, executor 1, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:36:13 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 10) (172.18.0.7, executor 0, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:36:13 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 11) (172.18.0.4, executor 2, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:40731 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:38991 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:38627 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:40731 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:38627 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:38991 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:40731 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:38627 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 10) in 517 ms on 172.18.0.7 (executor 0) (1/4)
25/03/30 12:36:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:38991 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:36:13 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 9) in 603 ms on 172.18.0.9 (executor 1) (2/4)
25/03/30 12:36:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 8) in 630 ms on 172.18.0.4 (executor 2) (3/4)
25/03/30 12:36:13 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 11) in 630 ms on 172.18.0.4 (executor 2) (4/4)
25/03/30 12:36:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:36:13 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.644 s
25/03/30 12:36:13 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:36:13 INFO DAGScheduler: running: Set()
25/03/30 12:36:13 INFO DAGScheduler: waiting: Set()
25/03/30 12:36:13 INFO DAGScheduler: failed: Set()
25/03/30 12:36:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:36:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:36:13 INFO CodeGenerator: Code generated in 14.612893 ms
25/03/30 12:36:14 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:36:14 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:36:14 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:36:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:36:14 INFO DAGScheduler: Missing parents: List()
25/03/30 12:36:14 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:36:14 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:36:14 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 363.4 MiB)
25/03/30 12:36:14 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:46651 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:36:14 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:36:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:36:14 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:36:14 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 12) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:36:14 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.9:38627 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:36:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:54388
25/03/30 12:36:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 12) in 473 ms on 172.18.0.9 (executor 1) (1/1)
25/03/30 12:36:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:36:14 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.489 s
25/03/30 12:36:14 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:36:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:36:14 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.498624 s
25/03/30 12:36:14 INFO FileFormatWriter: Start to commit write Job 4da9f009-64e4-459a-a766-acc92d82f975.
25/03/30 12:36:14 INFO FileFormatWriter: Write Job 4da9f009-64e4-459a-a766-acc92d82f975 committed. Elapsed time: 43 ms.
25/03/30 12:36:14 INFO FileFormatWriter: Finished processing stats for write job 4da9f009-64e4-459a-a766-acc92d82f975.
25/03/30 12:36:14 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
25/03/30 12:36:14 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:36:14 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:36:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:36:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:36:14 INFO MemoryStore: MemoryStore cleared
25/03/30 12:36:14 INFO BlockManager: BlockManager stopped
25/03/30 12:36:14 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:36:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:36:14 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:36:14 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:36:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a164d74-dcf0-41ec-959c-01739f9d86fa
25/03/30 12:36:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff5d1b01-98c7-4194-a9d2-1b0c0debb687
25/03/30 12:36:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff5d1b01-98c7-4194-a9d2-1b0c0debb687/pyspark-2f788b23-0e64-46cd-90cd-1c2854a69db0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:37:02 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:37:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:37:02 INFO ResourceUtils: ==============================================================
25/03/30 12:37:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:37:02 INFO ResourceUtils: ==============================================================
25/03/30 12:37:02 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:37:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:37:02 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:37:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:37:02 INFO SecurityManager: Changing view acls to: root
25/03/30 12:37:02 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:37:02 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:37:02 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:37:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:37:02 INFO Utils: Successfully started service 'sparkDriver' on port 46023.
25/03/30 12:37:02 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:37:02 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:37:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:37:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:37:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:37:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c2c42ab9-e53d-4255-8ad8-3e0100ea55e7
25/03/30 12:37:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:37:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:37:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:37:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:37:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 21 ms (0 ms spent in bootstraps)
25/03/30 12:37:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330123703-0005
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123703-0005/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123703-0005/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123703-0005/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123703-0005/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123703-0005/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123703-0005/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:37:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41953.
25/03/30 12:37:03 INFO NettyBlockTransferService: Server created on a339e4692fd5:41953
25/03/30 12:37:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:37:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 41953, None)
25/03/30 12:37:03 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:41953 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 41953, None)
25/03/30 12:37:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 41953, None)
25/03/30 12:37:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 41953, None)
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123703-0005/0 is now RUNNING
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123703-0005/2 is now RUNNING
25/03/30 12:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123703-0005/1 is now RUNNING
25/03/30 12:37:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:37:03 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:37:03 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:37:05 INFO InMemoryFileIndex: It took 73 ms to list leaf files for 1 paths.
25/03/30 12:37:05 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 4 paths.
25/03/30 12:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:55312) with ID 0,  ResourceProfileId 0
25/03/30 12:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:54776) with ID 2,  ResourceProfileId 0
25/03/30 12:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:35696) with ID 1,  ResourceProfileId 0
25/03/30 12:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:43111 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 43111, None)
25/03/30 12:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:40843 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 40843, None)
25/03/30 12:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:37337 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 37337, None)
25/03/30 12:37:06 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:37:06 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:37:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:37:07 INFO CodeGenerator: Code generated in 134.998376 ms
25/03/30 12:37:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:37:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:37:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:41953 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:37:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:37:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:37:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:37:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:37:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:37:07 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:37:07 INFO DAGScheduler: Missing parents: List()
25/03/30 12:37:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:37:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:37:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:37:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:41953 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:37:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:37:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:37:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:37:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 2, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:37:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:43111 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:37:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:43111 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:37:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1437 ms on 172.18.0.4 (executor 2) (1/1)
25/03/30 12:37:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:37:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.506 s
25/03/30 12:37:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:37:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:37:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.539303 s
25/03/30 12:37:08 INFO CodeGenerator: Code generated in 7.656238 ms
25/03/30 12:37:08 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:37:08 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:37:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:37:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:37:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:37:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:41953 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:37:08 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:37:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:37:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:37:09 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:37:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:37:09 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:37:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:37:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:37:09 INFO metastore: Connected to metastore.
25/03/30 12:37:09 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5633d2b3-30af-4c68-9bf6-7bd232929b56, clientType=HIVECLI]
25/03/30 12:37:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:37:09 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:37:09 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:37:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:37:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:37:09 INFO metastore: Connected to metastore.
25/03/30 12:37:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:37:09 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:37:09 INFO metastore: Connected to metastore.
25/03/30 12:37:09 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
25/03/30 12:37:09 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:37:09 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:37:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:37:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:37:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:37:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:37:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:37:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:41953 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.4:43111 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:37:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:37:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:37:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:37:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:37:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:37:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:37:10 INFO CodeGenerator: Code generated in 17.747434 ms
25/03/30 12:37:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:37:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.2 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:41953 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:37:10 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:37:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5243566 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:37:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:37:10 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 8 output partitions
25/03/30 12:37:10 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:37:10 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:37:10 INFO DAGScheduler: Missing parents: List()
25/03/30 12:37:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:37:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:37:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:41953 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:37:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:37:10 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/03/30 12:37:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
25/03/30 12:37:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 1, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.7, executor 0, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.4, executor 2, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.9, executor 1, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.7, executor 0, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.4, executor 2, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.9, executor 1, partition 6, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.18.0.7, executor 0, partition 7, ANY, 4947 bytes) taskResourceAssignments Map()
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:43111 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:43111 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:40843 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:37:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:37337 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:37:10 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 644 ms on 172.18.0.4 (executor 2) (1/8)
25/03/30 12:37:10 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 643 ms on 172.18.0.4 (executor 2) (2/8)
25/03/30 12:37:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:40843 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:37:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:37337 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 2036 ms on 172.18.0.7 (executor 0) (3/8)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2038 ms on 172.18.0.7 (executor 0) (4/8)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2038 ms on 172.18.0.7 (executor 0) (5/8)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 2097 ms on 172.18.0.9 (executor 1) (6/8)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2097 ms on 172.18.0.9 (executor 1) (7/8)
25/03/30 12:37:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2278 ms on 172.18.0.9 (executor 1) (8/8)
25/03/30 12:37:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:37:12 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.282 s
25/03/30 12:37:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:37:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:37:12 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.286008 s
25/03/30 12:37:12 INFO CodeGenerator: Code generated in 6.350587 ms
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:41953 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:37:12 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:37:12 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:37:12 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:37:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:37:12 INFO CodeGenerator: Code generated in 22.11716 ms
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:41953 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:37:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:37:12 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:37:12 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:37:12 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:37:12 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:37:12 INFO DAGScheduler: Missing parents: List()
25/03/30 12:37:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:37:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:41953 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:37:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:37:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:37:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:37:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 9) (172.18.0.7, executor 0, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:37:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 10) (172.18.0.4, executor 2, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:37:12 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 11) (172.18.0.9, executor 1, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:37:12 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 12) (172.18.0.7, executor 0, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:43111 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:40843 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:37337 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:43111 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:40843 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:37337 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:37:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:43111 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:37:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:40843 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:37:13 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 10) in 452 ms on 172.18.0.4 (executor 2) (1/4)
25/03/30 12:37:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:37337 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:37:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 9) in 560 ms on 172.18.0.7 (executor 0) (2/4)
25/03/30 12:37:13 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 12) in 558 ms on 172.18.0.7 (executor 0) (3/4)
25/03/30 12:37:13 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 11) in 569 ms on 172.18.0.9 (executor 1) (4/4)
25/03/30 12:37:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:37:13 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.583 s
25/03/30 12:37:13 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:37:13 INFO DAGScheduler: running: Set()
25/03/30 12:37:13 INFO DAGScheduler: waiting: Set()
25/03/30 12:37:13 INFO DAGScheduler: failed: Set()
25/03/30 12:37:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:37:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:37:13 INFO CodeGenerator: Code generated in 13.074116 ms
25/03/30 12:37:13 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:37:13 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:37:13 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:37:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:37:13 INFO DAGScheduler: Missing parents: List()
25/03/30 12:37:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:37:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:37:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 363.4 MiB)
25/03/30 12:37:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:41953 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:37:13 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:37:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:37:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:37:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 13) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:37:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.9:37337 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:37:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:35696
25/03/30 12:37:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 13) in 304 ms on 172.18.0.9 (executor 1) (1/1)
25/03/30 12:37:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:37:13 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.321 s
25/03/30 12:37:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:37:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:37:13 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.328868 s
25/03/30 12:37:13 INFO FileFormatWriter: Start to commit write Job 4d10cef6-2915-4be6-8978-e2734cc0f13a.
25/03/30 12:37:13 INFO FileFormatWriter: Write Job 4d10cef6-2915-4be6-8978-e2734cc0f13a committed. Elapsed time: 39 ms.
25/03/30 12:37:13 INFO FileFormatWriter: Finished processing stats for write job 4d10cef6-2915-4be6-8978-e2734cc0f13a.
25/03/30 12:37:13 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
25/03/30 12:37:13 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:37:13 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:37:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:37:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:37:13 INFO MemoryStore: MemoryStore cleared
25/03/30 12:37:13 INFO BlockManager: BlockManager stopped
25/03/30 12:37:13 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:37:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:37:13 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:37:14 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:37:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-679f3caf-7ad0-4aed-a39a-a9657408167f
25/03/30 12:37:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce370267-1ee2-4f65-a37b-20b5603e3fdd
25/03/30 12:37:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce370267-1ee2-4f65-a37b-20b5603e3fdd/pyspark-fb8c1a26-5663-4edf-a304-74fcf84bd11e
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:38:02 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:38:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:38:02 INFO ResourceUtils: ==============================================================
25/03/30 12:38:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:38:02 INFO ResourceUtils: ==============================================================
25/03/30 12:38:02 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:38:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:38:02 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:38:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:38:02 INFO SecurityManager: Changing view acls to: root
25/03/30 12:38:02 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:38:02 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:38:02 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:38:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:38:02 INFO Utils: Successfully started service 'sparkDriver' on port 35085.
25/03/30 12:38:02 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:38:02 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:38:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:38:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:38:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:38:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-66471beb-5472-4e1b-a19e-f628c2f10343
25/03/30 12:38:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:38:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:38:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:38:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:38:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 22 ms (0 ms spent in bootstraps)
25/03/30 12:38:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330123803-0006
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123803-0006/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:38:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123803-0006/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123803-0006/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:38:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123803-0006/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123803-0006/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:38:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123803-0006/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:38:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46517.
25/03/30 12:38:03 INFO NettyBlockTransferService: Server created on a339e4692fd5:46517
25/03/30 12:38:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:38:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 46517, None)
25/03/30 12:38:03 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:46517 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 46517, None)
25/03/30 12:38:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 46517, None)
25/03/30 12:38:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 46517, None)
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123803-0006/1 is now RUNNING
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123803-0006/0 is now RUNNING
25/03/30 12:38:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123803-0006/2 is now RUNNING
25/03/30 12:38:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:38:03 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:38:03 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:38:05 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/03/30 12:38:05 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 4 paths.
25/03/30 12:38:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:54030) with ID 1,  ResourceProfileId 0
25/03/30 12:38:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:35724) with ID 0,  ResourceProfileId 0
25/03/30 12:38:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33746) with ID 2,  ResourceProfileId 0
25/03/30 12:38:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:45069 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 45069, None)
25/03/30 12:38:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:34753 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 34753, None)
25/03/30 12:38:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:38877 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 38877, None)
25/03/30 12:38:06 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:38:06 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:38:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:38:07 INFO CodeGenerator: Code generated in 141.945948 ms
25/03/30 12:38:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:38:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:38:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:46517 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:38:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:38:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:38:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:38:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:38:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:38:07 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:38:07 INFO DAGScheduler: Missing parents: List()
25/03/30 12:38:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:38:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:38:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:38:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:46517 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:38:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:38:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:38:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:38:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 1, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:38:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:34753 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:38:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:34753 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:38:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1403 ms on 172.18.0.9 (executor 1) (1/1)
25/03/30 12:38:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:38:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.472 s
25/03/30 12:38:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:38:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:38:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.504203 s
25/03/30 12:38:08 INFO CodeGenerator: Code generated in 8.108938 ms
25/03/30 12:38:08 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:38:08 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:38:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:38:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:38:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:38:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:46517 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:38:08 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:38:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:38:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:38:09 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:38:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:38:09 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:38:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:38:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:38:09 INFO metastore: Connected to metastore.
25/03/30 12:38:09 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4b068aee-907f-4b29-bef7-1e66558a3895, clientType=HIVECLI]
25/03/30 12:38:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:38:09 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:38:09 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:38:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:38:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:38:09 INFO metastore: Connected to metastore.
25/03/30 12:38:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:38:09 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:38:09 INFO metastore: Connected to metastore.
25/03/30 12:38:09 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
25/03/30 12:38:09 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:38:09 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:38:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:38:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:38:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:38:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:38:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:38:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:38:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:38:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:38:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:46517 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:38:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.9:34753 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:38:09 INFO CodeGenerator: Code generated in 16.523075 ms
25/03/30 12:38:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:38:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.6 KiB, free 365.2 MiB)
25/03/30 12:38:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:46517 (size: 33.6 KiB, free: 366.2 MiB)
25/03/30 12:38:09 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:38:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5593125 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:38:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:38:10 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 8 output partitions
25/03/30 12:38:10 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:38:10 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:38:10 INFO DAGScheduler: Missing parents: List()
25/03/30 12:38:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:38:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:38:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:38:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:46517 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:38:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:38:10 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
25/03/30 12:38:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
25/03/30 12:38:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 1, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.7, executor 0, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.4, executor 2, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.9, executor 1, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.7, executor 0, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.4, executor 2, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.9, executor 1, partition 6, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.18.0.7, executor 0, partition 7, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:38:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:34753 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:38:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:34753 (size: 33.6 KiB, free: 366.2 MiB)
25/03/30 12:38:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:38877 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:38:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:45069 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:38:10 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 674 ms on 172.18.0.9 (executor 1) (1/8)
25/03/30 12:38:10 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 680 ms on 172.18.0.9 (executor 1) (2/8)
25/03/30 12:38:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 930 ms on 172.18.0.9 (executor 1) (3/8)
25/03/30 12:38:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:45069 (size: 33.6 KiB, free: 366.3 MiB)
25/03/30 12:38:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:38877 (size: 33.6 KiB, free: 366.3 MiB)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2077 ms on 172.18.0.7 (executor 0) (4/8)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2077 ms on 172.18.0.4 (executor 2) (5/8)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2078 ms on 172.18.0.7 (executor 0) (6/8)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2078 ms on 172.18.0.4 (executor 2) (7/8)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 2077 ms on 172.18.0.7 (executor 0) (8/8)
25/03/30 12:38:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:38:12 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.085 s
25/03/30 12:38:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:38:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:38:12 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.088463 s
25/03/30 12:38:12 INFO CodeGenerator: Code generated in 6.315876 ms
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:46517 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:38:12 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:38:12 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:38:12 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:38:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:38:12 INFO CodeGenerator: Code generated in 22.811225 ms
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:46517 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:38:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:38:12 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:38:12 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:38:12 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:38:12 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:38:12 INFO DAGScheduler: Missing parents: List()
25/03/30 12:38:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:46517 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:38:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:38:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:38:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:38:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 9) (172.18.0.4, executor 2, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:38:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 10) (172.18.0.9, executor 1, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:38:12 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 11) (172.18.0.7, executor 0, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:38:12 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 12) (172.18.0.4, executor 2, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:34753 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:45069 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:38877 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:34753 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:38877 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:45069 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:34753 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 10) in 442 ms on 172.18.0.9 (executor 1) (1/4)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:38877 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:45069 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 9) in 542 ms on 172.18.0.4 (executor 2) (2/4)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 12) in 542 ms on 172.18.0.4 (executor 2) (3/4)
25/03/30 12:38:12 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 11) in 554 ms on 172.18.0.7 (executor 0) (4/4)
25/03/30 12:38:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:38:12 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.569 s
25/03/30 12:38:12 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:38:12 INFO DAGScheduler: running: Set()
25/03/30 12:38:12 INFO DAGScheduler: waiting: Set()
25/03/30 12:38:12 INFO DAGScheduler: failed: Set()
25/03/30 12:38:12 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:38:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:38:12 INFO CodeGenerator: Code generated in 12.464486 ms
25/03/30 12:38:12 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:38:12 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:38:12 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:38:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:38:12 INFO DAGScheduler: Missing parents: List()
25/03/30 12:38:12 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:38:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.3 KiB, free 363.4 MiB)
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:46517 (size: 87.3 KiB, free: 366.1 MiB)
25/03/30 12:38:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:38:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:38:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:38:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 13) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:38:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.7:45069 (size: 87.3 KiB, free: 366.1 MiB)
25/03/30 12:38:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:35724
25/03/30 12:38:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 13) in 454 ms on 172.18.0.7 (executor 0) (1/1)
25/03/30 12:38:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:38:13 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.468 s
25/03/30 12:38:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:38:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:38:13 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.475549 s
25/03/30 12:38:13 INFO FileFormatWriter: Start to commit write Job 5d6594da-d509-43b3-9068-85179c70d115.
25/03/30 12:38:13 INFO FileFormatWriter: Write Job 5d6594da-d509-43b3-9068-85179c70d115 committed. Elapsed time: 38 ms.
25/03/30 12:38:13 INFO FileFormatWriter: Finished processing stats for write job 5d6594da-d509-43b3-9068-85179c70d115.
25/03/30 12:38:13 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
25/03/30 12:38:13 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:38:13 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:38:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:38:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:38:13 INFO MemoryStore: MemoryStore cleared
25/03/30 12:38:13 INFO BlockManager: BlockManager stopped
25/03/30 12:38:13 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:38:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:38:13 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:38:13 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:38:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b224026-e837-4fc1-a939-6e4b14f6e057
25/03/30 12:38:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd04e2f9-1e6e-4afa-b405-8988f442ed00
25/03/30 12:38:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b224026-e837-4fc1-a939-6e4b14f6e057/pyspark-e61b31f5-37d6-4e90-82fd-4d78b5e79aa0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:39:02 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:39:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:39:02 INFO ResourceUtils: ==============================================================
25/03/30 12:39:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:39:02 INFO ResourceUtils: ==============================================================
25/03/30 12:39:02 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:39:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:39:02 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:39:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:39:02 INFO SecurityManager: Changing view acls to: root
25/03/30 12:39:02 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:39:02 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:39:02 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:39:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:39:02 INFO Utils: Successfully started service 'sparkDriver' on port 35825.
25/03/30 12:39:02 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:39:02 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:39:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:39:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:39:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:39:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-43ba0e9d-aeef-4648-8620-1c81e0a80733
25/03/30 12:39:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:39:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:39:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:39:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:39:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 22 ms (0 ms spent in bootstraps)
25/03/30 12:39:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330123903-0007
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123903-0007/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:39:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123903-0007/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123903-0007/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:39:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123903-0007/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330123903-0007/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:39:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330123903-0007/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:39:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41269.
25/03/30 12:39:03 INFO NettyBlockTransferService: Server created on a339e4692fd5:41269
25/03/30 12:39:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:39:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 41269, None)
25/03/30 12:39:03 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:41269 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 41269, None)
25/03/30 12:39:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 41269, None)
25/03/30 12:39:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 41269, None)
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123903-0007/0 is now RUNNING
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123903-0007/1 is now RUNNING
25/03/30 12:39:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330123903-0007/2 is now RUNNING
25/03/30 12:39:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:39:03 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:39:03 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:39:05 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/03/30 12:39:05 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 4 paths.
25/03/30 12:39:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:36784) with ID 0,  ResourceProfileId 0
25/03/30 12:39:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:57924) with ID 1,  ResourceProfileId 0
25/03/30 12:39:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:44862) with ID 2,  ResourceProfileId 0
25/03/30 12:39:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:36847 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 36847, None)
25/03/30 12:39:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33991 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 33991, None)
25/03/30 12:39:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:35369 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 35369, None)
25/03/30 12:39:06 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:39:06 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:39:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:39:07 INFO CodeGenerator: Code generated in 136.807977 ms
25/03/30 12:39:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:39:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:39:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:41269 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:39:07 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:39:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:39:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:39:07 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:39:07 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:39:07 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:39:07 INFO DAGScheduler: Missing parents: List()
25/03/30 12:39:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:39:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:39:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:39:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:41269 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:39:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:39:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:39:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:39:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 2, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:39:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:35369 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:39:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:35369 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:39:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1466 ms on 172.18.0.4 (executor 2) (1/1)
25/03/30 12:39:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:39:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.536 s
25/03/30 12:39:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:39:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:39:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.568643 s
25/03/30 12:39:08 INFO CodeGenerator: Code generated in 7.964388 ms
25/03/30 12:39:08 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:39:08 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:39:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:39:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:39:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:39:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:41269 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:39:08 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:39:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:39:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:39:09 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:39:09 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:39:09 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:39:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:39:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:39:09 INFO metastore: Connected to metastore.
25/03/30 12:39:09 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=bdf0e50f-6e48-44a1-b767-09b4f21bb11e, clientType=HIVECLI]
25/03/30 12:39:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:39:09 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:39:09 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:39:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:39:09 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:39:09 INFO metastore: Connected to metastore.
25/03/30 12:39:09 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:39:09 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:39:09 INFO metastore: Connected to metastore.
25/03/30 12:39:09 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
25/03/30 12:39:09 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:39:09 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:39:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:39:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:39:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:39:09 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:39:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:39:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:39:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:39:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:39:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:41269 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:39:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.4:35369 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:39:10 INFO CodeGenerator: Code generated in 16.752703 ms
25/03/30 12:39:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:39:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.2 MiB)
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:41269 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:39:10 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:39:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 5942685 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:39:10 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:39:10 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 9 output partitions
25/03/30 12:39:10 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:39:10 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:39:10 INFO DAGScheduler: Missing parents: List()
25/03/30 12:39:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:39:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:39:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:41269 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:39:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:39:10 INFO DAGScheduler: Submitting 9 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
25/03/30 12:39:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 9 tasks resource profile 0
25/03/30 12:39:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 0, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.9, executor 1, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.4, executor 2, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.7, executor 0, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.9, executor 1, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.4, executor 2, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.7, executor 0, partition 6, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.18.0.9, executor 1, partition 7, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (172.18.0.4, executor 2, partition 8, ANY, 4947 bytes) taskResourceAssignments Map()
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:35369 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:35369 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:33991 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:39:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:36847 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:39:10 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 653 ms on 172.18.0.4 (executor 2) (1/9)
25/03/30 12:39:10 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 693 ms on 172.18.0.4 (executor 2) (2/9)
25/03/30 12:39:10 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 699 ms on 172.18.0.4 (executor 2) (3/9)
25/03/30 12:39:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:36847 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:39:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:33991 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 2078 ms on 172.18.0.7 (executor 0) (4/9)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2079 ms on 172.18.0.7 (executor 0) (5/9)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2129 ms on 172.18.0.9 (executor 1) (6/9)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 2128 ms on 172.18.0.9 (executor 1) (7/9)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2129 ms on 172.18.0.9 (executor 1) (8/9)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2289 ms on 172.18.0.7 (executor 0) (9/9)
25/03/30 12:39:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:39:12 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.295 s
25/03/30 12:39:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:39:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:39:12 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.298945 s
25/03/30 12:39:12 INFO CodeGenerator: Code generated in 6.635914 ms
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:41269 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:39:12 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:39:12 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:39:12 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:39:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:39:12 INFO CodeGenerator: Code generated in 25.846117 ms
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:41269 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:39:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:39:12 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:39:12 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:39:12 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:39:12 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:39:12 INFO DAGScheduler: Missing parents: List()
25/03/30 12:39:12 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:39:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:41269 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:39:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:39:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:39:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:39:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10) (172.18.0.7, executor 0, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:39:12 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 11) (172.18.0.9, executor 1, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:39:12 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 12) (172.18.0.4, executor 2, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:39:12 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 13) (172.18.0.7, executor 0, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:35369 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:36847 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:33991 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:35369 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:33991 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:36847 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:35369 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:39:12 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 12) in 431 ms on 172.18.0.4 (executor 2) (1/4)
25/03/30 12:39:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:33991 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:39:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:36847 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:39:13 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 11) in 528 ms on 172.18.0.9 (executor 1) (2/4)
25/03/30 12:39:13 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 546 ms on 172.18.0.7 (executor 0) (3/4)
25/03/30 12:39:13 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 13) in 545 ms on 172.18.0.7 (executor 0) (4/4)
25/03/30 12:39:13 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:39:13 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.560 s
25/03/30 12:39:13 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:39:13 INFO DAGScheduler: running: Set()
25/03/30 12:39:13 INFO DAGScheduler: waiting: Set()
25/03/30 12:39:13 INFO DAGScheduler: failed: Set()
25/03/30 12:39:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:39:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:39:13 INFO CodeGenerator: Code generated in 11.67757 ms
25/03/30 12:39:13 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:39:13 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:39:13 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:39:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:39:13 INFO DAGScheduler: Missing parents: List()
25/03/30 12:39:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:39:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:39:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 363.4 MiB)
25/03/30 12:39:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:41269 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:39:13 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:39:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:39:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:39:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 14) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:39:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.7:36847 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:39:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.7:36784
25/03/30 12:39:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 14) in 292 ms on 172.18.0.7 (executor 0) (1/1)
25/03/30 12:39:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:39:13 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.307 s
25/03/30 12:39:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:39:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:39:13 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.314656 s
25/03/30 12:39:13 INFO FileFormatWriter: Start to commit write Job 23237d85-d586-4129-b181-937ed046f03b.
25/03/30 12:39:13 INFO FileFormatWriter: Write Job 23237d85-d586-4129-b181-937ed046f03b committed. Elapsed time: 40 ms.
25/03/30 12:39:13 INFO FileFormatWriter: Finished processing stats for write job 23237d85-d586-4129-b181-937ed046f03b.
25/03/30 12:39:13 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
25/03/30 12:39:13 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:39:13 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:39:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:39:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:39:13 INFO MemoryStore: MemoryStore cleared
25/03/30 12:39:13 INFO BlockManager: BlockManager stopped
25/03/30 12:39:13 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:39:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:39:13 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:39:14 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:39:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-66b9e52b-4c7f-424f-8c4a-df4b4165b991
25/03/30 12:39:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-66b9e52b-4c7f-424f-8c4a-df4b4165b991/pyspark-41bc68a0-0b8a-474c-a69d-bbdbc8625cfe
25/03/30 12:39:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-770b34fb-733c-420f-a69e-98b3ee168cc5
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/03/30 12:40:10 INFO SparkContext: Running Spark version 3.2.2
25/03/30 12:40:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 12:40:10 INFO ResourceUtils: ==============================================================
25/03/30 12:40:10 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 12:40:10 INFO ResourceUtils: ==============================================================
25/03/30 12:40:10 INFO SparkContext: Submitted application: Load countries data into Hive
25/03/30 12:40:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 12:40:10 INFO ResourceProfile: Limiting resource is cpu
25/03/30 12:40:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 12:40:10 INFO SecurityManager: Changing view acls to: root
25/03/30 12:40:10 INFO SecurityManager: Changing modify acls to: root
25/03/30 12:40:10 INFO SecurityManager: Changing view acls groups to: 
25/03/30 12:40:10 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 12:40:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/03/30 12:40:11 INFO Utils: Successfully started service 'sparkDriver' on port 44045.
25/03/30 12:40:11 INFO SparkEnv: Registering MapOutputTracker
25/03/30 12:40:11 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 12:40:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 12:40:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 12:40:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 12:40:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6324bb64-eb6f-4fcf-8efb-c8e47669a85c
25/03/30 12:40:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/03/30 12:40:11 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 12:40:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 12:40:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a339e4692fd5:4040
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/03/30 12:40:11 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 21 ms (0 ms spent in bootstraps)
25/03/30 12:40:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330124011-0008
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330124011-0008/0 on worker-20250330123156-172.18.0.7-45199 (172.18.0.7:45199) with 4 core(s)
25/03/30 12:40:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330124011-0008/0 on hostPort 172.18.0.7:45199 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330124011-0008/1 on worker-20250330123157-172.18.0.9-42421 (172.18.0.9:42421) with 4 core(s)
25/03/30 12:40:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330124011-0008/1 on hostPort 172.18.0.9:42421 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330124011-0008/2 on worker-20250330123156-172.18.0.4-40883 (172.18.0.4:40883) with 4 core(s)
25/03/30 12:40:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330124011-0008/2 on hostPort 172.18.0.4:40883 with 4 core(s), 1024.0 MiB RAM
25/03/30 12:40:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33403.
25/03/30 12:40:11 INFO NettyBlockTransferService: Server created on a339e4692fd5:33403
25/03/30 12:40:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 12:40:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a339e4692fd5, 33403, None)
25/03/30 12:40:11 INFO BlockManagerMasterEndpoint: Registering block manager a339e4692fd5:33403 with 366.3 MiB RAM, BlockManagerId(driver, a339e4692fd5, 33403, None)
25/03/30 12:40:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a339e4692fd5, 33403, None)
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330124011-0008/1 is now RUNNING
25/03/30 12:40:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a339e4692fd5, 33403, None)
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330124011-0008/0 is now RUNNING
25/03/30 12:40:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330124011-0008/2 is now RUNNING
25/03/30 12:40:11 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/03/30 12:40:12 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/03/30 12:40:12 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/03/30 12:40:13 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/03/30 12:40:13 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 4 paths.
25/03/30 12:40:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:50140) with ID 2,  ResourceProfileId 0
25/03/30 12:40:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:38066) with ID 1,  ResourceProfileId 0
25/03/30 12:40:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:38062) with ID 0,  ResourceProfileId 0
25/03/30 12:40:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:39415 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.4, 39415, None)
25/03/30 12:40:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33393 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 33393, None)
25/03/30 12:40:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:33901 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 33901, None)
25/03/30 12:40:14 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:40:14 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/03/30 12:40:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:40:15 INFO CodeGenerator: Code generated in 137.48668 ms
25/03/30 12:40:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/03/30 12:40:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/03/30 12:40:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a339e4692fd5:33403 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:40:15 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:40:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:40:15 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/03/30 12:40:15 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:40:15 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/03/30 12:40:15 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:40:15 INFO DAGScheduler: Missing parents: List()
25/03/30 12:40:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:40:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/03/30 12:40:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/03/30 12:40:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a339e4692fd5:33403 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:40:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/03/30 12:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:40:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/03/30 12:40:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 2, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/03/30 12:40:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.4:39415 (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:40:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:39415 (size: 32.6 KiB, free: 366.3 MiB)
25/03/30 12:40:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1392 ms on 172.18.0.4 (executor 2) (1/1)
25/03/30 12:40:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/03/30 12:40:16 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.458 s
25/03/30 12:40:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:40:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/03/30 12:40:16 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.489140 s
25/03/30 12:40:16 INFO CodeGenerator: Code generated in 7.694586 ms
25/03/30 12:40:16 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:40:16 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:40:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/03/30 12:40:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/03/30 12:40:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/03/30 12:40:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a339e4692fd5:33403 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:40:17 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/03/30 12:40:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:40:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:40:17 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/03/30 12:40:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/03/30 12:40:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/03/30 12:40:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:40:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:40:17 INFO metastore: Connected to metastore.
25/03/30 12:40:17 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5bb1d43d-2a4a-406e-8149-ed86dec91a74, clientType=HIVECLI]
25/03/30 12:40:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/03/30 12:40:17 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/03/30 12:40:17 INFO metastore: Closed a connection to metastore, current connections: 0
25/03/30 12:40:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:40:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/03/30 12:40:17 INFO metastore: Connected to metastore.
25/03/30 12:40:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/03/30 12:40:17 INFO metastore: Opened a connection to metastore, current connections: 2
25/03/30 12:40:17 INFO metastore: Connected to metastore.
25/03/30 12:40:17 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.
25/03/30 12:40:17 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:40:17 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:40:17 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:40:17 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID),IsNotNull(CountryName)
25/03/30 12:40:17 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#24),isnotnull(CountryName#25)
25/03/30 12:40:17 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: int, CountryName: string>
25/03/30 12:40:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:40:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:40:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:40:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:40:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/03/30 12:40:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/03/30 12:40:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/03/30 12:40:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a339e4692fd5:33403 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.4:39415 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/03/30 12:40:18 INFO CodeGenerator: Code generated in 15.67549 ms
25/03/30 12:40:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/03/30 12:40:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.2 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a339e4692fd5:33403 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:40:18 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:40:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6292244 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:40:18 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:40:18 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 9 output partitions
25/03/30 12:40:18 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/03/30 12:40:18 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:40:18 INFO DAGScheduler: Missing parents: List()
25/03/30 12:40:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/03/30 12:40:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/03/30 12:40:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on a339e4692fd5:33403 (size: 6.0 KiB, free: 366.2 MiB)
25/03/30 12:40:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/03/30 12:40:18 INFO DAGScheduler: Submitting 9 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
25/03/30 12:40:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 9 tasks resource profile 0
25/03/30 12:40:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 0, partition 0, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.9, executor 1, partition 1, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (172.18.0.4, executor 2, partition 2, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (172.18.0.7, executor 0, partition 3, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (172.18.0.9, executor 1, partition 4, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (172.18.0.4, executor 2, partition 5, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (172.18.0.7, executor 0, partition 6, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (172.18.0.9, executor 1, partition 7, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (172.18.0.4, executor 2, partition 8, ANY, 5095 bytes) taskResourceAssignments Map()
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.4:39415 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.4:39415 (size: 33.5 KiB, free: 366.2 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.7:33901 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:40:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:33393 (size: 6.0 KiB, free: 366.3 MiB)
25/03/30 12:40:18 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 649 ms on 172.18.0.4 (executor 2) (1/9)
25/03/30 12:40:18 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 650 ms on 172.18.0.4 (executor 2) (2/9)
25/03/30 12:40:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 651 ms on 172.18.0.4 (executor 2) (3/9)
25/03/30 12:40:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.7:33901 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:40:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:33393 (size: 33.5 KiB, free: 366.3 MiB)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1997 ms on 172.18.0.7 (executor 0) (4/9)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1999 ms on 172.18.0.7 (executor 0) (5/9)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2059 ms on 172.18.0.9 (executor 1) (6/9)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2060 ms on 172.18.0.9 (executor 1) (7/9)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 2059 ms on 172.18.0.9 (executor 1) (8/9)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2180 ms on 172.18.0.7 (executor 0) (9/9)
25/03/30 12:40:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/03/30 12:40:20 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.185 s
25/03/30 12:40:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:40:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/03/30 12:40:20 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.189310 s
25/03/30 12:40:20 INFO CodeGenerator: Code generated in 6.527212 ms
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1028.0 KiB, free 364.2 MiB)
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 364.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on a339e4692fd5:33403 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/03/30 12:40:20 INFO FileSourceStrategy: Pushed Filters: 
25/03/30 12:40:20 INFO FileSourceStrategy: Post-Scan Filters: 
25/03/30 12:40:20 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/03/30 12:40:20 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:40:20 INFO CodeGenerator: Code generated in 22.615131 ms
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on a339e4692fd5:33403 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO SparkContext: Created broadcast 6 from insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:40:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/03/30 12:40:20 INFO DAGScheduler: Registering RDD 17 (insertInto at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/03/30 12:40:20 INFO DAGScheduler: Got map stage job 2 (insertInto at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/03/30 12:40:20 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:40:20 INFO DAGScheduler: Parents of final stage: List()
25/03/30 12:40:20 INFO DAGScheduler: Missing parents: List()
25/03/30 12:40:20 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 39.3 KiB, free 363.8 MiB)
25/03/30 12:40:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 363.7 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on a339e4692fd5:33403 (size: 18.7 KiB, free: 366.1 MiB)
25/03/30 12:40:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/03/30 12:40:20 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/03/30 12:40:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/03/30 12:40:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10) (172.18.0.7, executor 0, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:40:20 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 11) (172.18.0.9, executor 1, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:40:20 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 12) (172.18.0.4, executor 2, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:40:20 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 13) (172.18.0.7, executor 0, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.4:39415 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.7:33901 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:33393 (size: 18.7 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.4:39415 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:33393 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.7:33901 (size: 3.5 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.4:39415 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:40:20 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 12) in 440 ms on 172.18.0.4 (executor 2) (1/4)
25/03/30 12:40:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:33393 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:40:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.7:33901 (size: 32.6 KiB, free: 366.2 MiB)
25/03/30 12:40:21 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 11) in 541 ms on 172.18.0.9 (executor 1) (2/4)
25/03/30 12:40:21 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 13) in 575 ms on 172.18.0.7 (executor 0) (3/4)
25/03/30 12:40:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 577 ms on 172.18.0.7 (executor 0) (4/4)
25/03/30 12:40:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/03/30 12:40:21 INFO DAGScheduler: ShuffleMapStage 2 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.590 s
25/03/30 12:40:21 INFO DAGScheduler: looking for newly runnable stages
25/03/30 12:40:21 INFO DAGScheduler: running: Set()
25/03/30 12:40:21 INFO DAGScheduler: waiting: Set()
25/03/30 12:40:21 INFO DAGScheduler: failed: Set()
25/03/30 12:40:21 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/03/30 12:40:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/03/30 12:40:21 INFO CodeGenerator: Code generated in 12.693706 ms
25/03/30 12:40:21 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/03/30 12:40:21 INFO DAGScheduler: Got job 3 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/03/30 12:40:21 INFO DAGScheduler: Final stage: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0)
25/03/30 12:40:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/03/30 12:40:21 INFO DAGScheduler: Missing parents: List()
25/03/30 12:40:21 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/03/30 12:40:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 235.3 KiB, free 363.5 MiB)
25/03/30 12:40:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 363.4 MiB)
25/03/30 12:40:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on a339e4692fd5:33403 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:40:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/03/30 12:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/03/30 12:40:21 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/03/30 12:40:21 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 14) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/03/30 12:40:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.9:33393 (size: 87.4 KiB, free: 366.1 MiB)
25/03/30 12:40:21 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:38066
25/03/30 12:40:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 14) in 456 ms on 172.18.0.9 (executor 1) (1/1)
25/03/30 12:40:21 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/03/30 12:40:21 INFO DAGScheduler: ResultStage 4 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.471 s
25/03/30 12:40:21 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
25/03/30 12:40:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
25/03/30 12:40:21 INFO DAGScheduler: Job 3 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.479323 s
25/03/30 12:40:21 INFO FileFormatWriter: Start to commit write Job 94fbac6e-8477-4d79-9606-16ff8e1c1602.
25/03/30 12:40:21 INFO FileFormatWriter: Write Job 94fbac6e-8477-4d79-9606-16ff8e1c1602 committed. Elapsed time: 40 ms.
25/03/30 12:40:21 INFO FileFormatWriter: Finished processing stats for write job 94fbac6e-8477-4d79-9606-16ff8e1c1602.
25/03/30 12:40:21 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.
25/03/30 12:40:21 INFO SparkUI: Stopped Spark web UI at http://a339e4692fd5:4040
25/03/30 12:40:21 INFO StandaloneSchedulerBackend: Shutting down all executors
25/03/30 12:40:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/03/30 12:40:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/03/30 12:40:21 INFO MemoryStore: MemoryStore cleared
25/03/30 12:40:21 INFO BlockManager: BlockManager stopped
25/03/30 12:40:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/03/30 12:40:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/03/30 12:40:21 INFO SparkContext: Successfully stopped SparkContext
25/03/30 12:40:21 INFO ShutdownHookManager: Shutdown hook called
25/03/30 12:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a960df8a-358b-45a0-bb94-8ff16808050e
25/03/30 12:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9bcf4120-a9d1-4f9d-881a-f90924cce0ed
25/03/30 12:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-9bcf4120-a9d1-4f9d-881a-f90924cce0ed/pyspark-fc170a9a-5dbc-402c-8046-add0df8b6307
Spark job completed successfully.
/opt/bitnami/python/bin/python3: can't open file '/opt/bitnami/spark/jobs/load_countries_into_hihve.py': [Errno 2] No such file or directory
log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Spark job FAILED!
