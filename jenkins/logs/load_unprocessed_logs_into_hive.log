Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:02:24 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:02:24 INFO ResourceUtils: ==============================================================
25/04/06 13:02:24 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:02:24 INFO ResourceUtils: ==============================================================
25/04/06 13:02:24 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:02:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:02:24 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:02:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:02:24 INFO SecurityManager: Changing view acls to: root
25/04/06 13:02:24 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:02:24 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:02:24 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:02:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:02:24 INFO Utils: Successfully started service 'sparkDriver' on port 32863.
25/04/06 13:02:24 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:02:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:02:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:02:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:02:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:02:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e61e110b-fdbd-48c9-bf05-8e925f6eec82
25/04/06 13:02:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:02:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:02:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:02:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:02:25 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130225-0016
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46393.
25/04/06 13:02:25 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46393
25/04/06 13:02:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:02:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46393 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/2 is now RUNNING
25/04/06 13:02:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/0 is now RUNNING
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/1 is now RUNNING
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:02:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:02:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:02:27 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42926) with ID 1,  ResourceProfileId 0
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:34976) with ID 2,  ResourceProfileId 0
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:54766) with ID 0,  ResourceProfileId 0
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:41307 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 41307, None)
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41679 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41679, None)
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:35431 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 35431, None)
25/04/06 13:02:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:02:27 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:02:27 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:02:27 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:02:27 INFO DAGScheduler: Missing parents: List()
25/04/06 13:02:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:02:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:02:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:02:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46393 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:02:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:02:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:02:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:02:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:35431 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1187 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:02:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:02:28 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.281 s
25/04/06 13:02:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:02:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:02:28 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.313783 s
25/04/06 13:02:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46393 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:35431 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:02:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:02:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:02:30 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ee96fb28-b02b-4d94-83d8-6a68a452555f, clientType=HIVECLI]
25/04/06 13:02:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:02:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:02:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 34, in <module>
    logs.write.mode("overwrite").insertInto("unprocessedlogs")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: `default`.`unprocessedlogs` requires that the data to be inserted have the same number of columns as the target table: target table has 6 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).
25/04/06 13:02:31 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:02:31 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:02:31 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:02:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:02:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:02:31 INFO MemoryStore: MemoryStore cleared
25/04/06 13:02:31 INFO BlockManager: BlockManager stopped
25/04/06 13:02:31 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:02:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:02:31 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:02:31 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb83973b-598b-42cf-9a69-b4bf0ccc4d27
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc4eebb4-d4e4-4d25-b10d-6ba792d78885
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc4eebb4-d4e4-4d25-b10d-6ba792d78885/pyspark-268cef26-3139-4fbc-98a5-dc206b524549
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:05:52 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:05:52 INFO ResourceUtils: ==============================================================
25/04/06 13:05:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:05:52 INFO ResourceUtils: ==============================================================
25/04/06 13:05:52 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:05:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:05:52 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:05:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:05:52 INFO SecurityManager: Changing view acls to: root
25/04/06 13:05:52 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:05:52 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:05:52 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:05:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:05:52 INFO Utils: Successfully started service 'sparkDriver' on port 40125.
25/04/06 13:05:52 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:05:53 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:05:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:05:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-91e8561f-2773-423b-8cac-1076d80bf1e1
25/04/06 13:05:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:05:53 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:05:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:05:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:05:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130553-0018
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42135.
25/04/06 13:05:53 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42135
25/04/06 13:05:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:05:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42135 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/2 is now RUNNING
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/0 is now RUNNING
25/04/06 13:05:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/1 is now RUNNING
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:05:53 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:05:53 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:05:55 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59970) with ID 1,  ResourceProfileId 0
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:57896) with ID 0,  ResourceProfileId 0
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43054) with ID 2,  ResourceProfileId 0
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:35587 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 35587, None)
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35801 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35801, None)
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:41079 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 41079, None)
25/04/06 13:05:55 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:05:55 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:05:55 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:05:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:05:55 INFO DAGScheduler: Missing parents: List()
25/04/06 13:05:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:05:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:05:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:05:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42135 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:05:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:05:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:05:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:05:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:41079 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1209 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:05:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:05:56 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.302 s
25/04/06 13:05:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:05:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:05:57 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.333651 s
25/04/06 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42135 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:41079 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:58 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:05:58 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:05:58 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:05:58 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:05:58 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:58 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:05:58 INFO metastore: Connected to metastore.
25/04/06 13:05:58 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=383753ba-5d6c-4fed-83a6-74a0439e68d8, clientType=HIVECLI]
25/04/06 13:05:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:05:58 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:05:58 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:05:58 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:58 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:05:58 INFO metastore: Connected to metastore.
25/04/06 13:05:59 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:59 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:05:59 INFO metastore: Connected to metastore.
25/04/06 13:05:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 34, in <module>
    logs.write.mode("overwrite").insertInto("unprocessedlogs")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: `default`.`unprocessedlogs` requires that the data to be inserted have the same number of columns as the target table: target table has 6 column(s) but the inserted data has 7 column(s), including 0 partition column(s) having constant value(s).
25/04/06 13:05:59 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:05:59 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:05:59 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:05:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:05:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:05:59 INFO MemoryStore: MemoryStore cleared
25/04/06 13:05:59 INFO BlockManager: BlockManager stopped
25/04/06 13:05:59 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:05:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:05:59 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:05:59 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-053370a0-3a2c-48a9-b843-11dc747e4749
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8e1eb8a-9669-4763-9147-0f604b862dc1
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8e1eb8a-9669-4763-9147-0f604b862dc1/pyspark-8de92b75-b9b8-450c-b225-6eb487b8ee5f
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:07:48 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:07:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:07:48 INFO ResourceUtils: ==============================================================
25/04/06 13:07:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:07:48 INFO ResourceUtils: ==============================================================
25/04/06 13:07:48 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:07:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:07:48 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:07:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:07:48 INFO SecurityManager: Changing view acls to: root
25/04/06 13:07:48 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:07:48 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:07:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:07:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:07:49 INFO Utils: Successfully started service 'sparkDriver' on port 46855.
25/04/06 13:07:49 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:07:49 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:07:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:07:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-654311f3-1713-4450-b558-82c8fe779814
25/04/06 13:07:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:07:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:07:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:07:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:07:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130749-0020
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41837.
25/04/06 13:07:49 INFO NettyBlockTransferService: Server created on f2a344a33cdc:41837
25/04/06 13:07:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:07:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:41837 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/2 is now RUNNING
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/1 is now RUNNING
25/04/06 13:07:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/0 is now RUNNING
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:07:50 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:07:50 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:07:51 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:57874) with ID 1,  ResourceProfileId 0
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:60844) with ID 2,  ResourceProfileId 0
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:50498) with ID 0,  ResourceProfileId 0
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:43601 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 43601, None)
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46879 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46879, None)
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38399 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38399, None)
25/04/06 13:07:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:07:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:07:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:07:51 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:07:51 INFO DAGScheduler: Missing parents: List()
25/04/06 13:07:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:07:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:07:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:07:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:41837 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:07:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:07:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:07:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:07:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:43601 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:07:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:07:53 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.236 s
25/04/06 13:07:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:07:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:07:53 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.267792 s
25/04/06 13:07:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:41837 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:43601 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:07:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:07:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:07:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:54 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1567c1e9-f0bb-4685-a85a-0caa6d208a97, clientType=HIVECLI]
25/04/06 13:07:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:07:54 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:07:54 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:07:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:07:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:07:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:07:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:07:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:07:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:07:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:07:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO CodeGenerator: Code generated in 151.33813 ms
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:41837 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:07:55 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:07:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:07:55 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:07:55 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:07:55 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:07:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:07:55 INFO DAGScheduler: Missing parents: List()
25/04/06 13:07:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.4 KiB, free 365.7 MiB)
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:41837 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:07:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:07:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:07:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:07:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:38399 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:07:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:38399 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:07:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1938 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:07:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:07:57 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.965 s
25/04/06 13:07:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:07:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:07:57 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.973030 s
25/04/06 13:07:57 INFO FileFormatWriter: Start to commit write Job 35b4ab55-b30f-4896-9d82-6057d0d3afda.
25/04/06 13:07:57 INFO FileFormatWriter: Write Job 35b4ab55-b30f-4896-9d82-6057d0d3afda committed. Elapsed time: 35 ms.
25/04/06 13:07:57 INFO FileFormatWriter: Finished processing stats for write job 35b4ab55-b30f-4896-9d82-6057d0d3afda.
25/04/06 13:07:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:07:57 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:07:57 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:07:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:07:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:07:57 INFO MemoryStore: MemoryStore cleared
25/04/06 13:07:57 INFO BlockManager: BlockManager stopped
25/04/06 13:07:57 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:07:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:07:57 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:07:57 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-de9ce325-954f-4709-acd9-a96e3a9023a7
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-23cbc119-a989-4510-8778-faa9e0a80137/pyspark-e7a2cca9-3f06-41c9-8898-547b7640c599
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-23cbc119-a989-4510-8778-faa9e0a80137
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:10:09 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:10:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:10:09 INFO ResourceUtils: ==============================================================
25/04/06 13:10:09 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:10:09 INFO ResourceUtils: ==============================================================
25/04/06 13:10:09 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:10:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:10:09 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:10:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:10:09 INFO SecurityManager: Changing view acls to: root
25/04/06 13:10:09 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:10:09 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:10:09 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:10:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:10:09 INFO Utils: Successfully started service 'sparkDriver' on port 35009.
25/04/06 13:10:09 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:10:09 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:10:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:10:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:10:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:10:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e48f12f-3d76-49f8-a9a9-8abf2a5dec5f
25/04/06 13:10:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:10:09 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:10:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:10:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:10:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131010-0022
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33291.
25/04/06 13:10:10 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33291
25/04/06 13:10:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:10:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33291 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/0 is now RUNNING
25/04/06 13:10:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/1 is now RUNNING
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/2 is now RUNNING
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:10:10 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:10:10 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:10:11 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:44952) with ID 1,  ResourceProfileId 0
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:49880) with ID 2,  ResourceProfileId 0
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46778) with ID 0,  ResourceProfileId 0
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:42355 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 42355, None)
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35099 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35099, None)
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:46375 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 46375, None)
25/04/06 13:10:12 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:10:12 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:10:12 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:10:12 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:10:12 INFO DAGScheduler: Missing parents: List()
25/04/06 13:10:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:10:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:10:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:10:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33291 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:10:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:10:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:10:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:10:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:42355 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1140 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:10:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:10:13 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.230 s
25/04/06 13:10:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:10:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:10:13 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.260209 s
25/04/06 13:10:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33291 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:42355 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:14 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:10:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:10:15 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:10:15 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7043ca30-39bb-47e7-8348-17641a78ceef, clientType=HIVECLI]
25/04/06 13:10:15 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:10:15 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:10:15 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:10:15 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:10:15 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:10:15 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:10:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO CodeGenerator: Code generated in 154.370682 ms
25/04/06 13:10:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:10:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:10:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:33291 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:10:15 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:10:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:10:16 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:10:16 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:10:16 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:10:16 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:10:16 INFO DAGScheduler: Missing parents: List()
25/04/06 13:10:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:10:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.4 KiB, free 365.7 MiB)
25/04/06 13:10:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:10:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:33291 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:10:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:10:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:10:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:10:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:46375 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:10:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:46375 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:10:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1954 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:10:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:10:17 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.981 s
25/04/06 13:10:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:10:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:10:17 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.987834 s
25/04/06 13:10:17 INFO FileFormatWriter: Start to commit write Job e9e479ca-d252-48cc-9cfb-6211f022dcac.
25/04/06 13:10:18 INFO FileFormatWriter: Write Job e9e479ca-d252-48cc-9cfb-6211f022dcac committed. Elapsed time: 37 ms.
25/04/06 13:10:18 INFO FileFormatWriter: Finished processing stats for write job e9e479ca-d252-48cc-9cfb-6211f022dcac.
25/04/06 13:10:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:10:18 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:10:18 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:10:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:10:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:10:18 INFO MemoryStore: MemoryStore cleared
25/04/06 13:10:18 INFO BlockManager: BlockManager stopped
25/04/06 13:10:18 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:10:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:10:18 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:10:18 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-d91908a2-d721-41c4-a0e8-8652a7734671
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f20f00c3-488d-44ee-b3d9-5415de5ec34d
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f20f00c3-488d-44ee-b3d9-5415de5ec34d/pyspark-cffad3ff-a994-4946-a004-09626ff18cc9
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:12:15 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:12:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:12:15 INFO ResourceUtils: ==============================================================
25/04/06 13:12:15 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:12:15 INFO ResourceUtils: ==============================================================
25/04/06 13:12:15 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:12:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:12:15 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:12:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:12:15 INFO SecurityManager: Changing view acls to: root
25/04/06 13:12:15 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:12:15 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:12:15 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:12:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:12:15 INFO Utils: Successfully started service 'sparkDriver' on port 33079.
25/04/06 13:12:15 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:12:15 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:12:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:12:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-955635f7-f4ca-418b-b79a-746d90f08ec9
25/04/06 13:12:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:12:15 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:12:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:12:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:12:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131215-0024
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35135.
25/04/06 13:12:15 INFO NettyBlockTransferService: Server created on f2a344a33cdc:35135
25/04/06 13:12:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:12:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:35135 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/2 is now RUNNING
25/04/06 13:12:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/0 is now RUNNING
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/1 is now RUNNING
25/04/06 13:12:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:12:16 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:12:16 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:12:17 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:33254) with ID 2,  ResourceProfileId 0
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46616) with ID 0,  ResourceProfileId 0
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42294) with ID 1,  ResourceProfileId 0
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35067 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35067, None)
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38023 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38023, None)
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45291 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45291, None)
25/04/06 13:12:18 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:12:18 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:12:18 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:12:18 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:12:18 INFO DAGScheduler: Missing parents: List()
25/04/06 13:12:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:12:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:12:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:12:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:35135 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:12:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:12:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:12:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:12:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:45291 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1200 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:12:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:12:19 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.293 s
25/04/06 13:12:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:12:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:12:19 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.326921 s
25/04/06 13:12:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:35135 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:45291 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:20 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:12:20 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:12:20 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:12:21 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5dc733ea-b055-4220-b6db-4fada04577dc, clientType=HIVECLI]
25/04/06 13:12:21 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:12:21 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:12:21 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:12:22 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:12:22 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:12:22 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:12:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:12:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:12:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:12:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:12:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO CodeGenerator: Code generated in 155.411127 ms
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:35135 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:12:22 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:12:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:12:22 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:12:22 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:12:22 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:12:22 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:12:22 INFO DAGScheduler: Missing parents: List()
25/04/06 13:12:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.6 KiB, free 365.6 MiB)
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:35135 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:12:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:12:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:12:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:12:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:35067 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:12:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:35067 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:12:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2108 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:12:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:12:24 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.140 s
25/04/06 13:12:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:12:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:12:24 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.149424 s
25/04/06 13:12:24 INFO FileFormatWriter: Start to commit write Job f8790279-2a41-4dee-99d6-4d0f9ea85016.
25/04/06 13:12:24 INFO FileFormatWriter: Write Job f8790279-2a41-4dee-99d6-4d0f9ea85016 committed. Elapsed time: 38 ms.
25/04/06 13:12:24 INFO FileFormatWriter: Finished processing stats for write job f8790279-2a41-4dee-99d6-4d0f9ea85016.
25/04/06 13:12:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:12:24 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:12:24 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:12:24 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:12:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:12:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:12:24 INFO MemoryStore: MemoryStore cleared
25/04/06 13:12:24 INFO BlockManager: BlockManager stopped
25/04/06 13:12:24 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:12:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:12:24 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:12:25 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-5500e226-7bd5-4231-8c2b-9c9c3215b00f/pyspark-787f90cb-4450-4fc7-b725-3660a8855e9d
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-5500e226-7bd5-4231-8c2b-9c9c3215b00f
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-d8d6670a-1ba1-44dc-aafa-e68801a80a27
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:16:44 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:16:44 INFO ResourceUtils: ==============================================================
25/04/06 13:16:44 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:16:44 INFO ResourceUtils: ==============================================================
25/04/06 13:16:44 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:16:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:16:44 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:16:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:16:44 INFO SecurityManager: Changing view acls to: root
25/04/06 13:16:44 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:16:44 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:16:44 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:16:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:16:44 INFO Utils: Successfully started service 'sparkDriver' on port 39505.
25/04/06 13:16:44 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:16:44 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:16:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:16:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21b50c04-c8a9-4714-b2e7-a42ca8be9ac4
25/04/06 13:16:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:16:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:16:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:16:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:16:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131644-0026
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45917.
25/04/06 13:16:44 INFO NettyBlockTransferService: Server created on f2a344a33cdc:45917
25/04/06 13:16:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:16:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:45917 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/1 is now RUNNING
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/2 is now RUNNING
25/04/06 13:16:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/0 is now RUNNING
25/04/06 13:16:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:16:45 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:16:45 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:16:46 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:60942) with ID 2,  ResourceProfileId 0
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:45218) with ID 0,  ResourceProfileId 0
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48424) with ID 1,  ResourceProfileId 0
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45121 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45121, None)
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:36143 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 36143, None)
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46643 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46643, None)
25/04/06 13:16:47 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:16:47 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:16:47 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:16:47 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:16:47 INFO DAGScheduler: Missing parents: List()
25/04/06 13:16:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:16:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:16:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:16:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:45917 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:16:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:16:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:16:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:16:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:45121 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1172 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:16:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:16:48 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.273 s
25/04/06 13:16:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:16:48 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.304731 s
25/04/06 13:16:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:45917 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:45121 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:16:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:16:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:16:50 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0c32d0c7-2a40-45d4-8828-ff8af13bb8d9, clientType=HIVECLI]
25/04/06 13:16:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:16:50 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:16:50 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:16:50 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:16:50 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:16:50 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:16:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO CodeGenerator: Code generated in 157.693995 ms
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:16:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:45917 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:16:50 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:16:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:16:50 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:16:50 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:16:50 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:16:50 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:16:50 INFO DAGScheduler: Missing parents: List()
25/04/06 13:16:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.6 KiB, free 365.6 MiB)
25/04/06 13:16:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:45917 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:16:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:16:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:16:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:16:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:16:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46643 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:16:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46643 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:16:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2022 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:16:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:16:52 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.051 s
25/04/06 13:16:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:16:52 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.059132 s
25/04/06 13:16:52 INFO FileFormatWriter: Start to commit write Job 9d0b270c-0ecf-4c57-a671-ef228191c01b.
25/04/06 13:16:53 INFO FileFormatWriter: Write Job 9d0b270c-0ecf-4c57-a671-ef228191c01b committed. Elapsed time: 37 ms.
25/04/06 13:16:53 INFO FileFormatWriter: Finished processing stats for write job 9d0b270c-0ecf-4c57-a671-ef228191c01b.
25/04/06 13:16:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:16:53 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:16:53 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:16:53 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:16:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:16:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:16:53 INFO MemoryStore: MemoryStore cleared
25/04/06 13:16:53 INFO BlockManager: BlockManager stopped
25/04/06 13:16:53 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:16:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:16:53 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:16:53 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d96d674-3b5d-4b8b-b06e-b1c297058e34
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a9de192-ed8a-4590-aa23-dcb7afacd7ac/pyspark-1a229d69-2180-4369-b799-7e46bf2dd1a1
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a9de192-ed8a-4590-aa23-dcb7afacd7ac
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:17:45 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:17:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:17:45 INFO ResourceUtils: ==============================================================
25/04/06 13:17:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:17:45 INFO ResourceUtils: ==============================================================
25/04/06 13:17:45 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:17:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:17:45 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:17:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:17:45 INFO SecurityManager: Changing view acls to: root
25/04/06 13:17:45 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:17:45 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:17:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:17:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:17:45 INFO Utils: Successfully started service 'sparkDriver' on port 37899.
25/04/06 13:17:45 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:17:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:17:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:17:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c4449845-a0eb-499a-8f81-47ae542a6152
25/04/06 13:17:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:17:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:17:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:17:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:17:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131745-0028
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44473.
25/04/06 13:17:45 INFO NettyBlockTransferService: Server created on f2a344a33cdc:44473
25/04/06 13:17:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:17:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/0 is now RUNNING
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/1 is now RUNNING
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:44473 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/2 is now RUNNING
25/04/06 13:17:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:17:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:17:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:17:47 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:51268) with ID 0,  ResourceProfileId 0
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:54460) with ID 1,  ResourceProfileId 0
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:47092) with ID 2,  ResourceProfileId 0
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:37913 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 37913, None)
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:37757 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 37757, None)
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33977 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33977, None)
25/04/06 13:17:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:17:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:17:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:17:48 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:17:48 INFO DAGScheduler: Missing parents: List()
25/04/06 13:17:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:17:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:17:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:17:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:44473 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:17:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:17:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:17:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:17:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:37757 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1231 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:17:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:17:49 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.338 s
25/04/06 13:17:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:17:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:17:49 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.373418 s
25/04/06 13:17:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:44473 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:37757 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:17:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:17:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:17:51 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=33cf079a-422e-4aa3-afaa-dab44295f06b, clientType=HIVECLI]
25/04/06 13:17:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:17:51 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:17:51 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:17:51 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:17:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:17:51 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:17:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:17:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:17:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:17:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:17:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:52 INFO CodeGenerator: Code generated in 158.096706 ms
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:44473 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:17:52 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:17:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:17:52 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:17:52 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:17:52 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:17:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:17:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:17:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:44473 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:17:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:17:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:17:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:17:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:33977 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:17:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:33977 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:17:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2106 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:17:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:17:54 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.134 s
25/04/06 13:17:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:17:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:17:54 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.141417 s
25/04/06 13:17:54 INFO FileFormatWriter: Start to commit write Job d4826559-82eb-423d-90a7-d5a4b40049f9.
25/04/06 13:17:54 INFO FileFormatWriter: Write Job d4826559-82eb-423d-90a7-d5a4b40049f9 committed. Elapsed time: 38 ms.
25/04/06 13:17:54 INFO FileFormatWriter: Finished processing stats for write job d4826559-82eb-423d-90a7-d5a4b40049f9.
25/04/06 13:17:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:17:54 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:17:54 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:17:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:17:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:17:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:17:54 INFO MemoryStore: MemoryStore cleared
25/04/06 13:17:54 INFO BlockManager: BlockManager stopped
25/04/06 13:17:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:17:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:17:54 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:17:54 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-64c4a5ca-a24e-4be2-acc2-e9a2e2892c4d
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-678eafb6-99df-4f25-b281-4022f21f3f9d
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-64c4a5ca-a24e-4be2-acc2-e9a2e2892c4d/pyspark-f741deb4-1878-428c-b8f5-ad5b69f0ea33
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:18:56 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:18:57 INFO ResourceUtils: ==============================================================
25/04/06 13:18:57 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:18:57 INFO ResourceUtils: ==============================================================
25/04/06 13:18:57 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:18:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:18:57 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:18:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:18:57 INFO SecurityManager: Changing view acls to: root
25/04/06 13:18:57 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:18:57 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:18:57 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:18:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:18:57 INFO Utils: Successfully started service 'sparkDriver' on port 43073.
25/04/06 13:18:57 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:18:57 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:18:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:18:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e3313658-c8b2-4c71-8b39-a22e9aae1fb9
25/04/06 13:18:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:18:57 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:18:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:18:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:18:57 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131857-0030
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33687.
25/04/06 13:18:57 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33687
25/04/06 13:18:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:18:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33687 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/2 is now RUNNING
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/1 is now RUNNING
25/04/06 13:18:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/0 is now RUNNING
25/04/06 13:18:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:18:58 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:18:58 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:18:59 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53548) with ID 2,  ResourceProfileId 0
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:53868) with ID 1,  ResourceProfileId 0
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:49734) with ID 0,  ResourceProfileId 0
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:35955 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 35955, None)
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33409 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33409, None)
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:44345 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 44345, None)
25/04/06 13:19:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:19:00 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:19:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:19:00 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:19:00 INFO DAGScheduler: Missing parents: List()
25/04/06 13:19:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:19:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:19:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:19:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33687 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:19:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:19:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:19:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:19:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:35955 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1227 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:19:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:19:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.329 s
25/04/06 13:19:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:19:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:19:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.363689 s
25/04/06 13:19:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33687 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:35955 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:02 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:19:02 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:19:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:19:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ca2f6b81-f1f3-4a66-a774-ef94ddabec24, clientType=HIVECLI]
25/04/06 13:19:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:19:03 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:19:03 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 33, in <module>
    spark.sql(f"""
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: DELETE is only supported with v2 tables.
25/04/06 13:19:03 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:19:03 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:19:03 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:19:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:19:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:19:03 INFO MemoryStore: MemoryStore cleared
25/04/06 13:19:03 INFO BlockManager: BlockManager stopped
25/04/06 13:19:03 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:19:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:19:03 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:19:03 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee1b1df6-57d8-49ce-866a-bbee595d7469
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee1b1df6-57d8-49ce-866a-bbee595d7469/pyspark-348935c1-b7b8-45ff-aaf8-4c0c03ab0c12
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d969da4-3833-4a2e-a693-dc3f52932485
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:23:17 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:23:17 INFO ResourceUtils: ==============================================================
25/04/06 13:23:17 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:23:17 INFO ResourceUtils: ==============================================================
25/04/06 13:23:17 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:23:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:23:17 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:23:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:23:17 INFO SecurityManager: Changing view acls to: root
25/04/06 13:23:17 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:23:17 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:23:17 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:23:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:23:17 INFO Utils: Successfully started service 'sparkDriver' on port 40763.
25/04/06 13:23:17 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:23:17 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:23:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:23:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73fe6ead-fcaa-4765-a8ba-52daf20b44b0
25/04/06 13:23:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:23:17 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:23:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:23:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:23:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132317-0032
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46749.
25/04/06 13:23:17 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46749
25/04/06 13:23:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:23:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46749 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/1 is now RUNNING
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/0 is now RUNNING
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/2 is now RUNNING
25/04/06 13:23:18 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:23:18 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:23:18 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:23:19 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:34254) with ID 2,  ResourceProfileId 0
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35964) with ID 1,  ResourceProfileId 0
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:40844) with ID 0,  ResourceProfileId 0
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38159 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38159, None)
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44693 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44693, None)
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:39431 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 39431, None)
25/04/06 13:23:19 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:23:19 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:23:19 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:23:19 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:23:19 INFO DAGScheduler: Missing parents: List()
25/04/06 13:23:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:23:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:23:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:23:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46749 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:23:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:23:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:23:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44693 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1167 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:23:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:23:21 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.257 s
25/04/06 13:23:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:23:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:23:21 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.288696 s
25/04/06 13:23:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46749 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44693 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:22 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:23:22 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:23:22 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:23:22 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:23:22 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:22 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:23:22 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:23:23 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=19123dc8-aafa-466d-bdb5-4678e01186be, clientType=HIVECLI]
25/04/06 13:23:23 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:23:23 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:23:23 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:23:23 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:23 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:23:23 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:23 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:23:23 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:23:23 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:23:23 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:23:23 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:23:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO CodeGenerator: Code generated in 151.061178 ms
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:46749 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:23:23 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:23:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:23:23 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:23:23 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:23:23 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:23:23 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:23:23 INFO DAGScheduler: Missing parents: List()
25/04/06 13:23:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:46749 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:23:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:23:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:23:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:23:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:44693 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:23:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:44693 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:23:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 948 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:23:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:23:24 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.975 s
25/04/06 13:23:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:23:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:23:24 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.982789 s
25/04/06 13:23:24 INFO FileFormatWriter: Start to commit write Job b6761802-64f6-4f1c-a016-269a1e40db40.
25/04/06 13:23:24 INFO FileFormatWriter: Write Job b6761802-64f6-4f1c-a016-269a1e40db40 committed. Elapsed time: 41 ms.
25/04/06 13:23:24 INFO FileFormatWriter: Finished processing stats for write job b6761802-64f6-4f1c-a016-269a1e40db40.
25/04/06 13:23:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:23:24 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:23:24 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:23:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:23:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:23:24 INFO MemoryStore: MemoryStore cleared
25/04/06 13:23:24 INFO BlockManager: BlockManager stopped
25/04/06 13:23:24 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:23:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:23:24 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:23:25 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab9a0bb-60ba-4fa5-89d6-d7792e4df5d4/pyspark-9b35c003-27ee-441d-8720-c66a017b6a83
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-687fb58c-43cd-4cd5-be5b-ad6b4c79213c
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab9a0bb-60ba-4fa5-89d6-d7792e4df5d4
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:24:20 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:24:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:24:20 INFO ResourceUtils: ==============================================================
25/04/06 13:24:20 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:24:20 INFO ResourceUtils: ==============================================================
25/04/06 13:24:20 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:24:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:24:20 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:24:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:24:20 INFO SecurityManager: Changing view acls to: root
25/04/06 13:24:20 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:24:20 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:24:20 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:24:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:24:20 INFO Utils: Successfully started service 'sparkDriver' on port 44183.
25/04/06 13:24:20 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:24:20 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:24:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:24:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:24:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:24:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1296992-3c43-47ce-80fe-aaaa1cf0e11d
25/04/06 13:24:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:24:20 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:24:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:24:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:24:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:24:21 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132421-0034
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40489.
25/04/06 13:24:21 INFO NettyBlockTransferService: Server created on f2a344a33cdc:40489
25/04/06 13:24:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:24:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:40489 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/1 is now RUNNING
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/2 is now RUNNING
25/04/06 13:24:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/0 is now RUNNING
25/04/06 13:24:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:24:21 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:24:21 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:24:22 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:38336) with ID 2,  ResourceProfileId 0
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:44988) with ID 0,  ResourceProfileId 0
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:52600) with ID 1,  ResourceProfileId 0
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:34337 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 34337, None)
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44103 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44103, None)
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40435 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40435, None)
25/04/06 13:24:23 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:24:23 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:24:23 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:24:23 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:24:23 INFO DAGScheduler: Missing parents: List()
25/04/06 13:24:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:24:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:24:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:24:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:40489 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:24:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:24:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:24:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44103 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:24:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:24:24 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.248 s
25/04/06 13:24:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:24:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:24:24 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.279528 s
25/04/06 13:24:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:40489 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44103 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:24:25 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:24:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:24:25 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:24:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:25 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:24:25 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:24:26 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3a230f76-b4bd-46ed-95a0-7f56756b0c6b, clientType=HIVECLI]
25/04/06 13:24:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:24:26 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:24:26 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:24:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:26 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:24:26 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:26 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:24:26 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:24:26 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:24:26 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:24:26 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:24:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:27 INFO CodeGenerator: Code generated in 156.68444 ms
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:40489 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:24:27 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:24:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:24:27 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:24:27 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:24:27 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:24:27 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:24:27 INFO DAGScheduler: Missing parents: List()
25/04/06 13:24:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:40489 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:24:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:24:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:24:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:24:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:40435 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:24:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:40435 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:24:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2105 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:24:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:24:29 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.134 s
25/04/06 13:24:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:24:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:24:29 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.143177 s
25/04/06 13:24:29 INFO FileFormatWriter: Start to commit write Job 519c1952-9e3d-4aca-a5d0-e608d5145e4f.
25/04/06 13:24:29 INFO FileFormatWriter: Write Job 519c1952-9e3d-4aca-a5d0-e608d5145e4f committed. Elapsed time: 43 ms.
25/04/06 13:24:29 INFO FileFormatWriter: Finished processing stats for write job 519c1952-9e3d-4aca-a5d0-e608d5145e4f.
25/04/06 13:24:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:24:29 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:24:29 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:24:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:24:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:24:29 INFO MemoryStore: MemoryStore cleared
25/04/06 13:24:29 INFO BlockManager: BlockManager stopped
25/04/06 13:24:29 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:24:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:24:29 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:24:29 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-982fdc94-5bae-4222-88e0-98ddfd27564d
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f3aef6a-a780-4aae-b75c-7ef6d9fc5c4d
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-982fdc94-5bae-4222-88e0-98ddfd27564d/pyspark-d632337f-cb27-4995-9fce-edfa4eef562a
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:25:30 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:25:30 INFO ResourceUtils: ==============================================================
25/04/06 13:25:30 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:25:30 INFO ResourceUtils: ==============================================================
25/04/06 13:25:30 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:25:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:25:30 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:25:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:25:31 INFO SecurityManager: Changing view acls to: root
25/04/06 13:25:31 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:25:31 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:25:31 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:25:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:25:31 INFO Utils: Successfully started service 'sparkDriver' on port 42601.
25/04/06 13:25:31 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:25:31 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:25:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:25:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e9215b4-ee31-4cb9-a9b5-32cfd92a48d9
25/04/06 13:25:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:25:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:25:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:25:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:25:31 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132531-0036
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46437.
25/04/06 13:25:31 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46437
25/04/06 13:25:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:25:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/1 is now RUNNING
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/0 is now RUNNING
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46437 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/2 is now RUNNING
25/04/06 13:25:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:25:32 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:25:32 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:25:33 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43198) with ID 2,  ResourceProfileId 0
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:45850) with ID 1,  ResourceProfileId 0
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:39758) with ID 0,  ResourceProfileId 0
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44429 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44429, None)
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46611 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46611, None)
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39853 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39853, None)
25/04/06 13:25:33 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:25:33 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:25:33 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:25:33 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:25:33 INFO DAGScheduler: Missing parents: List()
25/04/06 13:25:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:25:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:25:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:25:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46437 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:25:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:25:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:25:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:25:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44429 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1163 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:25:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:25:35 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.253 s
25/04/06 13:25:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:25:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:25:35 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.284884 s
25/04/06 13:25:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46437 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44429 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:25:36 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:25:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:25:36 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:25:36 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:36 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:25:36 INFO metastore: Connected to metastore.
25/04/06 13:25:36 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:25:37 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ef41f7ea-f7e3-47a2-b876-838d2c33b79e, clientType=HIVECLI]
25/04/06 13:25:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:25:37 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:25:37 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:25:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:25:37 INFO metastore: Connected to metastore.
25/04/06 13:25:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:37 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:25:37 INFO metastore: Connected to metastore.
25/04/06 13:25:37 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:25:37 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:25:37 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:25:37 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:25:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO CodeGenerator: Code generated in 146.889585 ms
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:46437 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:25:37 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:25:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:25:37 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:25:37 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:25:37 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:25:37 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:25:37 INFO DAGScheduler: Missing parents: List()
25/04/06 13:25:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:46437 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:25:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:25:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:25:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:25:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46611 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:25:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46611 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:25:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2049 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:25:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:25:39 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.075 s
25/04/06 13:25:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:25:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:25:39 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.082457 s
25/04/06 13:25:39 INFO FileFormatWriter: Start to commit write Job de4f512b-bf0a-43c9-891e-f30c91a84983.
25/04/06 13:25:39 INFO FileFormatWriter: Write Job de4f512b-bf0a-43c9-891e-f30c91a84983 committed. Elapsed time: 39 ms.
25/04/06 13:25:39 INFO FileFormatWriter: Finished processing stats for write job de4f512b-bf0a-43c9-891e-f30c91a84983.
25/04/06 13:25:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:25:39 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:25:39 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:25:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:25:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:25:39 INFO MemoryStore: MemoryStore cleared
25/04/06 13:25:39 INFO BlockManager: BlockManager stopped
25/04/06 13:25:39 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:25:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:25:39 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:25:39 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5533134-2070-492c-922d-80429c094985
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f2a5a59-a49c-47b1-8937-7b8f7e54214f
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5533134-2070-492c-922d-80429c094985/pyspark-924676ab-cd63-4e18-8aa8-b1175f8b4d57
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:29:46 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:29:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:29:46 INFO ResourceUtils: ==============================================================
25/04/06 13:29:46 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:29:46 INFO ResourceUtils: ==============================================================
25/04/06 13:29:46 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:29:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:29:46 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:29:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:29:46 INFO SecurityManager: Changing view acls to: root
25/04/06 13:29:46 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:29:46 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:29:46 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:29:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:29:46 INFO Utils: Successfully started service 'sparkDriver' on port 36911.
25/04/06 13:29:46 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:29:46 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:29:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:29:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:29:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:29:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7113cd49-8a06-4166-8f97-c45728fcd6b7
25/04/06 13:29:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:29:46 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:29:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:29:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:29:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:29:47 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132947-0040
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42863.
25/04/06 13:29:47 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42863
25/04/06 13:29:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:29:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42863 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/1 is now RUNNING
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/2 is now RUNNING
25/04/06 13:29:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/0 is now RUNNING
25/04/06 13:29:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:29:47 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:29:47 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:29:49 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:36604) with ID 1,  ResourceProfileId 0
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:58386) with ID 2,  ResourceProfileId 0
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:38226) with ID 0,  ResourceProfileId 0
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43579 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43579, None)
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45721 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45721, None)
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:44677 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 44677, None)
25/04/06 13:29:49 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:29:49 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:29:49 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:29:49 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:29:49 INFO DAGScheduler: Missing parents: List()
25/04/06 13:29:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:29:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:29:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:29:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42863 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:29:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:29:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:29:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:29:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:43579 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1227 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:29:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:29:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.326 s
25/04/06 13:29:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:29:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:29:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.361409 s
25/04/06 13:29:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42863 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:43579 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:52 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:29:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:29:52 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:29:52 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:52 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c320ce46-656c-48bd-8726-e06e01fd3822, clientType=HIVECLI]
25/04/06 13:29:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:29:52 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:29:52 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:29:53 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:29:53 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:29:53 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:29:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO CodeGenerator: Code generated in 159.683879 ms
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:42863 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:29:53 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:29:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:29:53 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:29:53 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:29:53 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:29:53 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:29:53 INFO DAGScheduler: Missing parents: List()
25/04/06 13:29:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:42863 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:29:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:29:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:29:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:29:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:45721 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:29:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:45721 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:29:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2077 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:29:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:29:55 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.108 s
25/04/06 13:29:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:29:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:29:55 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.116639 s
25/04/06 13:29:55 INFO FileFormatWriter: Start to commit write Job 5ab5a544-c67e-478b-b0a7-7832f820366e.
25/04/06 13:29:55 INFO FileFormatWriter: Write Job 5ab5a544-c67e-478b-b0a7-7832f820366e committed. Elapsed time: 42 ms.
25/04/06 13:29:55 INFO FileFormatWriter: Finished processing stats for write job 5ab5a544-c67e-478b-b0a7-7832f820366e.
25/04/06 13:29:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:29:55 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:29:55 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:29:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:29:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:29:55 INFO MemoryStore: MemoryStore cleared
25/04/06 13:29:55 INFO BlockManager: BlockManager stopped
25/04/06 13:29:55 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:29:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:29:55 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:29:55 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9750c10-a3ef-4249-b9c6-7b5be5310184
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-0fe216c4-474c-41c3-8c7d-58b0a4449085
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9750c10-a3ef-4249-b9c6-7b5be5310184/pyspark-1c917937-d29d-42a2-a806-d85da8cd08e6
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:33:31 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:33:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:33:31 INFO ResourceUtils: ==============================================================
25/04/06 13:33:31 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:33:31 INFO ResourceUtils: ==============================================================
25/04/06 13:33:31 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:33:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:33:31 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:33:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:33:31 INFO SecurityManager: Changing view acls to: root
25/04/06 13:33:31 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:33:31 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:33:31 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:33:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:33:31 INFO Utils: Successfully started service 'sparkDriver' on port 40615.
25/04/06 13:33:31 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:33:31 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:33:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:33:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:33:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:33:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f685e956-6910-4597-99be-f3186171b70c
25/04/06 13:33:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:33:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:33:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:33:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:33:32 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406133332-0044
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34859.
25/04/06 13:33:32 INFO NettyBlockTransferService: Server created on f2a344a33cdc:34859
25/04/06 13:33:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:33:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:34859 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/1 is now RUNNING
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/0 is now RUNNING
25/04/06 13:33:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/2 is now RUNNING
25/04/06 13:33:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:33:32 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:33:32 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:33:34 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:36228) with ID 0,  ResourceProfileId 0
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:59670) with ID 2,  ResourceProfileId 0
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42276) with ID 1,  ResourceProfileId 0
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:37841 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 37841, None)
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:42991 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 42991, None)
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46017 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46017, None)
25/04/06 13:33:34 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:33:34 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:33:34 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:33:34 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:33:34 INFO DAGScheduler: Missing parents: List()
25/04/06 13:33:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:33:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:33:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:33:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:34859 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:33:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:33:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:33:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:33:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:42991 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1191 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:33:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:33:35 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.297 s
25/04/06 13:33:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:33:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:33:35 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.334685 s
25/04/06 13:33:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:34859 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:42991 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:37 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:33:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:33:37 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:33:37 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:37 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d7bf0853-c551-4ef2-95ef-2667ba96839c, clientType=HIVECLI]
25/04/06 13:33:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:33:37 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:33:37 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:38 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:33:38 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:33:38 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:33:38 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:33:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO CodeGenerator: Code generated in 146.441413 ms
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:34859 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:33:38 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:33:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:33:38 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:33:38 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:33:38 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:33:38 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:33:38 INFO DAGScheduler: Missing parents: List()
25/04/06 13:33:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:34859 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:33:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:33:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:33:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:33:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46017 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:33:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46017 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:33:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1969 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:33:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:33:40 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.995 s
25/04/06 13:33:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:33:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:33:40 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.002401 s
25/04/06 13:33:40 INFO FileFormatWriter: Start to commit write Job 27ca2f65-847d-4e5e-968f-a7a58c1f392b.
25/04/06 13:33:40 INFO FileFormatWriter: Write Job 27ca2f65-847d-4e5e-968f-a7a58c1f392b committed. Elapsed time: 36 ms.
25/04/06 13:33:40 INFO FileFormatWriter: Finished processing stats for write job 27ca2f65-847d-4e5e-968f-a7a58c1f392b.
25/04/06 13:33:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:33:40 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:33:40 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:33:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:33:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:33:40 INFO MemoryStore: MemoryStore cleared
25/04/06 13:33:40 INFO BlockManager: BlockManager stopped
25/04/06 13:33:40 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:33:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:33:40 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:33:41 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0e09ab28-3d8c-44be-bd64-5cc5e863bcb9/pyspark-9af375ff-6f56-42ba-866c-7ed0fe24ce77
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0e09ab28-3d8c-44be-bd64-5cc5e863bcb9
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce5d68a3-3da9-442e-8008-902d7778f9ca
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:40:53 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:40:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:40:53 INFO ResourceUtils: ==============================================================
25/04/06 13:40:53 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:40:53 INFO ResourceUtils: ==============================================================
25/04/06 13:40:53 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:40:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:40:53 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:40:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:40:54 INFO SecurityManager: Changing view acls to: root
25/04/06 13:40:54 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:40:54 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:40:54 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:40:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:40:54 INFO Utils: Successfully started service 'sparkDriver' on port 42449.
25/04/06 13:40:54 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:40:54 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:40:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:40:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-814872a6-4921-40ce-a3de-1e165df791c6
25/04/06 13:40:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:40:54 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:40:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:40:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:40:54 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134054-0047
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42007.
25/04/06 13:40:54 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42007
25/04/06 13:40:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:40:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42007 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/1 is now RUNNING
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/2 is now RUNNING
25/04/06 13:40:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/0 is now RUNNING
25/04/06 13:40:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:40:55 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:40:55 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:40:56 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53620) with ID 2,  ResourceProfileId 0
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:34068) with ID 0,  ResourceProfileId 0
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:37924) with ID 1,  ResourceProfileId 0
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:40187 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 40187, None)
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:34575 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 34575, None)
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42639 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42639, None)
25/04/06 13:40:56 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:40:56 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:40:56 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:40:56 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:40:56 INFO DAGScheduler: Missing parents: List()
25/04/06 13:40:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:40:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:40:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 13:40:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42007 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:40:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:40:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:40:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:40:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:40187 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1147 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:40:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:40:57 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.239 s
25/04/06 13:40:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:40:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:40:57 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.270709 s
25/04/06 13:40:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42007 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:40187 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:40:59 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:40:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:40:59 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:40:59 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:40:59 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:40:59 INFO metastore: Connected to metastore.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 20, in <module>
    spark.sql(f"""
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Table or view not found: unprocessedlogs; line 2 pos 15;
'DropTable false, false
+- 'UnresolvedTableOrView [unprocessedlogs], DROP TABLE, true

25/04/06 13:40:59 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:40:59 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:40:59 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:40:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:40:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:40:59 INFO MemoryStore: MemoryStore cleared
25/04/06 13:40:59 INFO BlockManager: BlockManager stopped
25/04/06 13:40:59 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:40:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:40:59 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:40:59 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd05d8e0-e495-4b9e-b225-b0a3b0fb5356
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd05d8e0-e495-4b9e-b225-b0a3b0fb5356/pyspark-91112c66-b23a-4be3-bfac-2a58ce7514eb
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-027a23ba-ac81-4dfc-9537-3730fb37dbf6
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:44:11 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:44:11 INFO ResourceUtils: ==============================================================
25/04/06 13:44:11 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:44:11 INFO ResourceUtils: ==============================================================
25/04/06 13:44:11 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:44:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:44:11 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:44:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:44:11 INFO SecurityManager: Changing view acls to: root
25/04/06 13:44:11 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:44:11 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:44:11 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:44:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:44:11 INFO Utils: Successfully started service 'sparkDriver' on port 36235.
25/04/06 13:44:11 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:44:11 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:44:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:44:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:44:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:44:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f6a2156-2f69-476b-8f73-c7d3b9aa4209
25/04/06 13:44:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:44:12 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:44:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:44:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:44:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134412-0051
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39627.
25/04/06 13:44:12 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39627
25/04/06 13:44:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:44:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39627 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/0 is now RUNNING
25/04/06 13:44:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/1 is now RUNNING
25/04/06 13:44:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/2 is now RUNNING
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:44:12 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:44:12 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:44:14 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:39954) with ID 2,  ResourceProfileId 0
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:37496) with ID 1,  ResourceProfileId 0
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:35002) with ID 0,  ResourceProfileId 0
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42943 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42943, None)
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:42831 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 42831, None)
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:40795 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 40795, None)
25/04/06 13:44:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:44:14 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:44:14 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:44:14 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:44:14 INFO DAGScheduler: Missing parents: List()
25/04/06 13:44:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:44:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:44:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:44:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39627 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:44:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:44:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:44:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:44:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:42831 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1176 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:44:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:44:15 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.269 s
25/04/06 13:44:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:44:15 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.303375 s
25/04/06 13:44:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39627 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:42831 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:16 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:44:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:44:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:44:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:44:16 INFO MemoryStore: MemoryStore cleared
25/04/06 13:44:16 INFO BlockManager: BlockManager stopped
25/04/06 13:44:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:44:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:44:17 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:44:17 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0db0e36-fec2-4345-88b3-b31bf6ef36be
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0db0e36-fec2-4345-88b3-b31bf6ef36be/pyspark-c3082c1f-36dc-428f-87c6-bf213955b723
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-8da67680-2bbb-42e5-a666-76b06aaedc89
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:46:11 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:46:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:46:11 INFO ResourceUtils: ==============================================================
25/04/06 13:46:11 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:46:11 INFO ResourceUtils: ==============================================================
25/04/06 13:46:11 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:46:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:46:11 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:46:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:46:11 INFO SecurityManager: Changing view acls to: root
25/04/06 13:46:11 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:46:11 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:46:11 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:46:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:46:11 INFO Utils: Successfully started service 'sparkDriver' on port 40275.
25/04/06 13:46:11 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:46:11 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:46:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:46:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-74ccc77a-a695-448a-aebf-4da956d5556a
25/04/06 13:46:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:46:11 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:46:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:46:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:46:11 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134611-0055
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40665.
25/04/06 13:46:11 INFO NettyBlockTransferService: Server created on f2a344a33cdc:40665
25/04/06 13:46:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:46:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:40665 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/1 is now RUNNING
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/2 is now RUNNING
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/0 is now RUNNING
25/04/06 13:46:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:46:12 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:46:12 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:46:13 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53708) with ID 2,  ResourceProfileId 0
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:43120) with ID 0,  ResourceProfileId 0
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48760) with ID 1,  ResourceProfileId 0
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:46741 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 46741, None)
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45873 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 45873, None)
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:34339 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 34339, None)
25/04/06 13:46:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:46:13 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:46:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:46:13 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:46:13 INFO DAGScheduler: Missing parents: List()
25/04/06 13:46:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:46:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:46:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:46:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:40665 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:46:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:46:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:46:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:46:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:34339 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1194 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:46:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:46:15 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.283 s
25/04/06 13:46:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:46:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:46:15 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.314317 s
25/04/06 13:46:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:40665 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:34339 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:16 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:46:16 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:46:16 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:46:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=292c2066-ddab-43c0-a853-26f46b1c4104, clientType=HIVECLI]
25/04/06 13:46:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:46:17 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:46:17 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:46:17 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:46:17 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:46:17 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:46:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO CodeGenerator: Code generated in 155.999227 ms
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:46:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:40665 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:46:17 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:46:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:46:17 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:46:17 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:46:17 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:46:17 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:46:17 INFO DAGScheduler: Missing parents: List()
25/04/06 13:46:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:46:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:40665 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:46:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:46:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:46:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:46:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:45873 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:46:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:45873 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:46:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2021 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:46:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:46:19 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.051 s
25/04/06 13:46:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:46:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:46:19 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.060016 s
25/04/06 13:46:19 INFO FileFormatWriter: Start to commit write Job dab12bad-7bc3-40be-91ea-a1a17c781262.
25/04/06 13:46:20 INFO FileFormatWriter: Write Job dab12bad-7bc3-40be-91ea-a1a17c781262 committed. Elapsed time: 38 ms.
25/04/06 13:46:20 INFO FileFormatWriter: Finished processing stats for write job dab12bad-7bc3-40be-91ea-a1a17c781262.
25/04/06 13:46:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:46:20 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:46:20 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:46:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:46:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:46:20 INFO MemoryStore: MemoryStore cleared
25/04/06 13:46:20 INFO BlockManager: BlockManager stopped
25/04/06 13:46:20 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:46:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:46:20 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:46:20 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-5092ff7f-fec7-46a5-ad89-c8665fde40aa
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c9f29d0-60ed-41cc-a862-419a703f2690/pyspark-8416e3d5-610f-4a25-a6ee-f0c217efc461
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c9f29d0-60ed-41cc-a862-419a703f2690
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:47:07 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:47:07 INFO ResourceUtils: ==============================================================
25/04/06 13:47:07 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:47:07 INFO ResourceUtils: ==============================================================
25/04/06 13:47:07 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:47:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:47:07 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:47:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:47:07 INFO SecurityManager: Changing view acls to: root
25/04/06 13:47:07 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:47:07 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:47:07 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:47:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:47:07 INFO Utils: Successfully started service 'sparkDriver' on port 39047.
25/04/06 13:47:07 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:47:07 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:47:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:47:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:47:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8269376-cbc0-43cd-b0ad-8b9d2f50cd87
25/04/06 13:47:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:47:07 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:47:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:47:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134708-0058
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39023.
25/04/06 13:47:08 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39023
25/04/06 13:47:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:47:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39023 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/1 is now RUNNING
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/2 is now RUNNING
25/04/06 13:47:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/0 is now RUNNING
25/04/06 13:47:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:47:08 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:47:08 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:47:09 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/06 13:47:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:59254) with ID 2,  ResourceProfileId 0
25/04/06 13:47:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46346) with ID 0,  ResourceProfileId 0
25/04/06 13:47:10 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35458) with ID 1,  ResourceProfileId 0
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:32837 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 32837, None)
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:38421 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 38421, None)
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:38799 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 38799, None)
25/04/06 13:47:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:47:10 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:47:10 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:47:10 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:47:10 INFO DAGScheduler: Missing parents: List()
25/04/06 13:47:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:47:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:47:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:47:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39023 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:47:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:47:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:47:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:47:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:38421 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1165 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:47:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:47:11 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.256 s
25/04/06 13:47:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:47:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:47:11 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.287696 s
25/04/06 13:47:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39023 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:38421 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:12 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:47:12 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:47:13 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:47:13 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4ddb2889-4b66-48fc-b2a7-cc78f104c08e, clientType=HIVECLI]
25/04/06 13:47:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:47:13 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:47:13 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:47:13 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:47:13 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:47:13 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:47:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO CodeGenerator: Code generated in 150.005733 ms
25/04/06 13:47:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:47:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:47:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:39023 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:47:13 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:47:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:47:14 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:47:14 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:47:14 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:47:14 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:47:14 INFO DAGScheduler: Missing parents: List()
25/04/06 13:47:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:47:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:47:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:47:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:39023 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:47:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:47:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:47:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:47:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:47:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:32837 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:47:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:32837 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:47:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1971 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:47:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:47:16 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.002 s
25/04/06 13:47:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:47:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:47:16 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.009427 s
25/04/06 13:47:16 INFO FileFormatWriter: Start to commit write Job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93.
25/04/06 13:47:16 INFO FileFormatWriter: Write Job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93 committed. Elapsed time: 36 ms.
25/04/06 13:47:16 INFO FileFormatWriter: Finished processing stats for write job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93.
25/04/06 13:47:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:47:16 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:47:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:47:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:47:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:47:16 INFO MemoryStore: MemoryStore cleared
25/04/06 13:47:16 INFO BlockManager: BlockManager stopped
25/04/06 13:47:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:47:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:47:16 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:47:16 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-43612a98-3424-4484-9194-580101aae204
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-43612a98-3424-4484-9194-580101aae204/pyspark-1cb42c04-689b-4916-b35e-aa4fc42bd04d
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-d542e678-30be-4b06-97dd-a81d91170671
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:49:33 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:49:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:49:33 INFO ResourceUtils: ==============================================================
25/04/06 13:49:33 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:49:33 INFO ResourceUtils: ==============================================================
25/04/06 13:49:33 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:49:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:49:33 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:49:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:49:33 INFO SecurityManager: Changing view acls to: root
25/04/06 13:49:33 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:49:33 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:49:33 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:49:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:49:34 INFO Utils: Successfully started service 'sparkDriver' on port 41031.
25/04/06 13:49:34 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:49:34 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:49:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:49:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5857296a-89da-46d3-b18f-a872d8d8038f
25/04/06 13:49:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:49:34 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:49:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:49:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:49:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134934-0061
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36319.
25/04/06 13:49:34 INFO NettyBlockTransferService: Server created on f2a344a33cdc:36319
25/04/06 13:49:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:49:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:36319 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/2 is now RUNNING
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/0 is now RUNNING
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/1 is now RUNNING
25/04/06 13:49:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:49:35 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:49:35 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:49:36 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:41172) with ID 0,  ResourceProfileId 0
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43894) with ID 2,  ResourceProfileId 0
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58714) with ID 1,  ResourceProfileId 0
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39109 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39109, None)
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45185 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45185, None)
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35835 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35835, None)
25/04/06 13:49:36 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:49:36 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:49:36 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:49:36 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:49:36 INFO DAGScheduler: Missing parents: List()
25/04/06 13:49:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:49:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:49:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:49:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:36319 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:49:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:49:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:49:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:49:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:39109 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1173 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:49:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:49:37 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.264 s
25/04/06 13:49:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:49:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:49:37 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.294354 s
25/04/06 13:49:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:36319 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:39109 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:39 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:49:39 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:49:39 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:49:39 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:39 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=88b9c493-bf6f-4927-bb3e-da86e4546699, clientType=HIVECLI]
25/04/06 13:49:39 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:49:39 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:49:39 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:40 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:49:40 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:49:40 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:49:40 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:49:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO CodeGenerator: Code generated in 145.936806 ms
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:36319 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:49:40 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:49:40 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:49:40 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:49:40 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:49:40 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:49:40 INFO DAGScheduler: Missing parents: List()
25/04/06 13:49:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:36319 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:49:40 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:49:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:49:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:35835 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:49:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:35835 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:49:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2069 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:49:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:49:42 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.097 s
25/04/06 13:49:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:49:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:49:42 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.104480 s
25/04/06 13:49:42 INFO FileFormatWriter: Start to commit write Job 264b7ffb-59e2-4e23-87f5-4c548900e36f.
25/04/06 13:49:42 INFO FileFormatWriter: Write Job 264b7ffb-59e2-4e23-87f5-4c548900e36f committed. Elapsed time: 40 ms.
25/04/06 13:49:42 INFO FileFormatWriter: Finished processing stats for write job 264b7ffb-59e2-4e23-87f5-4c548900e36f.
25/04/06 13:49:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:49:42 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:49:42 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:49:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:49:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:49:42 INFO MemoryStore: MemoryStore cleared
25/04/06 13:49:42 INFO BlockManager: BlockManager stopped
25/04/06 13:49:42 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:49:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:49:42 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:49:42 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-233bc9fd-a6fd-43f7-8c67-b84e86adc0ce
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-96a80e19-e12f-4513-acc1-6e85f425306d/pyspark-d5d7164f-ea32-415e-89e1-934ea7bea272
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-96a80e19-e12f-4513-acc1-6e85f425306d
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:50:29 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:50:29 INFO ResourceUtils: ==============================================================
25/04/06 13:50:29 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:50:29 INFO ResourceUtils: ==============================================================
25/04/06 13:50:29 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:50:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:50:29 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:50:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:50:29 INFO SecurityManager: Changing view acls to: root
25/04/06 13:50:29 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:50:29 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:50:29 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:50:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:50:30 INFO Utils: Successfully started service 'sparkDriver' on port 38265.
25/04/06 13:50:30 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:50:30 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:50:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:50:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4fab1ea2-a647-41fb-b8d3-6ea10dc702b4
25/04/06 13:50:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:50:30 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:50:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:50:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:50:30 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135030-0064
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45865.
25/04/06 13:50:30 INFO NettyBlockTransferService: Server created on f2a344a33cdc:45865
25/04/06 13:50:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:50:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:45865 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/1 is now RUNNING
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/2 is now RUNNING
25/04/06 13:50:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/0 is now RUNNING
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:50:31 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:50:31 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:50:32 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:60274) with ID 0,  ResourceProfileId 0
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:52098) with ID 2,  ResourceProfileId 0
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42876) with ID 1,  ResourceProfileId 0
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39163 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39163, None)
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39677 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39677, None)
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44439 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44439, None)
25/04/06 13:50:32 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:50:32 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:50:32 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:50:32 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:50:32 INFO DAGScheduler: Missing parents: List()
25/04/06 13:50:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:50:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:50:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:50:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:45865 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:50:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:50:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:50:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:50:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:39677 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1169 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:50:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:50:34 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.263 s
25/04/06 13:50:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:50:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:50:34 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.296125 s
25/04/06 13:50:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:45865 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:39677 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:35 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:50:35 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:50:35 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:50:35 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:35 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7897d5ed-9c7e-4893-a1d5-c181fc882bc5, clientType=HIVECLI]
25/04/06 13:50:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:50:35 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:50:35 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:50:36 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:50:36 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:50:36 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:50:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:50:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:50:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:50:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:50:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO CodeGenerator: Code generated in 150.64367 ms
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:45865 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:50:36 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:50:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:50:36 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:50:36 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:50:36 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:50:36 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:50:36 INFO DAGScheduler: Missing parents: List()
25/04/06 13:50:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:45865 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:50:36 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:50:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:50:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:50:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:39163 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:50:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:39163 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:50:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2033 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:50:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:50:38 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.059 s
25/04/06 13:50:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:50:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:50:38 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.066270 s
25/04/06 13:50:38 INFO FileFormatWriter: Start to commit write Job 0eda766d-d2f7-419a-ba97-4006d52fc870.
25/04/06 13:50:38 INFO FileFormatWriter: Write Job 0eda766d-d2f7-419a-ba97-4006d52fc870 committed. Elapsed time: 38 ms.
25/04/06 13:50:38 INFO FileFormatWriter: Finished processing stats for write job 0eda766d-d2f7-419a-ba97-4006d52fc870.
25/04/06 13:50:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:50:38 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:50:38 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:50:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:50:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:50:38 INFO MemoryStore: MemoryStore cleared
25/04/06 13:50:38 INFO BlockManager: BlockManager stopped
25/04/06 13:50:38 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:50:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:50:38 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:50:38 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-68978a64-b98f-4231-af69-77024da6e4bb
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec385eb-5ec3-4280-a1b3-5eba92c4d659
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec385eb-5ec3-4280-a1b3-5eba92c4d659/pyspark-fc7fd61d-7923-4cff-9cd1-0e5e7c5ce301
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:51:45 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:51:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:51:45 INFO ResourceUtils: ==============================================================
25/04/06 13:51:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:51:45 INFO ResourceUtils: ==============================================================
25/04/06 13:51:45 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:51:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:51:45 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:51:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:51:45 INFO SecurityManager: Changing view acls to: root
25/04/06 13:51:45 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:51:45 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:51:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:51:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:51:45 INFO Utils: Successfully started service 'sparkDriver' on port 38103.
25/04/06 13:51:45 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:51:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:51:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:51:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:51:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:51:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2fef2795-4294-4862-8206-eeed811fe5d4
25/04/06 13:51:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:51:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:51:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:51:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:51:46 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135146-0067
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38661.
25/04/06 13:51:46 INFO NettyBlockTransferService: Server created on f2a344a33cdc:38661
25/04/06 13:51:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:51:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:38661 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/0 is now RUNNING
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/1 is now RUNNING
25/04/06 13:51:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/2 is now RUNNING
25/04/06 13:51:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:51:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:51:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:51:48 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:58324) with ID 0,  ResourceProfileId 0
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:36894) with ID 2,  ResourceProfileId 0
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35192) with ID 1,  ResourceProfileId 0
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39205 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39205, None)
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38387 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38387, None)
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40599 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40599, None)
25/04/06 13:51:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:51:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:51:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:51:48 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:51:48 INFO DAGScheduler: Missing parents: List()
25/04/06 13:51:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:51:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:51:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:51:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:38661 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:51:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:51:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:51:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:51:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:40599 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1221 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:51:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:51:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.330 s
25/04/06 13:51:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:51:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:51:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.365568 s
25/04/06 13:51:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:38661 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:40599 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:51:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:51:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:51:51 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:51:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:51:51 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0ee92ad9-53d3-4a85-a0d8-83c7504cb1f9, clientType=HIVECLI]
25/04/06 13:51:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:51:52 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:51:52 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:51:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:51:52 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:52 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:51:52 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:51:52 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:51:52 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:51:52 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:51:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:51:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:51:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO CodeGenerator: Code generated in 154.361292 ms
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:38661 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:51:52 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:51:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:51:52 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:51:52 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:51:52 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:51:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:51:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:51:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:38661 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:51:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:51:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:51:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:51:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:39205 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:51:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:39205 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:51:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2086 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:51:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:51:54 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.117 s
25/04/06 13:51:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:51:54 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.125195 s
25/04/06 13:51:54 INFO FileFormatWriter: Start to commit write Job ca7f7628-d041-4e7b-974f-2416903512b6.
25/04/06 13:51:54 INFO FileFormatWriter: Write Job ca7f7628-d041-4e7b-974f-2416903512b6 committed. Elapsed time: 40 ms.
25/04/06 13:51:54 INFO FileFormatWriter: Finished processing stats for write job ca7f7628-d041-4e7b-974f-2416903512b6.
25/04/06 13:51:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:51:54 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:51:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:51:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:51:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:51:54 INFO MemoryStore: MemoryStore cleared
25/04/06 13:51:54 INFO BlockManager: BlockManager stopped
25/04/06 13:51:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:51:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:51:54 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:51:55 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-28421016-aade-4b0f-9101-de87dff437d0
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-28421016-aade-4b0f-9101-de87dff437d0/pyspark-29c8e1b1-29f8-4850-b65e-229bcde877cb
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-9fe9a24a-042d-4b46-8f3d-0e60c5d78197
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:53:12 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:53:12 INFO ResourceUtils: ==============================================================
25/04/06 13:53:12 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:53:12 INFO ResourceUtils: ==============================================================
25/04/06 13:53:12 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:53:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:53:12 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:53:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:53:12 INFO SecurityManager: Changing view acls to: root
25/04/06 13:53:12 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:53:12 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:53:12 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:53:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:53:13 INFO Utils: Successfully started service 'sparkDriver' on port 43297.
25/04/06 13:53:13 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:53:13 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:53:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:53:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-52eeadb0-4d5b-41cb-b30c-8227f446e00e
25/04/06 13:53:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:53:13 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:53:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:53:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:53:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135313-0070
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34469.
25/04/06 13:53:13 INFO NettyBlockTransferService: Server created on f2a344a33cdc:34469
25/04/06 13:53:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:53:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:34469 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/0 is now RUNNING
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/1 is now RUNNING
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/2 is now RUNNING
25/04/06 13:53:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:53:14 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:53:14 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:53:15 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:41226) with ID 0,  ResourceProfileId 0
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58192) with ID 1,  ResourceProfileId 0
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:54352) with ID 2,  ResourceProfileId 0
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40089 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40089, None)
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41373 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41373, None)
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45365 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45365, None)
25/04/06 13:53:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:53:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:53:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:53:15 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:53:15 INFO DAGScheduler: Missing parents: List()
25/04/06 13:53:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:53:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:53:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:53:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:34469 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:53:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:53:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:53:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:53:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:40089 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1205 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:53:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:53:17 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.299 s
25/04/06 13:53:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:53:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:53:17 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.333482 s
25/04/06 13:53:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:34469 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:40089 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:53:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:53:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:53:18 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:53:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:53:18 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=04efa787-0f3a-47f4-ab19-0dac169e72c6, clientType=HIVECLI]
25/04/06 13:53:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:53:19 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:53:19 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:53:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:19 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:53:19 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:19 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:53:19 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:53:19 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:53:19 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:53:19 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:53:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:53:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:53:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:53:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:53:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO CodeGenerator: Code generated in 148.067268 ms
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:34469 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:53:19 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:53:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:53:19 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:53:19 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:53:19 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:53:19 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:53:19 INFO DAGScheduler: Missing parents: List()
25/04/06 13:53:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:34469 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:53:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:53:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:53:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:53:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:45365 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:53:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:45365 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:53:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2069 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:53:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:53:21 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.099 s
25/04/06 13:53:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:53:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:53:21 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.107527 s
25/04/06 13:53:21 INFO FileFormatWriter: Start to commit write Job 05fb17ee-f745-4c25-8ba8-db5e992b1e56.
25/04/06 13:53:21 INFO FileFormatWriter: Write Job 05fb17ee-f745-4c25-8ba8-db5e992b1e56 committed. Elapsed time: 42 ms.
25/04/06 13:53:21 INFO FileFormatWriter: Finished processing stats for write job 05fb17ee-f745-4c25-8ba8-db5e992b1e56.
25/04/06 13:53:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:53:21 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:53:21 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:53:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:53:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:53:21 INFO MemoryStore: MemoryStore cleared
25/04/06 13:53:21 INFO BlockManager: BlockManager stopped
25/04/06 13:53:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:53:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:53:21 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:53:22 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-076029e2-cd61-4aa0-9414-979aade8dabe
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-c261bd92-480c-40b0-b022-6e6c5a78e2ee
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-076029e2-cd61-4aa0-9414-979aade8dabe/pyspark-efa2a6eb-ed17-44eb-ba9c-8abddefa6536
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:57:50 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:57:50 INFO ResourceUtils: ==============================================================
25/04/06 13:57:50 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:57:50 INFO ResourceUtils: ==============================================================
25/04/06 13:57:50 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:57:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:57:50 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:57:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:57:50 INFO SecurityManager: Changing view acls to: root
25/04/06 13:57:50 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:57:50 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:57:50 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:57:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:57:50 INFO Utils: Successfully started service 'sparkDriver' on port 38825.
25/04/06 13:57:50 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:57:50 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:57:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:57:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:57:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:57:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-63d576f4-4142-4ec0-bc30-7fc77390485c
25/04/06 13:57:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:57:50 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:57:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:57:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:57:51 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135751-0072
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42283.
25/04/06 13:57:51 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42283
25/04/06 13:57:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:57:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42283 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/0 is now RUNNING
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/1 is now RUNNING
25/04/06 13:57:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/2 is now RUNNING
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:57:51 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:57:51 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:57:52 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:33252) with ID 1,  ResourceProfileId 0
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:48972) with ID 2,  ResourceProfileId 0
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:40692) with ID 0,  ResourceProfileId 0
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41879 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41879, None)
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:38331 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 38331, None)
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45773 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 45773, None)
25/04/06 13:57:53 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:57:53 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:57:53 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:57:53 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:57:53 INFO DAGScheduler: Missing parents: List()
25/04/06 13:57:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:57:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:57:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:57:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42283 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:57:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:57:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:57:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:57:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:38331 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1142 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:57:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:57:54 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.232 s
25/04/06 13:57:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:57:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:57:54 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.263166 s
25/04/06 13:57:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42283 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:38331 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:57:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:57:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:57:55 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:57:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:57:55 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:57:56 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=de8842e6-a95d-4562-8dea-e7d27ef0ba74, clientType=HIVECLI]
25/04/06 13:57:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:57:56 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:57:56 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:57:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:56 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:57:56 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:56 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:57:56 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:57:56 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:57:56 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:57:56 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:57:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:57 INFO CodeGenerator: Code generated in 153.567956 ms
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:42283 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:57:57 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:57:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:57:57 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:57:57 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:57:57 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:57:57 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:57:57 INFO DAGScheduler: Missing parents: List()
25/04/06 13:57:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:42283 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:57:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:57:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:38331 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:38331 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:57:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 934 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:57:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:57:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.961 s
25/04/06 13:57:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:57:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:57:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.968674 s
25/04/06 13:57:58 INFO FileFormatWriter: Start to commit write Job fc294250-f550-48ec-a3c0-08eaa3805a34.
25/04/06 13:57:58 INFO FileFormatWriter: Write Job fc294250-f550-48ec-a3c0-08eaa3805a34 committed. Elapsed time: 36 ms.
25/04/06 13:57:58 INFO FileFormatWriter: Finished processing stats for write job fc294250-f550-48ec-a3c0-08eaa3805a34.
25/04/06 13:57:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:57:58 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:57:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:57:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:57:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:57:58 INFO MemoryStore: MemoryStore cleared
25/04/06 13:57:58 INFO BlockManager: BlockManager stopped
25/04/06 13:57:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:57:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:57:58 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:57:58 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed699ff1-5d74-47c8-b538-7fe25346ec96/pyspark-1f50683a-8d63-4436-bea3-e10cbf943180
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9baa16f0-fb55-4119-bf62-38f508d49478
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed699ff1-5d74-47c8-b538-7fe25346ec96
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:59:49 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:59:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:59:50 INFO ResourceUtils: ==============================================================
25/04/06 13:59:50 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:59:50 INFO ResourceUtils: ==============================================================
25/04/06 13:59:50 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:59:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:59:50 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:59:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:59:50 INFO SecurityManager: Changing view acls to: root
25/04/06 13:59:50 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:59:50 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:59:50 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:59:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:59:50 INFO Utils: Successfully started service 'sparkDriver' on port 46005.
25/04/06 13:59:50 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:59:50 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:59:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:59:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9c94c68a-0ad8-4b65-b87b-0afcf32c96c5
25/04/06 13:59:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:59:50 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:59:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:59:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:59:50 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135950-0074
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33649.
25/04/06 13:59:50 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33649
25/04/06 13:59:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:59:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33649 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/0 is now RUNNING
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/2 is now RUNNING
25/04/06 13:59:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/1 is now RUNNING
25/04/06 13:59:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:59:51 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:59:51 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:59:52 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:55838) with ID 1,  ResourceProfileId 0
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:48552) with ID 2,  ResourceProfileId 0
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:56404) with ID 0,  ResourceProfileId 0
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39441 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39441, None)
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:38995 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 38995, None)
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:36719 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 36719, None)
25/04/06 13:59:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:59:52 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:59:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:59:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:59:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:59:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:59:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:59:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:59:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33649 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:59:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:59:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:59:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:59:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:38995 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1217 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:59:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:59:54 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.314 s
25/04/06 13:59:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:59:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:59:54 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.345683 s
25/04/06 13:59:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33649 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:38995 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:59:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:59:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:59:55 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:59:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:59:55 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:59:56 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f6b09ab3-6b1c-40bd-8ec2-4a64038c5c29, clientType=HIVECLI]
25/04/06 13:59:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:59:56 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:59:56 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:59:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:56 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:59:56 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:56 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:59:56 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:59:56 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:59:56 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:59:56 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:59:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:57 INFO CodeGenerator: Code generated in 156.402704 ms
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:33649 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:59:57 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:59:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:59:57 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:59:57 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:59:57 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:59:57 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:59:57 INFO DAGScheduler: Missing parents: List()
25/04/06 13:59:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:33649 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:59:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:59:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:59:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:59:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:38995 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:38995 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:59:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 977 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:59:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:59:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.003 s
25/04/06 13:59:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:59:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:59:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.010796 s
25/04/06 13:59:58 INFO FileFormatWriter: Start to commit write Job d66dc65a-4c85-4f3a-a461-d9bb655210c5.
25/04/06 13:59:58 INFO FileFormatWriter: Write Job d66dc65a-4c85-4f3a-a461-d9bb655210c5 committed. Elapsed time: 38 ms.
25/04/06 13:59:58 INFO FileFormatWriter: Finished processing stats for write job d66dc65a-4c85-4f3a-a461-d9bb655210c5.
25/04/06 13:59:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:59:58 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:59:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:59:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:59:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:59:58 INFO MemoryStore: MemoryStore cleared
25/04/06 13:59:58 INFO BlockManager: BlockManager stopped
25/04/06 13:59:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:59:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:59:58 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:59:58 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d6bdeeb-a188-499b-946c-4564a45681ac/pyspark-b1b14910-8eeb-4b5e-8ba9-9281954e1206
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d6bdeeb-a188-499b-946c-4564a45681ac
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-66d7e59e-3897-4775-b72f-d73fcee399b0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 14:23:35 INFO SparkContext: Running Spark version 3.2.2
25/04/06 14:23:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 14:23:35 INFO ResourceUtils: ==============================================================
25/04/06 14:23:35 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 14:23:35 INFO ResourceUtils: ==============================================================
25/04/06 14:23:35 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 14:23:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 14:23:35 INFO ResourceProfile: Limiting resource is cpu
25/04/06 14:23:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 14:23:35 INFO SecurityManager: Changing view acls to: root
25/04/06 14:23:35 INFO SecurityManager: Changing modify acls to: root
25/04/06 14:23:35 INFO SecurityManager: Changing view acls groups to: 
25/04/06 14:23:35 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 14:23:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 14:23:35 INFO Utils: Successfully started service 'sparkDriver' on port 33223.
25/04/06 14:23:35 INFO SparkEnv: Registering MapOutputTracker
25/04/06 14:23:35 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 14:23:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 14:23:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 14:23:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 14:23:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d91a5e7-80ff-49ae-ba0d-8dcd8b212b36
25/04/06 14:23:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 14:23:35 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 14:23:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 14:23:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 14:23:36 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406142336-0076
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39325.
25/04/06 14:23:36 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39325
25/04/06 14:23:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 14:23:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39325 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/2 is now RUNNING
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/1 is now RUNNING
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/0 is now RUNNING
25/04/06 14:23:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 14:23:36 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 14:23:36 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 14:23:37 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:59292) with ID 0,  ResourceProfileId 0
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43234) with ID 2,  ResourceProfileId 0
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40700) with ID 1,  ResourceProfileId 0
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40191 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40191, None)
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45179 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45179, None)
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:36303 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 36303, None)
25/04/06 14:23:38 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 14:23:38 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:23:38 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 14:23:38 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:23:38 INFO DAGScheduler: Missing parents: List()
25/04/06 14:23:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:23:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 14:23:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 14:23:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39325 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 14:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:23:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 14:23:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 14:23:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:40191 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1191 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:23:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 14:23:39 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.286 s
25/04/06 14:23:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 14:23:39 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.323137 s
25/04/06 14:23:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39325 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:40191 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:23:40 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 14:23:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:23:40 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 14:23:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:40 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:23:40 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 14:23:41 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a4124908-cc9d-432d-8e09-a72823c43e68, clientType=HIVECLI]
25/04/06 14:23:41 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 14:23:41 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 14:23:41 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 14:23:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:41 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:23:41 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:41 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 14:23:41 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 14:23:41 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 14:23:41 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 14:23:41 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 14:23:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO CodeGenerator: Code generated in 154.562926 ms
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:39325 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 14:23:42 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:23:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 14:23:42 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:23:42 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:23:42 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 14:23:42 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:23:42 INFO DAGScheduler: Missing parents: List()
25/04/06 14:23:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:39325 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:23:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 14:23:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:23:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 14:23:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:40191 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:23:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:40191 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 14:23:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 982 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:23:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 14:23:43 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.007 s
25/04/06 14:23:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:23:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 14:23:43 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.014978 s
25/04/06 14:23:43 INFO FileFormatWriter: Start to commit write Job 11b69889-b8b8-493a-a9b7-9f96f9b018f5.
25/04/06 14:23:43 INFO FileFormatWriter: Write Job 11b69889-b8b8-493a-a9b7-9f96f9b018f5 committed. Elapsed time: 36 ms.
25/04/06 14:23:43 INFO FileFormatWriter: Finished processing stats for write job 11b69889-b8b8-493a-a9b7-9f96f9b018f5.
25/04/06 14:23:43 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 14:23:43 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 14:23:43 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 14:23:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 14:23:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 14:23:43 INFO MemoryStore: MemoryStore cleared
25/04/06 14:23:43 INFO BlockManager: BlockManager stopped
25/04/06 14:23:43 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 14:23:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 14:23:43 INFO SparkContext: Successfully stopped SparkContext
25/04/06 14:23:43 INFO ShutdownHookManager: Shutdown hook called
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d51c49-7d1e-421b-918e-7d8aeb898606
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-3439da54-4ab5-454d-9a28-a6cb98a8c760/pyspark-660580b8-f79c-4dea-8133-6fb63e0ba834
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-3439da54-4ab5-454d-9a28-a6cb98a8c760
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 14:24:48 INFO SparkContext: Running Spark version 3.2.2
25/04/06 14:24:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 14:24:48 INFO ResourceUtils: ==============================================================
25/04/06 14:24:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 14:24:48 INFO ResourceUtils: ==============================================================
25/04/06 14:24:48 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 14:24:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 14:24:48 INFO ResourceProfile: Limiting resource is cpu
25/04/06 14:24:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 14:24:48 INFO SecurityManager: Changing view acls to: root
25/04/06 14:24:48 INFO SecurityManager: Changing modify acls to: root
25/04/06 14:24:48 INFO SecurityManager: Changing view acls groups to: 
25/04/06 14:24:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 14:24:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 14:24:48 INFO Utils: Successfully started service 'sparkDriver' on port 36325.
25/04/06 14:24:48 INFO SparkEnv: Registering MapOutputTracker
25/04/06 14:24:48 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 14:24:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 14:24:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 14:24:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 14:24:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-caf3e547-60f6-47ae-9f0b-5b953efc1ab1
25/04/06 14:24:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 14:24:48 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 14:24:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 14:24:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 14:24:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406142449-0078
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44107.
25/04/06 14:24:49 INFO NettyBlockTransferService: Server created on f2a344a33cdc:44107
25/04/06 14:24:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 14:24:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:44107 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/2 is now RUNNING
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/1 is now RUNNING
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/0 is now RUNNING
25/04/06 14:24:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 14:24:49 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 14:24:49 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 14:24:50 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:51542) with ID 0,  ResourceProfileId 0
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35868) with ID 1,  ResourceProfileId 0
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:51132) with ID 2,  ResourceProfileId 0
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39177 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39177, None)
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44275 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44275, None)
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45151 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45151, None)
25/04/06 14:24:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 14:24:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:24:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 14:24:51 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:24:51 INFO DAGScheduler: Missing parents: List()
25/04/06 14:24:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:24:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 14:24:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 14:24:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:44107 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 14:24:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:24:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 14:24:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 14:24:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:44275 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 14:24:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 14:24:52 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.247 s
25/04/06 14:24:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:24:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 14:24:52 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.277049 s
25/04/06 14:24:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:44107 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:44275 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:24:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 14:24:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:24:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 14:24:54 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0e3ed39a-a047-4ace-8b5d-2af68b4cf6bc, clientType=HIVECLI]
25/04/06 14:24:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 14:24:54 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 14:24:54 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 14:24:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 14:24:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 14:24:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 14:24:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO CodeGenerator: Code generated in 149.852151 ms
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:44107 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 14:24:55 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:24:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 14:24:55 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:24:55 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:24:55 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 14:24:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:24:55 INFO DAGScheduler: Missing parents: List()
25/04/06 14:24:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:44107 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:24:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 14:24:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:24:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 14:24:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:39177 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:24:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:39177 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 14:24:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1982 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:24:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 14:24:57 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.006 s
25/04/06 14:24:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:24:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 14:24:57 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.012373 s
25/04/06 14:24:57 INFO FileFormatWriter: Start to commit write Job 09241579-d0a7-41d8-9cc9-f98ed2d942a9.
25/04/06 14:24:57 INFO FileFormatWriter: Write Job 09241579-d0a7-41d8-9cc9-f98ed2d942a9 committed. Elapsed time: 41 ms.
25/04/06 14:24:57 INFO FileFormatWriter: Finished processing stats for write job 09241579-d0a7-41d8-9cc9-f98ed2d942a9.
25/04/06 14:24:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 14:24:57 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 14:24:57 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 14:24:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 14:24:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 14:24:57 INFO MemoryStore: MemoryStore cleared
25/04/06 14:24:57 INFO BlockManager: BlockManager stopped
25/04/06 14:24:57 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 14:24:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 14:24:57 INFO SparkContext: Successfully stopped SparkContext
25/04/06 14:24:57 INFO ShutdownHookManager: Shutdown hook called
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-7329fdb6-c55d-4088-9e3d-856be2185616
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-be92076b-4ff4-495a-b3ee-f01f00e9dcb9/pyspark-28c99f4a-9406-4f32-9abb-7930844c479d
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-be92076b-4ff4-495a-b3ee-f01f00e9dcb9
Spark job completed successfully.
