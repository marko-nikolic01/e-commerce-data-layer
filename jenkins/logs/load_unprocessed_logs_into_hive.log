Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:02:24 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:02:24 INFO ResourceUtils: ==============================================================
25/04/06 13:02:24 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:02:24 INFO ResourceUtils: ==============================================================
25/04/06 13:02:24 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:02:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:02:24 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:02:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:02:24 INFO SecurityManager: Changing view acls to: root
25/04/06 13:02:24 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:02:24 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:02:24 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:02:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:02:24 INFO Utils: Successfully started service 'sparkDriver' on port 32863.
25/04/06 13:02:24 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:02:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:02:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:02:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:02:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:02:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e61e110b-fdbd-48c9-bf05-8e925f6eec82
25/04/06 13:02:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:02:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:02:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:02:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:02:25 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130225-0016
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130225-0016/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130225-0016/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:02:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46393.
25/04/06 13:02:25 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46393
25/04/06 13:02:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:02:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46393 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/2 is now RUNNING
25/04/06 13:02:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46393, None)
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/0 is now RUNNING
25/04/06 13:02:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130225-0016/1 is now RUNNING
25/04/06 13:02:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:02:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:02:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:02:27 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42926) with ID 1,  ResourceProfileId 0
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:34976) with ID 2,  ResourceProfileId 0
25/04/06 13:02:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:54766) with ID 0,  ResourceProfileId 0
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:41307 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 41307, None)
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41679 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41679, None)
25/04/06 13:02:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:35431 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 35431, None)
25/04/06 13:02:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:02:27 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:02:27 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:02:27 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:02:27 INFO DAGScheduler: Missing parents: List()
25/04/06 13:02:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:02:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:02:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:02:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46393 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:02:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:02:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:02:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:02:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:35431 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1187 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:02:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:02:28 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.281 s
25/04/06 13:02:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:02:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:02:28 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.313783 s
25/04/06 13:02:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46393 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:35431 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:02:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:02:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:02:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:02:30 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ee96fb28-b02b-4d94-83d8-6a68a452555f, clientType=HIVECLI]
25/04/06 13:02:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:02:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:02:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:02:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:02:30 INFO metastore: Connected to metastore.
25/04/06 13:02:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 34, in <module>
    logs.write.mode("overwrite").insertInto("unprocessedlogs")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: `default`.`unprocessedlogs` requires that the data to be inserted have the same number of columns as the target table: target table has 6 column(s) but the inserted data has 8 column(s), including 0 partition column(s) having constant value(s).
25/04/06 13:02:31 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:02:31 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:02:31 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:02:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:02:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:02:31 INFO MemoryStore: MemoryStore cleared
25/04/06 13:02:31 INFO BlockManager: BlockManager stopped
25/04/06 13:02:31 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:02:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:02:31 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:02:31 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb83973b-598b-42cf-9a69-b4bf0ccc4d27
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc4eebb4-d4e4-4d25-b10d-6ba792d78885
25/04/06 13:02:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc4eebb4-d4e4-4d25-b10d-6ba792d78885/pyspark-268cef26-3139-4fbc-98a5-dc206b524549
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:05:52 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:05:52 INFO ResourceUtils: ==============================================================
25/04/06 13:05:52 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:05:52 INFO ResourceUtils: ==============================================================
25/04/06 13:05:52 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:05:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:05:52 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:05:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:05:52 INFO SecurityManager: Changing view acls to: root
25/04/06 13:05:52 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:05:52 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:05:52 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:05:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:05:52 INFO Utils: Successfully started service 'sparkDriver' on port 40125.
25/04/06 13:05:52 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:05:53 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:05:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:05:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-91e8561f-2773-423b-8cac-1076d80bf1e1
25/04/06 13:05:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:05:53 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:05:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:05:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:05:53 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130553-0018
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130553-0018/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130553-0018/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:05:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42135.
25/04/06 13:05:53 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42135
25/04/06 13:05:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:05:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42135 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/2 is now RUNNING
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/0 is now RUNNING
25/04/06 13:05:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42135, None)
25/04/06 13:05:53 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130553-0018/1 is now RUNNING
25/04/06 13:05:53 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:05:53 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:05:53 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:05:55 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59970) with ID 1,  ResourceProfileId 0
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:57896) with ID 0,  ResourceProfileId 0
25/04/06 13:05:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43054) with ID 2,  ResourceProfileId 0
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:35587 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 35587, None)
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35801 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35801, None)
25/04/06 13:05:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:41079 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 41079, None)
25/04/06 13:05:55 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:05:55 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:05:55 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:05:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:05:55 INFO DAGScheduler: Missing parents: List()
25/04/06 13:05:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:05:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:05:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:05:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42135 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:05:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:05:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:05:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:05:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:41079 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1209 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:05:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:05:56 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.302 s
25/04/06 13:05:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:05:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:05:57 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.333651 s
25/04/06 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42135 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:41079 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:05:58 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:05:58 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:05:58 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:05:58 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:05:58 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:58 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:05:58 INFO metastore: Connected to metastore.
25/04/06 13:05:58 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=383753ba-5d6c-4fed-83a6-74a0439e68d8, clientType=HIVECLI]
25/04/06 13:05:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:05:58 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:05:58 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:05:58 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:58 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:05:58 INFO metastore: Connected to metastore.
25/04/06 13:05:59 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:05:59 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:05:59 INFO metastore: Connected to metastore.
25/04/06 13:05:59 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 34, in <module>
    logs.write.mode("overwrite").insertInto("unprocessedlogs")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: `default`.`unprocessedlogs` requires that the data to be inserted have the same number of columns as the target table: target table has 6 column(s) but the inserted data has 7 column(s), including 0 partition column(s) having constant value(s).
25/04/06 13:05:59 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:05:59 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:05:59 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:05:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:05:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:05:59 INFO MemoryStore: MemoryStore cleared
25/04/06 13:05:59 INFO BlockManager: BlockManager stopped
25/04/06 13:05:59 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:05:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:05:59 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:05:59 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-053370a0-3a2c-48a9-b843-11dc747e4749
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8e1eb8a-9669-4763-9147-0f604b862dc1
25/04/06 13:05:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8e1eb8a-9669-4763-9147-0f604b862dc1/pyspark-8de92b75-b9b8-450c-b225-6eb487b8ee5f
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:07:48 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:07:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:07:48 INFO ResourceUtils: ==============================================================
25/04/06 13:07:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:07:48 INFO ResourceUtils: ==============================================================
25/04/06 13:07:48 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:07:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:07:48 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:07:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:07:48 INFO SecurityManager: Changing view acls to: root
25/04/06 13:07:48 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:07:48 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:07:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:07:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:07:49 INFO Utils: Successfully started service 'sparkDriver' on port 46855.
25/04/06 13:07:49 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:07:49 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:07:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:07:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-654311f3-1713-4450-b558-82c8fe779814
25/04/06 13:07:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:07:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:07:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:07:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:07:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406130749-0020
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406130749-0020/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406130749-0020/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:07:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41837.
25/04/06 13:07:49 INFO NettyBlockTransferService: Server created on f2a344a33cdc:41837
25/04/06 13:07:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:07:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:41837 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/2 is now RUNNING
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/1 is now RUNNING
25/04/06 13:07:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 41837, None)
25/04/06 13:07:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406130749-0020/0 is now RUNNING
25/04/06 13:07:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:07:50 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:07:50 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:07:51 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:57874) with ID 1,  ResourceProfileId 0
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:60844) with ID 2,  ResourceProfileId 0
25/04/06 13:07:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:50498) with ID 0,  ResourceProfileId 0
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:43601 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 43601, None)
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46879 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46879, None)
25/04/06 13:07:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38399 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38399, None)
25/04/06 13:07:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:07:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:07:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:07:51 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:07:51 INFO DAGScheduler: Missing parents: List()
25/04/06 13:07:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:07:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:07:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:07:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:41837 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:07:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:07:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:07:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:07:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:43601 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:07:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:07:53 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.236 s
25/04/06 13:07:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:07:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:07:53 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.267792 s
25/04/06 13:07:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:41837 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:43601 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:07:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:07:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:07:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:07:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:54 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1567c1e9-f0bb-4685-a85a-0caa6d208a97, clientType=HIVECLI]
25/04/06 13:07:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:07:54 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:07:54 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:07:54 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:07:54 INFO metastore: Connected to metastore.
25/04/06 13:07:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:07:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:07:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:07:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:07:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:07:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:07:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:07:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:07:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:07:55 INFO CodeGenerator: Code generated in 151.33813 ms
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:41837 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:07:55 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:07:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:07:55 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:07:55 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:07:55 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:07:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:07:55 INFO DAGScheduler: Missing parents: List()
25/04/06 13:07:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.4 KiB, free 365.7 MiB)
25/04/06 13:07:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:41837 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:07:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:07:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:07:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:07:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:07:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:38399 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:07:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:38399 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:07:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1938 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:07:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:07:57 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.965 s
25/04/06 13:07:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:07:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:07:57 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.973030 s
25/04/06 13:07:57 INFO FileFormatWriter: Start to commit write Job 35b4ab55-b30f-4896-9d82-6057d0d3afda.
25/04/06 13:07:57 INFO FileFormatWriter: Write Job 35b4ab55-b30f-4896-9d82-6057d0d3afda committed. Elapsed time: 35 ms.
25/04/06 13:07:57 INFO FileFormatWriter: Finished processing stats for write job 35b4ab55-b30f-4896-9d82-6057d0d3afda.
25/04/06 13:07:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:07:57 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:07:57 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:07:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:07:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:07:57 INFO MemoryStore: MemoryStore cleared
25/04/06 13:07:57 INFO BlockManager: BlockManager stopped
25/04/06 13:07:57 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:07:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:07:57 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:07:57 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-de9ce325-954f-4709-acd9-a96e3a9023a7
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-23cbc119-a989-4510-8778-faa9e0a80137/pyspark-e7a2cca9-3f06-41c9-8898-547b7640c599
25/04/06 13:07:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-23cbc119-a989-4510-8778-faa9e0a80137
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:10:09 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:10:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:10:09 INFO ResourceUtils: ==============================================================
25/04/06 13:10:09 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:10:09 INFO ResourceUtils: ==============================================================
25/04/06 13:10:09 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:10:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:10:09 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:10:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:10:09 INFO SecurityManager: Changing view acls to: root
25/04/06 13:10:09 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:10:09 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:10:09 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:10:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:10:09 INFO Utils: Successfully started service 'sparkDriver' on port 35009.
25/04/06 13:10:09 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:10:09 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:10:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:10:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:10:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:10:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e48f12f-3d76-49f8-a9a9-8abf2a5dec5f
25/04/06 13:10:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:10:09 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:10:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:10:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:10:10 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131010-0022
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131010-0022/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131010-0022/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:10:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33291.
25/04/06 13:10:10 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33291
25/04/06 13:10:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:10:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33291 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/0 is now RUNNING
25/04/06 13:10:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33291, None)
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/1 is now RUNNING
25/04/06 13:10:10 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131010-0022/2 is now RUNNING
25/04/06 13:10:10 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:10:10 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:10:10 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:10:11 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:44952) with ID 1,  ResourceProfileId 0
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:49880) with ID 2,  ResourceProfileId 0
25/04/06 13:10:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46778) with ID 0,  ResourceProfileId 0
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:42355 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 42355, None)
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35099 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35099, None)
25/04/06 13:10:12 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:46375 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 46375, None)
25/04/06 13:10:12 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:10:12 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:10:12 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:10:12 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:10:12 INFO DAGScheduler: Missing parents: List()
25/04/06 13:10:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:10:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:10:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:10:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33291 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:10:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:10:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:10:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:10:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:42355 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1140 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:10:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:10:13 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.230 s
25/04/06 13:10:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:10:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:10:13 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.260209 s
25/04/06 13:10:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33291 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:42355 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:10:14 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:10:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:10:15 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:10:15 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7043ca30-39bb-47e7-8348-17641a78ceef, clientType=HIVECLI]
25/04/06 13:10:15 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:10:15 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:10:15 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:10:15 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:10:15 INFO metastore: Connected to metastore.
25/04/06 13:10:15 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:10:15 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:10:15 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:10:15 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:10:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:10:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:10:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:10:15 INFO CodeGenerator: Code generated in 154.370682 ms
25/04/06 13:10:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:10:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:10:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:33291 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:10:15 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:10:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:10:16 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:10:16 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:10:16 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:10:16 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:10:16 INFO DAGScheduler: Missing parents: List()
25/04/06 13:10:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:10:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.4 KiB, free 365.7 MiB)
25/04/06 13:10:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:10:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:33291 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:10:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:10:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:10:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:10:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:46375 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:10:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:46375 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:10:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1954 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:10:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:10:17 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.981 s
25/04/06 13:10:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:10:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:10:17 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.987834 s
25/04/06 13:10:17 INFO FileFormatWriter: Start to commit write Job e9e479ca-d252-48cc-9cfb-6211f022dcac.
25/04/06 13:10:18 INFO FileFormatWriter: Write Job e9e479ca-d252-48cc-9cfb-6211f022dcac committed. Elapsed time: 37 ms.
25/04/06 13:10:18 INFO FileFormatWriter: Finished processing stats for write job e9e479ca-d252-48cc-9cfb-6211f022dcac.
25/04/06 13:10:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:10:18 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:10:18 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:10:18 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:10:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:10:18 INFO MemoryStore: MemoryStore cleared
25/04/06 13:10:18 INFO BlockManager: BlockManager stopped
25/04/06 13:10:18 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:10:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:10:18 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:10:18 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-d91908a2-d721-41c4-a0e8-8652a7734671
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f20f00c3-488d-44ee-b3d9-5415de5ec34d
25/04/06 13:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-f20f00c3-488d-44ee-b3d9-5415de5ec34d/pyspark-cffad3ff-a994-4946-a004-09626ff18cc9
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:12:15 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:12:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:12:15 INFO ResourceUtils: ==============================================================
25/04/06 13:12:15 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:12:15 INFO ResourceUtils: ==============================================================
25/04/06 13:12:15 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:12:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:12:15 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:12:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:12:15 INFO SecurityManager: Changing view acls to: root
25/04/06 13:12:15 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:12:15 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:12:15 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:12:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:12:15 INFO Utils: Successfully started service 'sparkDriver' on port 33079.
25/04/06 13:12:15 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:12:15 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:12:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:12:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-955635f7-f4ca-418b-b79a-746d90f08ec9
25/04/06 13:12:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:12:15 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:12:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:12:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:12:15 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131215-0024
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131215-0024/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:12:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131215-0024/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:12:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35135.
25/04/06 13:12:15 INFO NettyBlockTransferService: Server created on f2a344a33cdc:35135
25/04/06 13:12:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:12:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:35135 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/2 is now RUNNING
25/04/06 13:12:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 35135, None)
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/0 is now RUNNING
25/04/06 13:12:15 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131215-0024/1 is now RUNNING
25/04/06 13:12:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:12:16 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:12:16 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:12:17 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:33254) with ID 2,  ResourceProfileId 0
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46616) with ID 0,  ResourceProfileId 0
25/04/06 13:12:17 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42294) with ID 1,  ResourceProfileId 0
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35067 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35067, None)
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38023 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38023, None)
25/04/06 13:12:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45291 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45291, None)
25/04/06 13:12:18 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:12:18 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:12:18 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:12:18 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:12:18 INFO DAGScheduler: Missing parents: List()
25/04/06 13:12:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:12:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:12:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:12:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:35135 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:12:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:12:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:12:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:12:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:45291 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1200 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:12:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:12:19 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.293 s
25/04/06 13:12:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:12:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:12:19 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.326921 s
25/04/06 13:12:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:35135 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:19 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:45291 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:12:20 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:12:20 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:12:20 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:12:21 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5dc733ea-b055-4220-b6db-4fada04577dc, clientType=HIVECLI]
25/04/06 13:12:21 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:12:21 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:12:21 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:12:21 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:12:21 INFO metastore: Connected to metastore.
25/04/06 13:12:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:12:22 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:12:22 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:12:22 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:12:22 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:12:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:12:22 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:12:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:12:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:12:22 INFO CodeGenerator: Code generated in 155.411127 ms
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:35135 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:12:22 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:12:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:12:22 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:12:22 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:12:22 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:12:22 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:12:22 INFO DAGScheduler: Missing parents: List()
25/04/06 13:12:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:12:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.6 KiB, free 365.6 MiB)
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:35135 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:12:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:12:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:12:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:12:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:12:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:35067 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:12:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:35067 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:12:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2108 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:12:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:12:24 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.140 s
25/04/06 13:12:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:12:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:12:24 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.149424 s
25/04/06 13:12:24 INFO FileFormatWriter: Start to commit write Job f8790279-2a41-4dee-99d6-4d0f9ea85016.
25/04/06 13:12:24 INFO FileFormatWriter: Write Job f8790279-2a41-4dee-99d6-4d0f9ea85016 committed. Elapsed time: 38 ms.
25/04/06 13:12:24 INFO FileFormatWriter: Finished processing stats for write job f8790279-2a41-4dee-99d6-4d0f9ea85016.
25/04/06 13:12:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:12:24 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:12:24 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:12:24 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:12:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:12:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:12:24 INFO MemoryStore: MemoryStore cleared
25/04/06 13:12:24 INFO BlockManager: BlockManager stopped
25/04/06 13:12:24 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:12:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:12:24 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:12:25 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-5500e226-7bd5-4231-8c2b-9c9c3215b00f/pyspark-787f90cb-4450-4fc7-b725-3660a8855e9d
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-5500e226-7bd5-4231-8c2b-9c9c3215b00f
25/04/06 13:12:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-d8d6670a-1ba1-44dc-aafa-e68801a80a27
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:16:44 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:16:44 INFO ResourceUtils: ==============================================================
25/04/06 13:16:44 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:16:44 INFO ResourceUtils: ==============================================================
25/04/06 13:16:44 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:16:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:16:44 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:16:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:16:44 INFO SecurityManager: Changing view acls to: root
25/04/06 13:16:44 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:16:44 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:16:44 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:16:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:16:44 INFO Utils: Successfully started service 'sparkDriver' on port 39505.
25/04/06 13:16:44 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:16:44 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:16:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:16:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21b50c04-c8a9-4714-b2e7-a42ca8be9ac4
25/04/06 13:16:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:16:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:16:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:16:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:16:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131644-0026
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131644-0026/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:16:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131644-0026/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:16:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45917.
25/04/06 13:16:44 INFO NettyBlockTransferService: Server created on f2a344a33cdc:45917
25/04/06 13:16:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:16:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:45917 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/1 is now RUNNING
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/2 is now RUNNING
25/04/06 13:16:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131644-0026/0 is now RUNNING
25/04/06 13:16:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 45917, None)
25/04/06 13:16:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:16:45 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:16:45 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:16:46 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:60942) with ID 2,  ResourceProfileId 0
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:45218) with ID 0,  ResourceProfileId 0
25/04/06 13:16:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48424) with ID 1,  ResourceProfileId 0
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45121 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45121, None)
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:36143 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 36143, None)
25/04/06 13:16:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46643 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46643, None)
25/04/06 13:16:47 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:16:47 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:16:47 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:16:47 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:16:47 INFO DAGScheduler: Missing parents: List()
25/04/06 13:16:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:16:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:16:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:16:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:45917 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:16:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:16:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:16:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:16:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:45121 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1172 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:16:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:16:48 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.273 s
25/04/06 13:16:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:16:48 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.304731 s
25/04/06 13:16:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:45917 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:45121 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:16:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:16:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:16:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:16:50 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0c32d0c7-2a40-45d4-8828-ff8af13bb8d9, clientType=HIVECLI]
25/04/06 13:16:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:16:50 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:16:50 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:16:50 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:16:50 INFO metastore: Connected to metastore.
25/04/06 13:16:50 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:16:50 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:16:50 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:16:50 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:16:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:16:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:16:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:16:50 INFO CodeGenerator: Code generated in 157.693995 ms
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:16:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:45917 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:16:50 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:16:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:16:50 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:16:50 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:16:50 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:16:50 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:16:50 INFO DAGScheduler: Missing parents: List()
25/04/06 13:16:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:16:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.6 KiB, free 365.6 MiB)
25/04/06 13:16:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:45917 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:16:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:16:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:16:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:16:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:16:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46643 (size: 74.6 KiB, free: 366.2 MiB)
25/04/06 13:16:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46643 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:16:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2022 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:16:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:16:52 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.051 s
25/04/06 13:16:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:16:52 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.059132 s
25/04/06 13:16:52 INFO FileFormatWriter: Start to commit write Job 9d0b270c-0ecf-4c57-a671-ef228191c01b.
25/04/06 13:16:53 INFO FileFormatWriter: Write Job 9d0b270c-0ecf-4c57-a671-ef228191c01b committed. Elapsed time: 37 ms.
25/04/06 13:16:53 INFO FileFormatWriter: Finished processing stats for write job 9d0b270c-0ecf-4c57-a671-ef228191c01b.
25/04/06 13:16:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:16:53 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:16:53 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:16:53 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:16:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:16:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:16:53 INFO MemoryStore: MemoryStore cleared
25/04/06 13:16:53 INFO BlockManager: BlockManager stopped
25/04/06 13:16:53 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:16:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:16:53 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:16:53 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d96d674-3b5d-4b8b-b06e-b1c297058e34
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a9de192-ed8a-4590-aa23-dcb7afacd7ac/pyspark-1a229d69-2180-4369-b799-7e46bf2dd1a1
25/04/06 13:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-0a9de192-ed8a-4590-aa23-dcb7afacd7ac
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:17:45 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:17:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:17:45 INFO ResourceUtils: ==============================================================
25/04/06 13:17:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:17:45 INFO ResourceUtils: ==============================================================
25/04/06 13:17:45 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:17:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:17:45 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:17:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:17:45 INFO SecurityManager: Changing view acls to: root
25/04/06 13:17:45 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:17:45 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:17:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:17:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:17:45 INFO Utils: Successfully started service 'sparkDriver' on port 37899.
25/04/06 13:17:45 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:17:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:17:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:17:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c4449845-a0eb-499a-8f81-47ae542a6152
25/04/06 13:17:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:17:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:17:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:17:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:17:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131745-0028
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131745-0028/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:17:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131745-0028/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:17:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44473.
25/04/06 13:17:45 INFO NettyBlockTransferService: Server created on f2a344a33cdc:44473
25/04/06 13:17:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:17:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/0 is now RUNNING
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/1 is now RUNNING
25/04/06 13:17:45 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:44473 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131745-0028/2 is now RUNNING
25/04/06 13:17:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 44473, None)
25/04/06 13:17:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:17:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:17:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:17:47 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:51268) with ID 0,  ResourceProfileId 0
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:54460) with ID 1,  ResourceProfileId 0
25/04/06 13:17:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:47092) with ID 2,  ResourceProfileId 0
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:37913 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 37913, None)
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:37757 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 37757, None)
25/04/06 13:17:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33977 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33977, None)
25/04/06 13:17:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:17:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:17:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:17:48 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:17:48 INFO DAGScheduler: Missing parents: List()
25/04/06 13:17:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:17:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:17:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:17:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:44473 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:17:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:17:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:17:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:17:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:37757 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1231 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:17:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:17:49 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.338 s
25/04/06 13:17:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:17:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:17:49 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.373418 s
25/04/06 13:17:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:44473 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:37757 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:17:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:17:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:17:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:17:51 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=33cf079a-422e-4aa3-afaa-dab44295f06b, clientType=HIVECLI]
25/04/06 13:17:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:17:51 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:17:51 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:17:51 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:17:51 INFO metastore: Connected to metastore.
25/04/06 13:17:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:17:51 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:17:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:17:51 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:17:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:17:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:17:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:17:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:17:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:17:52 INFO CodeGenerator: Code generated in 158.096706 ms
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:44473 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:17:52 INFO SparkContext: Created broadcast 1 from saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:17:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:17:52 INFO SparkContext: Starting job: saveAsTable at NativeMethodAccessorImpl.java:0
25/04/06 13:17:52 INFO DAGScheduler: Got job 1 (saveAsTable at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:17:52 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0)
25/04/06 13:17:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:17:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:17:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.0 KiB, free 365.7 MiB)
25/04/06 13:17:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:44473 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:17:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:17:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:17:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:17:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:17:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:33977 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:17:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:33977 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:17:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2106 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:17:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:17:54 INFO DAGScheduler: ResultStage 1 (saveAsTable at NativeMethodAccessorImpl.java:0) finished in 2.134 s
25/04/06 13:17:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:17:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:17:54 INFO DAGScheduler: Job 1 finished: saveAsTable at NativeMethodAccessorImpl.java:0, took 2.141417 s
25/04/06 13:17:54 INFO FileFormatWriter: Start to commit write Job d4826559-82eb-423d-90a7-d5a4b40049f9.
25/04/06 13:17:54 INFO FileFormatWriter: Write Job d4826559-82eb-423d-90a7-d5a4b40049f9 committed. Elapsed time: 38 ms.
25/04/06 13:17:54 INFO FileFormatWriter: Finished processing stats for write job d4826559-82eb-423d-90a7-d5a4b40049f9.
25/04/06 13:17:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:17:54 INFO HiveExternalCatalog: Persisting file based data source table `default`.`unprocessedlogs` into Hive metastore in Hive compatible format.
25/04/06 13:17:54 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:17:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:17:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:17:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:17:54 INFO MemoryStore: MemoryStore cleared
25/04/06 13:17:54 INFO BlockManager: BlockManager stopped
25/04/06 13:17:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:17:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:17:54 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:17:54 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-64c4a5ca-a24e-4be2-acc2-e9a2e2892c4d
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-678eafb6-99df-4f25-b281-4022f21f3f9d
25/04/06 13:17:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-64c4a5ca-a24e-4be2-acc2-e9a2e2892c4d/pyspark-f741deb4-1878-428c-b8f5-ad5b69f0ea33
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:18:56 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:18:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:18:57 INFO ResourceUtils: ==============================================================
25/04/06 13:18:57 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:18:57 INFO ResourceUtils: ==============================================================
25/04/06 13:18:57 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:18:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:18:57 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:18:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:18:57 INFO SecurityManager: Changing view acls to: root
25/04/06 13:18:57 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:18:57 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:18:57 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:18:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:18:57 INFO Utils: Successfully started service 'sparkDriver' on port 43073.
25/04/06 13:18:57 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:18:57 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:18:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:18:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e3313658-c8b2-4c71-8b39-a22e9aae1fb9
25/04/06 13:18:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:18:57 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:18:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:18:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:18:57 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406131857-0030
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406131857-0030/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:18:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406131857-0030/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:18:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33687.
25/04/06 13:18:57 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33687
25/04/06 13:18:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:18:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33687 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/2 is now RUNNING
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/1 is now RUNNING
25/04/06 13:18:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33687, None)
25/04/06 13:18:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406131857-0030/0 is now RUNNING
25/04/06 13:18:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:18:58 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:18:58 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:18:59 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53548) with ID 2,  ResourceProfileId 0
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:53868) with ID 1,  ResourceProfileId 0
25/04/06 13:18:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:49734) with ID 0,  ResourceProfileId 0
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:35955 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 35955, None)
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33409 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33409, None)
25/04/06 13:18:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:44345 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 44345, None)
25/04/06 13:19:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:19:00 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:19:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:19:00 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:19:00 INFO DAGScheduler: Missing parents: List()
25/04/06 13:19:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:19:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:19:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:19:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33687 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:19:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:19:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:19:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:19:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:35955 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1227 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:19:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:19:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.329 s
25/04/06 13:19:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:19:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:19:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.363689 s
25/04/06 13:19:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33687 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:35955 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:19:02 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:19:02 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:19:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:19:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ca2f6b81-f1f3-4a66-a774-ef94ddabec24, clientType=HIVECLI]
25/04/06 13:19:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:19:03 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:19:03 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:19:03 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:19:03 INFO metastore: Connected to metastore.
25/04/06 13:19:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 33, in <module>
    spark.sql(f"""
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: DELETE is only supported with v2 tables.
25/04/06 13:19:03 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:19:03 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:19:03 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:19:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:19:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:19:03 INFO MemoryStore: MemoryStore cleared
25/04/06 13:19:03 INFO BlockManager: BlockManager stopped
25/04/06 13:19:03 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:19:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:19:03 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:19:03 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee1b1df6-57d8-49ce-866a-bbee595d7469
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-ee1b1df6-57d8-49ce-866a-bbee595d7469/pyspark-348935c1-b7b8-45ff-aaf8-4c0c03ab0c12
25/04/06 13:19:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-0d969da4-3833-4a2e-a693-dc3f52932485
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:23:17 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:23:17 INFO ResourceUtils: ==============================================================
25/04/06 13:23:17 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:23:17 INFO ResourceUtils: ==============================================================
25/04/06 13:23:17 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:23:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:23:17 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:23:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:23:17 INFO SecurityManager: Changing view acls to: root
25/04/06 13:23:17 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:23:17 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:23:17 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:23:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:23:17 INFO Utils: Successfully started service 'sparkDriver' on port 40763.
25/04/06 13:23:17 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:23:17 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:23:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:23:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73fe6ead-fcaa-4765-a8ba-52daf20b44b0
25/04/06 13:23:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:23:17 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:23:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:23:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:23:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132317-0032
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132317-0032/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:23:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132317-0032/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:23:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46749.
25/04/06 13:23:17 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46749
25/04/06 13:23:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:23:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46749 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46749, None)
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/1 is now RUNNING
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/0 is now RUNNING
25/04/06 13:23:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132317-0032/2 is now RUNNING
25/04/06 13:23:18 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:23:18 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:23:18 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:23:19 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:34254) with ID 2,  ResourceProfileId 0
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35964) with ID 1,  ResourceProfileId 0
25/04/06 13:23:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:40844) with ID 0,  ResourceProfileId 0
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38159 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38159, None)
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44693 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44693, None)
25/04/06 13:23:19 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:39431 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 39431, None)
25/04/06 13:23:19 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:23:19 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:23:19 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:23:19 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:23:19 INFO DAGScheduler: Missing parents: List()
25/04/06 13:23:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:23:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:23:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:23:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46749 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:23:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:23:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:23:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:23:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44693 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1167 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:23:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:23:21 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.257 s
25/04/06 13:23:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:23:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:23:21 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.288696 s
25/04/06 13:23:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46749 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44693 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:23:22 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:23:22 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:23:22 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:23:22 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:23:22 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:22 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:23:22 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:23:23 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=19123dc8-aafa-466d-bdb5-4678e01186be, clientType=HIVECLI]
25/04/06 13:23:23 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:23:23 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:23:23 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:23:23 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:23 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:23:23 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:23:23 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:23:23 INFO metastore: Connected to metastore.
25/04/06 13:23:23 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:23:23 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:23:23 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:23:23 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:23:23 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:23:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:23:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:23:23 INFO CodeGenerator: Code generated in 151.061178 ms
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:46749 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:23:23 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:23:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:23:23 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:23:23 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:23:23 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:23:23 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:23:23 INFO DAGScheduler: Missing parents: List()
25/04/06 13:23:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:23:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:46749 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:23:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:23:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:23:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:23:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:23:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:44693 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:23:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:44693 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:23:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 948 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:23:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:23:24 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.975 s
25/04/06 13:23:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:23:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:23:24 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.982789 s
25/04/06 13:23:24 INFO FileFormatWriter: Start to commit write Job b6761802-64f6-4f1c-a016-269a1e40db40.
25/04/06 13:23:24 INFO FileFormatWriter: Write Job b6761802-64f6-4f1c-a016-269a1e40db40 committed. Elapsed time: 41 ms.
25/04/06 13:23:24 INFO FileFormatWriter: Finished processing stats for write job b6761802-64f6-4f1c-a016-269a1e40db40.
25/04/06 13:23:24 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:23:24 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:23:24 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:23:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:23:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:23:24 INFO MemoryStore: MemoryStore cleared
25/04/06 13:23:24 INFO BlockManager: BlockManager stopped
25/04/06 13:23:24 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:23:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:23:24 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:23:25 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab9a0bb-60ba-4fa5-89d6-d7792e4df5d4/pyspark-9b35c003-27ee-441d-8720-c66a017b6a83
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-687fb58c-43cd-4cd5-be5b-ad6b4c79213c
25/04/06 13:23:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-cab9a0bb-60ba-4fa5-89d6-d7792e4df5d4
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:24:20 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:24:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:24:20 INFO ResourceUtils: ==============================================================
25/04/06 13:24:20 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:24:20 INFO ResourceUtils: ==============================================================
25/04/06 13:24:20 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:24:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:24:20 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:24:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:24:20 INFO SecurityManager: Changing view acls to: root
25/04/06 13:24:20 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:24:20 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:24:20 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:24:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:24:20 INFO Utils: Successfully started service 'sparkDriver' on port 44183.
25/04/06 13:24:20 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:24:20 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:24:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:24:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:24:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:24:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1296992-3c43-47ce-80fe-aaaa1cf0e11d
25/04/06 13:24:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:24:20 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:24:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:24:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:24:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:24:21 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132421-0034
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132421-0034/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132421-0034/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:24:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40489.
25/04/06 13:24:21 INFO NettyBlockTransferService: Server created on f2a344a33cdc:40489
25/04/06 13:24:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:24:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:40489 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/1 is now RUNNING
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/2 is now RUNNING
25/04/06 13:24:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132421-0034/0 is now RUNNING
25/04/06 13:24:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 40489, None)
25/04/06 13:24:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:24:21 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:24:21 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:24:22 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:38336) with ID 2,  ResourceProfileId 0
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:44988) with ID 0,  ResourceProfileId 0
25/04/06 13:24:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:52600) with ID 1,  ResourceProfileId 0
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:34337 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 34337, None)
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44103 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44103, None)
25/04/06 13:24:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40435 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40435, None)
25/04/06 13:24:23 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:24:23 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:24:23 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:24:23 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:24:23 INFO DAGScheduler: Missing parents: List()
25/04/06 13:24:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:24:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:24:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:24:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:40489 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:24:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:24:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:24:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44103 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:24:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:24:24 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.248 s
25/04/06 13:24:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:24:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:24:24 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.279528 s
25/04/06 13:24:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:40489 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44103 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:24:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:24:25 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:24:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:24:25 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:24:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:25 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:24:25 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:24:26 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3a230f76-b4bd-46ed-95a0-7f56756b0c6b, clientType=HIVECLI]
25/04/06 13:24:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:24:26 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:24:26 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:24:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:26 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:24:26 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:24:26 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:24:26 INFO metastore: Connected to metastore.
25/04/06 13:24:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:24:26 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:24:26 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:24:26 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:24:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:24:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:24:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:24:27 INFO CodeGenerator: Code generated in 156.68444 ms
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:40489 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:24:27 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:24:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:24:27 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:24:27 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:24:27 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:24:27 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:24:27 INFO DAGScheduler: Missing parents: List()
25/04/06 13:24:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:24:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:40489 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:24:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:24:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:24:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:24:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:24:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:40435 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:24:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:40435 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:24:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2105 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:24:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:24:29 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.134 s
25/04/06 13:24:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:24:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:24:29 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.143177 s
25/04/06 13:24:29 INFO FileFormatWriter: Start to commit write Job 519c1952-9e3d-4aca-a5d0-e608d5145e4f.
25/04/06 13:24:29 INFO FileFormatWriter: Write Job 519c1952-9e3d-4aca-a5d0-e608d5145e4f committed. Elapsed time: 43 ms.
25/04/06 13:24:29 INFO FileFormatWriter: Finished processing stats for write job 519c1952-9e3d-4aca-a5d0-e608d5145e4f.
25/04/06 13:24:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:24:29 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:24:29 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:24:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:24:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:24:29 INFO MemoryStore: MemoryStore cleared
25/04/06 13:24:29 INFO BlockManager: BlockManager stopped
25/04/06 13:24:29 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:24:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:24:29 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:24:29 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-982fdc94-5bae-4222-88e0-98ddfd27564d
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-2f3aef6a-a780-4aae-b75c-7ef6d9fc5c4d
25/04/06 13:24:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-982fdc94-5bae-4222-88e0-98ddfd27564d/pyspark-d632337f-cb27-4995-9fce-edfa4eef562a
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:25:30 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:25:30 INFO ResourceUtils: ==============================================================
25/04/06 13:25:30 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:25:30 INFO ResourceUtils: ==============================================================
25/04/06 13:25:30 INFO SparkContext: Submitted application: Load logs data into Hive
25/04/06 13:25:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:25:30 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:25:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:25:31 INFO SecurityManager: Changing view acls to: root
25/04/06 13:25:31 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:25:31 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:25:31 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:25:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:25:31 INFO Utils: Successfully started service 'sparkDriver' on port 42601.
25/04/06 13:25:31 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:25:31 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:25:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:25:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e9215b4-ee31-4cb9-a9b5-32cfd92a48d9
25/04/06 13:25:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:25:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:25:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:25:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:25:31 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132531-0036
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132531-0036/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132531-0036/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:25:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46437.
25/04/06 13:25:31 INFO NettyBlockTransferService: Server created on f2a344a33cdc:46437
25/04/06 13:25:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:25:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/1 is now RUNNING
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/0 is now RUNNING
25/04/06 13:25:31 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:46437 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132531-0036/2 is now RUNNING
25/04/06 13:25:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 46437, None)
25/04/06 13:25:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:25:32 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:25:32 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:25:33 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43198) with ID 2,  ResourceProfileId 0
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:45850) with ID 1,  ResourceProfileId 0
25/04/06 13:25:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:39758) with ID 0,  ResourceProfileId 0
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:44429 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 44429, None)
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46611 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46611, None)
25/04/06 13:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39853 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39853, None)
25/04/06 13:25:33 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:25:33 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:25:33 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:25:33 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:25:33 INFO DAGScheduler: Missing parents: List()
25/04/06 13:25:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:25:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:25:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:25:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:46437 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:25:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:25:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:25:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:25:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:44429 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1163 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:25:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:25:35 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.253 s
25/04/06 13:25:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:25:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:25:35 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.284884 s
25/04/06 13:25:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:46437 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:44429 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:25:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:25:36 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:25:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:25:36 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:25:36 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:36 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:25:36 INFO metastore: Connected to metastore.
25/04/06 13:25:36 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:25:37 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ef41f7ea-f7e3-47a2-b876-838d2c33b79e, clientType=HIVECLI]
25/04/06 13:25:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:25:37 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:25:37 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:25:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:25:37 INFO metastore: Connected to metastore.
25/04/06 13:25:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:25:37 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:25:37 INFO metastore: Connected to metastore.
25/04/06 13:25:37 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:25:37 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:25:37 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:25:37 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:25:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:25:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:25:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:25:37 INFO CodeGenerator: Code generated in 146.889585 ms
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.6 KiB, free 366.0 MiB)
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:46437 (size: 33.8 KiB, free: 366.3 MiB)
25/04/06 13:25:37 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:25:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:25:37 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:25:37 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:25:37 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:25:37 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:25:37 INFO DAGScheduler: Missing parents: List()
25/04/06 13:25:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:25:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:46437 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:25:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:25:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:25:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:25:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:25:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46611 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:25:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46611 (size: 33.8 KiB, free: 366.2 MiB)
25/04/06 13:25:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2049 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:25:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:25:39 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.075 s
25/04/06 13:25:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:25:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:25:39 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.082457 s
25/04/06 13:25:39 INFO FileFormatWriter: Start to commit write Job de4f512b-bf0a-43c9-891e-f30c91a84983.
25/04/06 13:25:39 INFO FileFormatWriter: Write Job de4f512b-bf0a-43c9-891e-f30c91a84983 committed. Elapsed time: 39 ms.
25/04/06 13:25:39 INFO FileFormatWriter: Finished processing stats for write job de4f512b-bf0a-43c9-891e-f30c91a84983.
25/04/06 13:25:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:25:39 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:25:39 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:25:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:25:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:25:39 INFO MemoryStore: MemoryStore cleared
25/04/06 13:25:39 INFO BlockManager: BlockManager stopped
25/04/06 13:25:39 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:25:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:25:39 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:25:39 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5533134-2070-492c-922d-80429c094985
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f2a5a59-a49c-47b1-8937-7b8f7e54214f
25/04/06 13:25:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5533134-2070-492c-922d-80429c094985/pyspark-924676ab-cd63-4e18-8aa8-b1175f8b4d57
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:29:46 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:29:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:29:46 INFO ResourceUtils: ==============================================================
25/04/06 13:29:46 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:29:46 INFO ResourceUtils: ==============================================================
25/04/06 13:29:46 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:29:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:29:46 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:29:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:29:46 INFO SecurityManager: Changing view acls to: root
25/04/06 13:29:46 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:29:46 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:29:46 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:29:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:29:46 INFO Utils: Successfully started service 'sparkDriver' on port 36911.
25/04/06 13:29:46 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:29:46 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:29:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:29:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:29:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:29:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7113cd49-8a06-4166-8f97-c45728fcd6b7
25/04/06 13:29:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:29:46 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:29:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:29:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:29:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:29:47 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406132947-0040
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406132947-0040/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406132947-0040/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:29:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42863.
25/04/06 13:29:47 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42863
25/04/06 13:29:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:29:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42863 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/1 is now RUNNING
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/2 is now RUNNING
25/04/06 13:29:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406132947-0040/0 is now RUNNING
25/04/06 13:29:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42863, None)
25/04/06 13:29:47 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:29:47 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:29:47 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:29:49 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:36604) with ID 1,  ResourceProfileId 0
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:58386) with ID 2,  ResourceProfileId 0
25/04/06 13:29:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:38226) with ID 0,  ResourceProfileId 0
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43579 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43579, None)
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45721 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45721, None)
25/04/06 13:29:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:44677 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 44677, None)
25/04/06 13:29:49 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:29:49 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:29:49 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:29:49 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:29:49 INFO DAGScheduler: Missing parents: List()
25/04/06 13:29:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:29:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:29:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:29:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42863 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:29:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:29:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:29:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:29:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:43579 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1227 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:29:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:29:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.326 s
25/04/06 13:29:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:29:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:29:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.361409 s
25/04/06 13:29:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42863 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:43579 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:29:52 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:29:52 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:29:52 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:29:52 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:52 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c320ce46-656c-48bd-8726-e06e01fd3822, clientType=HIVECLI]
25/04/06 13:29:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:29:52 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:29:52 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:29:52 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:29:52 INFO metastore: Connected to metastore.
25/04/06 13:29:53 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:29:53 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:29:53 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:29:53 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:29:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:29:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:29:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:29:53 INFO CodeGenerator: Code generated in 159.683879 ms
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:42863 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:29:53 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:29:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:29:53 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:29:53 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:29:53 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:29:53 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:29:53 INFO DAGScheduler: Missing parents: List()
25/04/06 13:29:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:29:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:42863 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:29:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:29:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:29:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:29:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:29:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:45721 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:29:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:45721 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:29:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2077 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:29:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:29:55 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.108 s
25/04/06 13:29:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:29:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:29:55 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.116639 s
25/04/06 13:29:55 INFO FileFormatWriter: Start to commit write Job 5ab5a544-c67e-478b-b0a7-7832f820366e.
25/04/06 13:29:55 INFO FileFormatWriter: Write Job 5ab5a544-c67e-478b-b0a7-7832f820366e committed. Elapsed time: 42 ms.
25/04/06 13:29:55 INFO FileFormatWriter: Finished processing stats for write job 5ab5a544-c67e-478b-b0a7-7832f820366e.
25/04/06 13:29:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:29:55 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:29:55 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:29:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:29:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:29:55 INFO MemoryStore: MemoryStore cleared
25/04/06 13:29:55 INFO BlockManager: BlockManager stopped
25/04/06 13:29:55 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:29:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:29:55 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:29:55 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9750c10-a3ef-4249-b9c6-7b5be5310184
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-0fe216c4-474c-41c3-8c7d-58b0a4449085
25/04/06 13:29:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9750c10-a3ef-4249-b9c6-7b5be5310184/pyspark-1c917937-d29d-42a2-a806-d85da8cd08e6
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:33:31 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:33:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:33:31 INFO ResourceUtils: ==============================================================
25/04/06 13:33:31 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:33:31 INFO ResourceUtils: ==============================================================
25/04/06 13:33:31 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:33:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:33:31 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:33:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:33:31 INFO SecurityManager: Changing view acls to: root
25/04/06 13:33:31 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:33:31 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:33:31 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:33:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:33:31 INFO Utils: Successfully started service 'sparkDriver' on port 40615.
25/04/06 13:33:31 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:33:31 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:33:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:33:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:33:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:33:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f685e956-6910-4597-99be-f3186171b70c
25/04/06 13:33:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:33:31 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:33:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:33:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:33:32 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406133332-0044
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406133332-0044/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406133332-0044/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:33:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34859.
25/04/06 13:33:32 INFO NettyBlockTransferService: Server created on f2a344a33cdc:34859
25/04/06 13:33:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:33:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:34859 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/1 is now RUNNING
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/0 is now RUNNING
25/04/06 13:33:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406133332-0044/2 is now RUNNING
25/04/06 13:33:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 34859, None)
25/04/06 13:33:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:33:32 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:33:32 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:33:34 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:36228) with ID 0,  ResourceProfileId 0
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:59670) with ID 2,  ResourceProfileId 0
25/04/06 13:33:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42276) with ID 1,  ResourceProfileId 0
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:37841 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 37841, None)
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:42991 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 42991, None)
25/04/06 13:33:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46017 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46017, None)
25/04/06 13:33:34 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:33:34 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:33:34 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:33:34 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:33:34 INFO DAGScheduler: Missing parents: List()
25/04/06 13:33:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:33:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:33:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:33:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:34859 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:33:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:33:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:33:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:33:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:42991 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1191 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:33:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:33:35 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.297 s
25/04/06 13:33:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:33:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:33:35 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.334685 s
25/04/06 13:33:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:34859 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:36 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:42991 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:33:37 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:33:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:33:37 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:33:37 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:37 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d7bf0853-c551-4ef2-95ef-2667ba96839c, clientType=HIVECLI]
25/04/06 13:33:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:33:37 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:33:37 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:33:37 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:33:37 INFO metastore: Connected to metastore.
25/04/06 13:33:38 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:33:38 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:33:38 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:33:38 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:33:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:33:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:33:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:33:38 INFO CodeGenerator: Code generated in 146.441413 ms
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:34859 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:33:38 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:33:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:33:38 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:33:38 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:33:38 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:33:38 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:33:38 INFO DAGScheduler: Missing parents: List()
25/04/06 13:33:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:33:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:34859 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:33:38 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:33:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:33:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:33:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:33:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46017 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:33:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46017 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:33:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1969 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:33:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:33:40 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.995 s
25/04/06 13:33:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:33:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:33:40 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.002401 s
25/04/06 13:33:40 INFO FileFormatWriter: Start to commit write Job 27ca2f65-847d-4e5e-968f-a7a58c1f392b.
25/04/06 13:33:40 INFO FileFormatWriter: Write Job 27ca2f65-847d-4e5e-968f-a7a58c1f392b committed. Elapsed time: 36 ms.
25/04/06 13:33:40 INFO FileFormatWriter: Finished processing stats for write job 27ca2f65-847d-4e5e-968f-a7a58c1f392b.
25/04/06 13:33:40 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:33:40 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:33:40 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:33:40 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:33:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:33:40 INFO MemoryStore: MemoryStore cleared
25/04/06 13:33:40 INFO BlockManager: BlockManager stopped
25/04/06 13:33:40 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:33:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:33:40 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:33:41 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0e09ab28-3d8c-44be-bd64-5cc5e863bcb9/pyspark-9af375ff-6f56-42ba-866c-7ed0fe24ce77
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-0e09ab28-3d8c-44be-bd64-5cc5e863bcb9
25/04/06 13:33:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce5d68a3-3da9-442e-8008-902d7778f9ca
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:40:53 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:40:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:40:53 INFO ResourceUtils: ==============================================================
25/04/06 13:40:53 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:40:53 INFO ResourceUtils: ==============================================================
25/04/06 13:40:53 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:40:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:40:53 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:40:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:40:54 INFO SecurityManager: Changing view acls to: root
25/04/06 13:40:54 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:40:54 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:40:54 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:40:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:40:54 INFO Utils: Successfully started service 'sparkDriver' on port 42449.
25/04/06 13:40:54 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:40:54 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:40:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:40:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-814872a6-4921-40ce-a3de-1e165df791c6
25/04/06 13:40:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:40:54 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:40:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:40:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:40:54 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134054-0047
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134054-0047/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134054-0047/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:40:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42007.
25/04/06 13:40:54 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42007
25/04/06 13:40:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:40:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42007 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/1 is now RUNNING
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/2 is now RUNNING
25/04/06 13:40:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134054-0047/0 is now RUNNING
25/04/06 13:40:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42007, None)
25/04/06 13:40:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:40:55 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:40:55 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:40:56 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53620) with ID 2,  ResourceProfileId 0
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:34068) with ID 0,  ResourceProfileId 0
25/04/06 13:40:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:37924) with ID 1,  ResourceProfileId 0
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:40187 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 40187, None)
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:34575 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 34575, None)
25/04/06 13:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42639 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42639, None)
25/04/06 13:40:56 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:40:56 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:40:56 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:40:56 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:40:56 INFO DAGScheduler: Missing parents: List()
25/04/06 13:40:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:40:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:40:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 13:40:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42007 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:40:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:40:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:40:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:40:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:40187 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1147 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:40:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:40:57 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.239 s
25/04/06 13:40:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:40:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:40:57 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.270709 s
25/04/06 13:40:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42007 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:40187 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 13:40:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:40:59 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:40:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:40:59 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:40:59 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:40:59 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:40:59 INFO metastore: Connected to metastore.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 20, in <module>
    spark.sql(f"""
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Table or view not found: unprocessedlogs; line 2 pos 15;
'DropTable false, false
+- 'UnresolvedTableOrView [unprocessedlogs], DROP TABLE, true

25/04/06 13:40:59 INFO SparkContext: Invoking stop() from shutdown hook
25/04/06 13:40:59 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:40:59 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:40:59 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:40:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:40:59 INFO MemoryStore: MemoryStore cleared
25/04/06 13:40:59 INFO BlockManager: BlockManager stopped
25/04/06 13:40:59 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:40:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:40:59 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:40:59 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd05d8e0-e495-4b9e-b225-b0a3b0fb5356
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd05d8e0-e495-4b9e-b225-b0a3b0fb5356/pyspark-91112c66-b23a-4be3-bfac-2a58ce7514eb
25/04/06 13:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-027a23ba-ac81-4dfc-9537-3730fb37dbf6
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:44:11 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:44:11 INFO ResourceUtils: ==============================================================
25/04/06 13:44:11 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:44:11 INFO ResourceUtils: ==============================================================
25/04/06 13:44:11 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:44:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:44:11 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:44:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:44:11 INFO SecurityManager: Changing view acls to: root
25/04/06 13:44:11 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:44:11 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:44:11 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:44:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:44:11 INFO Utils: Successfully started service 'sparkDriver' on port 36235.
25/04/06 13:44:11 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:44:11 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:44:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:44:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:44:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:44:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6f6a2156-2f69-476b-8f73-c7d3b9aa4209
25/04/06 13:44:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:44:12 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:44:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:44:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:44:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134412-0051
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134412-0051/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134412-0051/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:44:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39627.
25/04/06 13:44:12 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39627
25/04/06 13:44:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:44:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39627 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/0 is now RUNNING
25/04/06 13:44:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/1 is now RUNNING
25/04/06 13:44:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39627, None)
25/04/06 13:44:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134412-0051/2 is now RUNNING
25/04/06 13:44:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:44:12 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:44:12 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:44:14 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:39954) with ID 2,  ResourceProfileId 0
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:37496) with ID 1,  ResourceProfileId 0
25/04/06 13:44:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:35002) with ID 0,  ResourceProfileId 0
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42943 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42943, None)
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:42831 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 42831, None)
25/04/06 13:44:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:40795 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 40795, None)
25/04/06 13:44:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:44:14 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:44:14 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:44:14 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:44:14 INFO DAGScheduler: Missing parents: List()
25/04/06 13:44:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:44:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:44:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:44:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39627 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:44:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:44:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:44:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:44:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:42831 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1176 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:44:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:44:15 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.269 s
25/04/06 13:44:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:44:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:44:15 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.303375 s
25/04/06 13:44:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39627 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:42831 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:44:16 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:44:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:44:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:44:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:44:16 INFO MemoryStore: MemoryStore cleared
25/04/06 13:44:16 INFO BlockManager: BlockManager stopped
25/04/06 13:44:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:44:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:44:17 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:44:17 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0db0e36-fec2-4345-88b3-b31bf6ef36be
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-e0db0e36-fec2-4345-88b3-b31bf6ef36be/pyspark-c3082c1f-36dc-428f-87c6-bf213955b723
25/04/06 13:44:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-8da67680-2bbb-42e5-a666-76b06aaedc89
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:46:11 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:46:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:46:11 INFO ResourceUtils: ==============================================================
25/04/06 13:46:11 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:46:11 INFO ResourceUtils: ==============================================================
25/04/06 13:46:11 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:46:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:46:11 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:46:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:46:11 INFO SecurityManager: Changing view acls to: root
25/04/06 13:46:11 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:46:11 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:46:11 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:46:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:46:11 INFO Utils: Successfully started service 'sparkDriver' on port 40275.
25/04/06 13:46:11 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:46:11 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:46:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:46:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-74ccc77a-a695-448a-aebf-4da956d5556a
25/04/06 13:46:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:46:11 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:46:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:46:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:46:11 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134611-0055
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134611-0055/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:46:11 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134611-0055/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:46:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40665.
25/04/06 13:46:11 INFO NettyBlockTransferService: Server created on f2a344a33cdc:40665
25/04/06 13:46:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:46:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:40665 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/1 is now RUNNING
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/2 is now RUNNING
25/04/06 13:46:11 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134611-0055/0 is now RUNNING
25/04/06 13:46:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 40665, None)
25/04/06 13:46:12 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:46:12 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:46:12 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:46:13 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:53708) with ID 2,  ResourceProfileId 0
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:43120) with ID 0,  ResourceProfileId 0
25/04/06 13:46:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48760) with ID 1,  ResourceProfileId 0
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:46741 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 46741, None)
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45873 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 45873, None)
25/04/06 13:46:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:34339 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 34339, None)
25/04/06 13:46:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:46:13 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:46:13 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:46:13 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:46:13 INFO DAGScheduler: Missing parents: List()
25/04/06 13:46:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:46:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:46:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:46:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:40665 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:46:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:46:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:46:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:46:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:34339 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1194 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:46:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:46:15 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.283 s
25/04/06 13:46:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:46:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:46:15 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.314317 s
25/04/06 13:46:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:40665 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:34339 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:46:16 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:46:16 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:46:16 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:46:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=292c2066-ddab-43c0-a853-26f46b1c4104, clientType=HIVECLI]
25/04/06 13:46:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:46:17 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:46:17 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:46:17 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:46:17 INFO metastore: Connected to metastore.
25/04/06 13:46:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:46:17 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:46:17 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:46:17 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:46:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:46:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:46:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:46:17 INFO CodeGenerator: Code generated in 155.999227 ms
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:46:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:40665 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:46:17 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:46:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:46:17 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:46:17 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:46:17 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:46:17 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:46:17 INFO DAGScheduler: Missing parents: List()
25/04/06 13:46:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:46:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:46:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:40665 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:46:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:46:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:46:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:46:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:45873 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:46:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:45873 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:46:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2021 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:46:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:46:19 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.051 s
25/04/06 13:46:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:46:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:46:19 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.060016 s
25/04/06 13:46:19 INFO FileFormatWriter: Start to commit write Job dab12bad-7bc3-40be-91ea-a1a17c781262.
25/04/06 13:46:20 INFO FileFormatWriter: Write Job dab12bad-7bc3-40be-91ea-a1a17c781262 committed. Elapsed time: 38 ms.
25/04/06 13:46:20 INFO FileFormatWriter: Finished processing stats for write job dab12bad-7bc3-40be-91ea-a1a17c781262.
25/04/06 13:46:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:46:20 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:46:20 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:46:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:46:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:46:20 INFO MemoryStore: MemoryStore cleared
25/04/06 13:46:20 INFO BlockManager: BlockManager stopped
25/04/06 13:46:20 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:46:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:46:20 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:46:20 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-5092ff7f-fec7-46a5-ad89-c8665fde40aa
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c9f29d0-60ed-41cc-a862-419a703f2690/pyspark-8416e3d5-610f-4a25-a6ee-f0c217efc461
25/04/06 13:46:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c9f29d0-60ed-41cc-a862-419a703f2690
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:47:07 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:47:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:47:07 INFO ResourceUtils: ==============================================================
25/04/06 13:47:07 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:47:07 INFO ResourceUtils: ==============================================================
25/04/06 13:47:07 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:47:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:47:07 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:47:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:47:07 INFO SecurityManager: Changing view acls to: root
25/04/06 13:47:07 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:47:07 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:47:07 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:47:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:47:07 INFO Utils: Successfully started service 'sparkDriver' on port 39047.
25/04/06 13:47:07 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:47:07 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:47:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:47:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:47:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:47:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8269376-cbc0-43cd-b0ad-8b9d2f50cd87
25/04/06 13:47:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:47:07 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:47:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:47:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:47:08 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134708-0058
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134708-0058/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134708-0058/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:47:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39023.
25/04/06 13:47:08 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39023
25/04/06 13:47:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:47:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39023 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/1 is now RUNNING
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/2 is now RUNNING
25/04/06 13:47:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134708-0058/0 is now RUNNING
25/04/06 13:47:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39023, None)
25/04/06 13:47:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:47:08 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:47:08 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:47:09 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/06 13:47:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:59254) with ID 2,  ResourceProfileId 0
25/04/06 13:47:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:46346) with ID 0,  ResourceProfileId 0
25/04/06 13:47:10 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35458) with ID 1,  ResourceProfileId 0
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:32837 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 32837, None)
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:38421 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 38421, None)
25/04/06 13:47:10 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:38799 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 38799, None)
25/04/06 13:47:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:47:10 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:47:10 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:47:10 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:47:10 INFO DAGScheduler: Missing parents: List()
25/04/06 13:47:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:47:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:47:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:47:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39023 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:47:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:47:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:47:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:47:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:38421 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1165 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:47:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:47:11 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.256 s
25/04/06 13:47:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:47:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:47:11 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.287696 s
25/04/06 13:47:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39023 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:38421 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:47:12 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:47:12 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:47:13 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:47:13 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4ddb2889-4b66-48fc-b2a7-cc78f104c08e, clientType=HIVECLI]
25/04/06 13:47:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:47:13 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:47:13 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:47:13 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:47:13 INFO metastore: Connected to metastore.
25/04/06 13:47:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:47:13 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:47:13 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:47:13 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:47:13 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:47:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:47:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:47:13 INFO CodeGenerator: Code generated in 150.005733 ms
25/04/06 13:47:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:47:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:47:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:39023 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:47:13 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:47:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:47:14 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:47:14 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:47:14 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:47:14 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:47:14 INFO DAGScheduler: Missing parents: List()
25/04/06 13:47:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:47:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:47:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:47:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:39023 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:47:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:47:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:47:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:47:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:47:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:32837 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:47:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:32837 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:47:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1971 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:47:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:47:16 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.002 s
25/04/06 13:47:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:47:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:47:16 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.009427 s
25/04/06 13:47:16 INFO FileFormatWriter: Start to commit write Job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93.
25/04/06 13:47:16 INFO FileFormatWriter: Write Job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93 committed. Elapsed time: 36 ms.
25/04/06 13:47:16 INFO FileFormatWriter: Finished processing stats for write job 397d7df7-f7d1-4aa6-a38c-c2a634bf7a93.
25/04/06 13:47:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:47:16 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:47:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:47:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:47:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:47:16 INFO MemoryStore: MemoryStore cleared
25/04/06 13:47:16 INFO BlockManager: BlockManager stopped
25/04/06 13:47:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:47:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:47:16 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:47:16 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-43612a98-3424-4484-9194-580101aae204
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-43612a98-3424-4484-9194-580101aae204/pyspark-1cb42c04-689b-4916-b35e-aa4fc42bd04d
25/04/06 13:47:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-d542e678-30be-4b06-97dd-a81d91170671
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:49:33 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:49:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:49:33 INFO ResourceUtils: ==============================================================
25/04/06 13:49:33 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:49:33 INFO ResourceUtils: ==============================================================
25/04/06 13:49:33 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:49:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:49:33 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:49:33 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:49:33 INFO SecurityManager: Changing view acls to: root
25/04/06 13:49:33 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:49:33 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:49:33 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:49:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:49:34 INFO Utils: Successfully started service 'sparkDriver' on port 41031.
25/04/06 13:49:34 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:49:34 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:49:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:49:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5857296a-89da-46d3-b18f-a872d8d8038f
25/04/06 13:49:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:49:34 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:49:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:49:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:49:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406134934-0061
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406134934-0061/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406134934-0061/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:49:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36319.
25/04/06 13:49:34 INFO NettyBlockTransferService: Server created on f2a344a33cdc:36319
25/04/06 13:49:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:49:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:36319 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/2 is now RUNNING
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/0 is now RUNNING
25/04/06 13:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406134934-0061/1 is now RUNNING
25/04/06 13:49:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 36319, None)
25/04/06 13:49:34 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:49:35 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:49:35 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:49:36 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:41172) with ID 0,  ResourceProfileId 0
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43894) with ID 2,  ResourceProfileId 0
25/04/06 13:49:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58714) with ID 1,  ResourceProfileId 0
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39109 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39109, None)
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45185 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45185, None)
25/04/06 13:49:36 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35835 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 35835, None)
25/04/06 13:49:36 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:49:36 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:49:36 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:49:36 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:49:36 INFO DAGScheduler: Missing parents: List()
25/04/06 13:49:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:49:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:49:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:49:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:36319 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:49:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:49:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:49:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:49:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:39109 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1173 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:49:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:49:37 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.264 s
25/04/06 13:49:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:49:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:49:37 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.294354 s
25/04/06 13:49:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:36319 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:39109 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:49:39 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:49:39 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:49:39 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:49:39 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:39 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=88b9c493-bf6f-4927-bb3e-da86e4546699, clientType=HIVECLI]
25/04/06 13:49:39 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:49:39 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:49:39 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:39 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:49:39 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:49:39 INFO metastore: Connected to metastore.
25/04/06 13:49:40 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:49:40 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:49:40 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:49:40 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:49:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:49:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:49:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:49:40 INFO CodeGenerator: Code generated in 145.936806 ms
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:36319 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:49:40 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:49:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:49:40 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:49:40 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:49:40 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:49:40 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:49:40 INFO DAGScheduler: Missing parents: List()
25/04/06 13:49:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:49:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:36319 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:49:40 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:49:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:49:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:49:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:49:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:35835 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:49:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:35835 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:49:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2069 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:49:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:49:42 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.097 s
25/04/06 13:49:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:49:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:49:42 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.104480 s
25/04/06 13:49:42 INFO FileFormatWriter: Start to commit write Job 264b7ffb-59e2-4e23-87f5-4c548900e36f.
25/04/06 13:49:42 INFO FileFormatWriter: Write Job 264b7ffb-59e2-4e23-87f5-4c548900e36f committed. Elapsed time: 40 ms.
25/04/06 13:49:42 INFO FileFormatWriter: Finished processing stats for write job 264b7ffb-59e2-4e23-87f5-4c548900e36f.
25/04/06 13:49:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:49:42 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:49:42 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:49:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:49:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:49:42 INFO MemoryStore: MemoryStore cleared
25/04/06 13:49:42 INFO BlockManager: BlockManager stopped
25/04/06 13:49:42 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:49:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:49:42 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:49:42 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-233bc9fd-a6fd-43f7-8c67-b84e86adc0ce
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-96a80e19-e12f-4513-acc1-6e85f425306d/pyspark-d5d7164f-ea32-415e-89e1-934ea7bea272
25/04/06 13:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-96a80e19-e12f-4513-acc1-6e85f425306d
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:50:29 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:50:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:50:29 INFO ResourceUtils: ==============================================================
25/04/06 13:50:29 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:50:29 INFO ResourceUtils: ==============================================================
25/04/06 13:50:29 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:50:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:50:29 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:50:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:50:29 INFO SecurityManager: Changing view acls to: root
25/04/06 13:50:29 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:50:29 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:50:29 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:50:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:50:30 INFO Utils: Successfully started service 'sparkDriver' on port 38265.
25/04/06 13:50:30 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:50:30 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:50:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:50:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4fab1ea2-a647-41fb-b8d3-6ea10dc702b4
25/04/06 13:50:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:50:30 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:50:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:50:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:50:30 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135030-0064
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135030-0064/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135030-0064/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:50:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45865.
25/04/06 13:50:30 INFO NettyBlockTransferService: Server created on f2a344a33cdc:45865
25/04/06 13:50:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:50:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:45865 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/1 is now RUNNING
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/2 is now RUNNING
25/04/06 13:50:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 45865, None)
25/04/06 13:50:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135030-0064/0 is now RUNNING
25/04/06 13:50:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:50:31 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:50:31 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:50:32 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:60274) with ID 0,  ResourceProfileId 0
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:52098) with ID 2,  ResourceProfileId 0
25/04/06 13:50:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42876) with ID 1,  ResourceProfileId 0
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39163 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39163, None)
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39677 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39677, None)
25/04/06 13:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44439 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44439, None)
25/04/06 13:50:32 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:50:32 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:50:32 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:50:32 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:50:32 INFO DAGScheduler: Missing parents: List()
25/04/06 13:50:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:50:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:50:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:50:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:45865 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:50:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:50:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:50:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:50:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:39677 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1169 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:50:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:50:34 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.263 s
25/04/06 13:50:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:50:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:50:34 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.296125 s
25/04/06 13:50:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:45865 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:39677 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:50:35 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:50:35 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:50:35 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:50:35 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:35 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=7897d5ed-9c7e-4893-a1d5-c181fc882bc5, clientType=HIVECLI]
25/04/06 13:50:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:50:35 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:50:35 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:50:35 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:50:35 INFO metastore: Connected to metastore.
25/04/06 13:50:36 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:50:36 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:50:36 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:50:36 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:50:36 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:50:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:50:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:50:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:50:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:50:36 INFO CodeGenerator: Code generated in 150.64367 ms
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:45865 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:50:36 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:50:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:50:36 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:50:36 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:50:36 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:50:36 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:50:36 INFO DAGScheduler: Missing parents: List()
25/04/06 13:50:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:50:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:45865 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:50:36 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:50:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:50:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:50:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:50:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:39163 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:50:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:39163 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:50:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2033 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:50:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:50:38 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.059 s
25/04/06 13:50:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:50:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:50:38 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.066270 s
25/04/06 13:50:38 INFO FileFormatWriter: Start to commit write Job 0eda766d-d2f7-419a-ba97-4006d52fc870.
25/04/06 13:50:38 INFO FileFormatWriter: Write Job 0eda766d-d2f7-419a-ba97-4006d52fc870 committed. Elapsed time: 38 ms.
25/04/06 13:50:38 INFO FileFormatWriter: Finished processing stats for write job 0eda766d-d2f7-419a-ba97-4006d52fc870.
25/04/06 13:50:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:50:38 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:50:38 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:50:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:50:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:50:38 INFO MemoryStore: MemoryStore cleared
25/04/06 13:50:38 INFO BlockManager: BlockManager stopped
25/04/06 13:50:38 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:50:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:50:38 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:50:38 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-68978a64-b98f-4231-af69-77024da6e4bb
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec385eb-5ec3-4280-a1b3-5eba92c4d659
25/04/06 13:50:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ec385eb-5ec3-4280-a1b3-5eba92c4d659/pyspark-fc7fd61d-7923-4cff-9cd1-0e5e7c5ce301
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:51:45 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:51:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:51:45 INFO ResourceUtils: ==============================================================
25/04/06 13:51:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:51:45 INFO ResourceUtils: ==============================================================
25/04/06 13:51:45 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:51:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:51:45 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:51:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:51:45 INFO SecurityManager: Changing view acls to: root
25/04/06 13:51:45 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:51:45 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:51:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:51:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:51:45 INFO Utils: Successfully started service 'sparkDriver' on port 38103.
25/04/06 13:51:45 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:51:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:51:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:51:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:51:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:51:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2fef2795-4294-4862-8206-eeed811fe5d4
25/04/06 13:51:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:51:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:51:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:51:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:51:46 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 25 ms (0 ms spent in bootstraps)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135146-0067
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135146-0067/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135146-0067/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:51:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38661.
25/04/06 13:51:46 INFO NettyBlockTransferService: Server created on f2a344a33cdc:38661
25/04/06 13:51:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:51:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:38661 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/0 is now RUNNING
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/1 is now RUNNING
25/04/06 13:51:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135146-0067/2 is now RUNNING
25/04/06 13:51:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 38661, None)
25/04/06 13:51:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:51:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:51:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:51:48 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:58324) with ID 0,  ResourceProfileId 0
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:36894) with ID 2,  ResourceProfileId 0
25/04/06 13:51:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35192) with ID 1,  ResourceProfileId 0
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39205 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39205, None)
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38387 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 38387, None)
25/04/06 13:51:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40599 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40599, None)
25/04/06 13:51:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:51:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:51:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:51:48 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:51:48 INFO DAGScheduler: Missing parents: List()
25/04/06 13:51:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:51:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:51:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:51:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:38661 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:51:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:51:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:51:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:51:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:40599 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1221 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:51:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:51:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.330 s
25/04/06 13:51:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:51:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:51:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.365568 s
25/04/06 13:51:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:38661 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:40599 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:51:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:51:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:51:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:51:51 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:51:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:51:51 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0ee92ad9-53d3-4a85-a0d8-83c7504cb1f9, clientType=HIVECLI]
25/04/06 13:51:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:51:52 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:51:52 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:51:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:51:52 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:51:52 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:51:52 INFO metastore: Connected to metastore.
25/04/06 13:51:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:51:52 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:51:52 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:51:52 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:51:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:51:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:51:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:51:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:51:52 INFO CodeGenerator: Code generated in 154.361292 ms
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:38661 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:51:52 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:51:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:51:52 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:51:52 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:51:52 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:51:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:51:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:51:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:51:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:38661 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:51:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:51:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:51:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:51:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:51:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:39205 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:51:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:39205 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:51:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2086 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:51:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:51:54 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.117 s
25/04/06 13:51:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:51:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:51:54 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.125195 s
25/04/06 13:51:54 INFO FileFormatWriter: Start to commit write Job ca7f7628-d041-4e7b-974f-2416903512b6.
25/04/06 13:51:54 INFO FileFormatWriter: Write Job ca7f7628-d041-4e7b-974f-2416903512b6 committed. Elapsed time: 40 ms.
25/04/06 13:51:54 INFO FileFormatWriter: Finished processing stats for write job ca7f7628-d041-4e7b-974f-2416903512b6.
25/04/06 13:51:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:51:54 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:51:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:51:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:51:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:51:54 INFO MemoryStore: MemoryStore cleared
25/04/06 13:51:54 INFO BlockManager: BlockManager stopped
25/04/06 13:51:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:51:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:51:54 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:51:55 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-28421016-aade-4b0f-9101-de87dff437d0
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-28421016-aade-4b0f-9101-de87dff437d0/pyspark-29c8e1b1-29f8-4850-b65e-229bcde877cb
25/04/06 13:51:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-9fe9a24a-042d-4b46-8f3d-0e60c5d78197
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:53:12 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:53:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:53:12 INFO ResourceUtils: ==============================================================
25/04/06 13:53:12 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:53:12 INFO ResourceUtils: ==============================================================
25/04/06 13:53:12 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:53:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:53:12 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:53:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:53:12 INFO SecurityManager: Changing view acls to: root
25/04/06 13:53:12 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:53:12 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:53:12 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:53:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:53:13 INFO Utils: Successfully started service 'sparkDriver' on port 43297.
25/04/06 13:53:13 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:53:13 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:53:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:53:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-52eeadb0-4d5b-41cb-b30c-8227f446e00e
25/04/06 13:53:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:53:13 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:53:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:53:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:53:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135313-0070
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135313-0070/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135313-0070/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:53:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34469.
25/04/06 13:53:13 INFO NettyBlockTransferService: Server created on f2a344a33cdc:34469
25/04/06 13:53:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:53:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:34469 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/0 is now RUNNING
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/1 is now RUNNING
25/04/06 13:53:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135313-0070/2 is now RUNNING
25/04/06 13:53:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 34469, None)
25/04/06 13:53:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:53:14 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:53:14 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:53:15 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:41226) with ID 0,  ResourceProfileId 0
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58192) with ID 1,  ResourceProfileId 0
25/04/06 13:53:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:54352) with ID 2,  ResourceProfileId 0
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40089 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40089, None)
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41373 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41373, None)
25/04/06 13:53:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45365 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45365, None)
25/04/06 13:53:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:53:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:53:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:53:15 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:53:15 INFO DAGScheduler: Missing parents: List()
25/04/06 13:53:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:53:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:53:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:53:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:34469 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:53:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:53:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:53:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:53:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:40089 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1205 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 13:53:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:53:17 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.299 s
25/04/06 13:53:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:53:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:53:17 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.333482 s
25/04/06 13:53:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:34469 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:40089 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:53:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:53:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:53:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:53:18 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:53:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:53:18 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=04efa787-0f3a-47f4-ab19-0dac169e72c6, clientType=HIVECLI]
25/04/06 13:53:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:53:19 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:53:19 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:53:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:19 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:53:19 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:53:19 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:53:19 INFO metastore: Connected to metastore.
25/04/06 13:53:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:53:19 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:53:19 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:53:19 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:53:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:53:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:53:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:53:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:53:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:53:19 INFO CodeGenerator: Code generated in 148.067268 ms
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:34469 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:53:19 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:53:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:53:19 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:53:19 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:53:19 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:53:19 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:53:19 INFO DAGScheduler: Missing parents: List()
25/04/06 13:53:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 13:53:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:34469 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:53:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:53:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:53:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:53:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:53:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:45365 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:53:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:45365 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:53:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2069 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:53:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:53:21 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.099 s
25/04/06 13:53:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:53:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:53:21 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.107527 s
25/04/06 13:53:21 INFO FileFormatWriter: Start to commit write Job 05fb17ee-f745-4c25-8ba8-db5e992b1e56.
25/04/06 13:53:21 INFO FileFormatWriter: Write Job 05fb17ee-f745-4c25-8ba8-db5e992b1e56 committed. Elapsed time: 42 ms.
25/04/06 13:53:21 INFO FileFormatWriter: Finished processing stats for write job 05fb17ee-f745-4c25-8ba8-db5e992b1e56.
25/04/06 13:53:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:53:21 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:53:21 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:53:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:53:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:53:21 INFO MemoryStore: MemoryStore cleared
25/04/06 13:53:21 INFO BlockManager: BlockManager stopped
25/04/06 13:53:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:53:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:53:21 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:53:22 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-076029e2-cd61-4aa0-9414-979aade8dabe
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-c261bd92-480c-40b0-b022-6e6c5a78e2ee
25/04/06 13:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-076029e2-cd61-4aa0-9414-979aade8dabe/pyspark-efa2a6eb-ed17-44eb-ba9c-8abddefa6536
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:57:50 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:57:50 INFO ResourceUtils: ==============================================================
25/04/06 13:57:50 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:57:50 INFO ResourceUtils: ==============================================================
25/04/06 13:57:50 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:57:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:57:50 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:57:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:57:50 INFO SecurityManager: Changing view acls to: root
25/04/06 13:57:50 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:57:50 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:57:50 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:57:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:57:50 INFO Utils: Successfully started service 'sparkDriver' on port 38825.
25/04/06 13:57:50 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:57:50 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:57:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:57:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:57:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:57:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-63d576f4-4142-4ec0-bc30-7fc77390485c
25/04/06 13:57:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:57:50 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:57:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:57:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:57:51 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135751-0072
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135751-0072/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135751-0072/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:57:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42283.
25/04/06 13:57:51 INFO NettyBlockTransferService: Server created on f2a344a33cdc:42283
25/04/06 13:57:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:57:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:42283 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/0 is now RUNNING
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/1 is now RUNNING
25/04/06 13:57:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 42283, None)
25/04/06 13:57:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135751-0072/2 is now RUNNING
25/04/06 13:57:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:57:51 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:57:51 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:57:52 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:33252) with ID 1,  ResourceProfileId 0
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:48972) with ID 2,  ResourceProfileId 0
25/04/06 13:57:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:40692) with ID 0,  ResourceProfileId 0
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41879 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41879, None)
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:38331 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 38331, None)
25/04/06 13:57:53 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45773 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 45773, None)
25/04/06 13:57:53 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:57:53 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:57:53 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:57:53 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:57:53 INFO DAGScheduler: Missing parents: List()
25/04/06 13:57:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:57:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:57:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:57:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:42283 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:57:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:57:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:57:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:57:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:38331 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1142 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:57:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:57:54 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.232 s
25/04/06 13:57:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:57:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:57:54 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.263166 s
25/04/06 13:57:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:42283 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:38331 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:57:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:57:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:57:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:57:55 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:57:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:57:55 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:57:56 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=de8842e6-a95d-4562-8dea-e7d27ef0ba74, clientType=HIVECLI]
25/04/06 13:57:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:57:56 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:57:56 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:57:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:56 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:57:56 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:57:56 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:57:56 INFO metastore: Connected to metastore.
25/04/06 13:57:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:57:56 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:57:56 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:57:56 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:57:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:57:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:57:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:57:57 INFO CodeGenerator: Code generated in 153.567956 ms
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:42283 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:57:57 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:57:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:57:57 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:57:57 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:57:57 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:57:57 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:57:57 INFO DAGScheduler: Missing parents: List()
25/04/06 13:57:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:57:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:42283 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:57:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:57:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.2, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.2:38331 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:57:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.2:38331 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:57:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 934 ms on 172.18.0.2 (executor 2) (1/1)
25/04/06 13:57:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:57:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.961 s
25/04/06 13:57:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:57:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:57:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.968674 s
25/04/06 13:57:58 INFO FileFormatWriter: Start to commit write Job fc294250-f550-48ec-a3c0-08eaa3805a34.
25/04/06 13:57:58 INFO FileFormatWriter: Write Job fc294250-f550-48ec-a3c0-08eaa3805a34 committed. Elapsed time: 36 ms.
25/04/06 13:57:58 INFO FileFormatWriter: Finished processing stats for write job fc294250-f550-48ec-a3c0-08eaa3805a34.
25/04/06 13:57:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:57:58 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:57:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:57:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:57:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:57:58 INFO MemoryStore: MemoryStore cleared
25/04/06 13:57:58 INFO BlockManager: BlockManager stopped
25/04/06 13:57:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:57:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:57:58 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:57:58 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed699ff1-5d74-47c8-b538-7fe25346ec96/pyspark-1f50683a-8d63-4436-bea3-e10cbf943180
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-9baa16f0-fb55-4119-bf62-38f508d49478
25/04/06 13:57:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed699ff1-5d74-47c8-b538-7fe25346ec96
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 13:59:49 INFO SparkContext: Running Spark version 3.2.2
25/04/06 13:59:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 13:59:50 INFO ResourceUtils: ==============================================================
25/04/06 13:59:50 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 13:59:50 INFO ResourceUtils: ==============================================================
25/04/06 13:59:50 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 13:59:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 13:59:50 INFO ResourceProfile: Limiting resource is cpu
25/04/06 13:59:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 13:59:50 INFO SecurityManager: Changing view acls to: root
25/04/06 13:59:50 INFO SecurityManager: Changing modify acls to: root
25/04/06 13:59:50 INFO SecurityManager: Changing view acls groups to: 
25/04/06 13:59:50 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 13:59:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 13:59:50 INFO Utils: Successfully started service 'sparkDriver' on port 46005.
25/04/06 13:59:50 INFO SparkEnv: Registering MapOutputTracker
25/04/06 13:59:50 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 13:59:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 13:59:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9c94c68a-0ad8-4b65-b87b-0afcf32c96c5
25/04/06 13:59:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 13:59:50 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 13:59:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 13:59:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 13:59:50 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406135950-0074
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406135950-0074/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 13:59:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406135950-0074/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 13:59:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33649.
25/04/06 13:59:50 INFO NettyBlockTransferService: Server created on f2a344a33cdc:33649
25/04/06 13:59:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 13:59:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:33649 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/0 is now RUNNING
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/2 is now RUNNING
25/04/06 13:59:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406135950-0074/1 is now RUNNING
25/04/06 13:59:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 33649, None)
25/04/06 13:59:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 13:59:51 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 13:59:51 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 13:59:52 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:55838) with ID 1,  ResourceProfileId 0
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:48552) with ID 2,  ResourceProfileId 0
25/04/06 13:59:52 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:56404) with ID 0,  ResourceProfileId 0
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:39441 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 39441, None)
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:38995 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 38995, None)
25/04/06 13:59:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:36719 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 36719, None)
25/04/06 13:59:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 13:59:52 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:59:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 13:59:52 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:59:52 INFO DAGScheduler: Missing parents: List()
25/04/06 13:59:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:59:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 13:59:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/06 13:59:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:33649 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 13:59:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:59:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 13:59:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 13:59:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:38995 (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1217 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:59:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 13:59:54 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.314 s
25/04/06 13:59:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:59:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 13:59:54 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.345683 s
25/04/06 13:59:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:33649 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:38995 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/06 13:59:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:59:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 13:59:55 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 13:59:55 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 13:59:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:59:55 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 13:59:56 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f6b09ab3-6b1c-40bd-8ec2-4a64038c5c29, clientType=HIVECLI]
25/04/06 13:59:56 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 13:59:56 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 13:59:56 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 13:59:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:56 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 13:59:56 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 13:59:56 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 13:59:56 INFO metastore: Connected to metastore.
25/04/06 13:59:56 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 13:59:56 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 13:59:56 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 13:59:56 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 13:59:56 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:56 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 13:59:56 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 13:59:56 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 13:59:57 INFO CodeGenerator: Code generated in 156.402704 ms
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:33649 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 13:59:57 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:59:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 13:59:57 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 13:59:57 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 13:59:57 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 13:59:57 INFO DAGScheduler: Parents of final stage: List()
25/04/06 13:59:57 INFO DAGScheduler: Missing parents: List()
25/04/06 13:59:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 13:59:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:33649 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:59:57 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 13:59:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 13:59:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 13:59:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:38995 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 13:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:38995 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 13:59:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 977 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 13:59:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 13:59:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.003 s
25/04/06 13:59:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 13:59:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 13:59:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.010796 s
25/04/06 13:59:58 INFO FileFormatWriter: Start to commit write Job d66dc65a-4c85-4f3a-a461-d9bb655210c5.
25/04/06 13:59:58 INFO FileFormatWriter: Write Job d66dc65a-4c85-4f3a-a461-d9bb655210c5 committed. Elapsed time: 38 ms.
25/04/06 13:59:58 INFO FileFormatWriter: Finished processing stats for write job d66dc65a-4c85-4f3a-a461-d9bb655210c5.
25/04/06 13:59:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 13:59:58 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 13:59:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 13:59:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 13:59:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 13:59:58 INFO MemoryStore: MemoryStore cleared
25/04/06 13:59:58 INFO BlockManager: BlockManager stopped
25/04/06 13:59:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 13:59:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 13:59:58 INFO SparkContext: Successfully stopped SparkContext
25/04/06 13:59:58 INFO ShutdownHookManager: Shutdown hook called
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d6bdeeb-a188-499b-946c-4564a45681ac/pyspark-b1b14910-8eeb-4b5e-8ba9-9281954e1206
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d6bdeeb-a188-499b-946c-4564a45681ac
25/04/06 13:59:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-66d7e59e-3897-4775-b72f-d73fcee399b0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 14:23:35 INFO SparkContext: Running Spark version 3.2.2
25/04/06 14:23:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 14:23:35 INFO ResourceUtils: ==============================================================
25/04/06 14:23:35 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 14:23:35 INFO ResourceUtils: ==============================================================
25/04/06 14:23:35 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 14:23:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 14:23:35 INFO ResourceProfile: Limiting resource is cpu
25/04/06 14:23:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 14:23:35 INFO SecurityManager: Changing view acls to: root
25/04/06 14:23:35 INFO SecurityManager: Changing modify acls to: root
25/04/06 14:23:35 INFO SecurityManager: Changing view acls groups to: 
25/04/06 14:23:35 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 14:23:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 14:23:35 INFO Utils: Successfully started service 'sparkDriver' on port 33223.
25/04/06 14:23:35 INFO SparkEnv: Registering MapOutputTracker
25/04/06 14:23:35 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 14:23:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 14:23:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 14:23:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 14:23:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d91a5e7-80ff-49ae-ba0d-8dcd8b212b36
25/04/06 14:23:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 14:23:35 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 14:23:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 14:23:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 14:23:36 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406142336-0076
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142336-0076/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142336-0076/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:23:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39325.
25/04/06 14:23:36 INFO NettyBlockTransferService: Server created on f2a344a33cdc:39325
25/04/06 14:23:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 14:23:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:39325 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/2 is now RUNNING
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/1 is now RUNNING
25/04/06 14:23:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142336-0076/0 is now RUNNING
25/04/06 14:23:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 39325, None)
25/04/06 14:23:36 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 14:23:36 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 14:23:36 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 14:23:37 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:59292) with ID 0,  ResourceProfileId 0
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:43234) with ID 2,  ResourceProfileId 0
25/04/06 14:23:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40700) with ID 1,  ResourceProfileId 0
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:40191 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 40191, None)
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45179 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45179, None)
25/04/06 14:23:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:36303 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 36303, None)
25/04/06 14:23:38 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 14:23:38 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:23:38 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 14:23:38 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:23:38 INFO DAGScheduler: Missing parents: List()
25/04/06 14:23:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:23:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 14:23:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 14:23:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:39325 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 14:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:23:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 14:23:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 14:23:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:40191 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1191 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:23:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 14:23:39 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.286 s
25/04/06 14:23:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 14:23:39 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.323137 s
25/04/06 14:23:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:39325 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:40191 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:23:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:23:40 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 14:23:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:23:40 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 14:23:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:40 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:23:40 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 14:23:41 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a4124908-cc9d-432d-8e09-a72823c43e68, clientType=HIVECLI]
25/04/06 14:23:41 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 14:23:41 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 14:23:41 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 14:23:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:41 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:23:41 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:23:41 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 14:23:41 INFO metastore: Connected to metastore.
25/04/06 14:23:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 14:23:41 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 14:23:41 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 14:23:41 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 14:23:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:23:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:23:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:23:42 INFO CodeGenerator: Code generated in 154.562926 ms
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:39325 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 14:23:42 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:23:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 14:23:42 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:23:42 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:23:42 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 14:23:42 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:23:42 INFO DAGScheduler: Missing parents: List()
25/04/06 14:23:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/06 14:23:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:39325 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:23:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 14:23:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:23:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 14:23:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 14:23:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:40191 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:23:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:40191 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 14:23:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 982 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:23:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 14:23:43 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.007 s
25/04/06 14:23:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:23:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 14:23:43 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.014978 s
25/04/06 14:23:43 INFO FileFormatWriter: Start to commit write Job 11b69889-b8b8-493a-a9b7-9f96f9b018f5.
25/04/06 14:23:43 INFO FileFormatWriter: Write Job 11b69889-b8b8-493a-a9b7-9f96f9b018f5 committed. Elapsed time: 36 ms.
25/04/06 14:23:43 INFO FileFormatWriter: Finished processing stats for write job 11b69889-b8b8-493a-a9b7-9f96f9b018f5.
25/04/06 14:23:43 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/06 14:23:43 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 14:23:43 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 14:23:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 14:23:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 14:23:43 INFO MemoryStore: MemoryStore cleared
25/04/06 14:23:43 INFO BlockManager: BlockManager stopped
25/04/06 14:23:43 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 14:23:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 14:23:43 INFO SparkContext: Successfully stopped SparkContext
25/04/06 14:23:43 INFO ShutdownHookManager: Shutdown hook called
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d51c49-7d1e-421b-918e-7d8aeb898606
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-3439da54-4ab5-454d-9a28-a6cb98a8c760/pyspark-660580b8-f79c-4dea-8133-6fb63e0ba834
25/04/06 14:23:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-3439da54-4ab5-454d-9a28-a6cb98a8c760
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/06 14:24:48 INFO SparkContext: Running Spark version 3.2.2
25/04/06 14:24:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/06 14:24:48 INFO ResourceUtils: ==============================================================
25/04/06 14:24:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/06 14:24:48 INFO ResourceUtils: ==============================================================
25/04/06 14:24:48 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/06 14:24:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/06 14:24:48 INFO ResourceProfile: Limiting resource is cpu
25/04/06 14:24:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/06 14:24:48 INFO SecurityManager: Changing view acls to: root
25/04/06 14:24:48 INFO SecurityManager: Changing modify acls to: root
25/04/06 14:24:48 INFO SecurityManager: Changing view acls groups to: 
25/04/06 14:24:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/06 14:24:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/06 14:24:48 INFO Utils: Successfully started service 'sparkDriver' on port 36325.
25/04/06 14:24:48 INFO SparkEnv: Registering MapOutputTracker
25/04/06 14:24:48 INFO SparkEnv: Registering BlockManagerMaster
25/04/06 14:24:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/06 14:24:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/06 14:24:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/06 14:24:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-caf3e547-60f6-47ae-9f0b-5b953efc1ab1
25/04/06 14:24:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/06 14:24:48 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/06 14:24:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/06 14:24:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://f2a344a33cdc:4040
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/06 14:24:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250406142449-0078
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/0 on worker-20250406115929-172.18.0.10-40655 (172.18.0.10:40655) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/0 on hostPort 172.18.0.10:40655 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/1 on worker-20250406115929-172.18.0.3-38295 (172.18.0.3:38295) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/1 on hostPort 172.18.0.3:38295 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250406142449-0078/2 on worker-20250406115929-172.18.0.2-44483 (172.18.0.2:44483) with 4 core(s)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: Granted executor ID app-20250406142449-0078/2 on hostPort 172.18.0.2:44483 with 4 core(s), 1024.0 MiB RAM
25/04/06 14:24:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44107.
25/04/06 14:24:49 INFO NettyBlockTransferService: Server created on f2a344a33cdc:44107
25/04/06 14:24:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/06 14:24:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO BlockManagerMasterEndpoint: Registering block manager f2a344a33cdc:44107 with 366.3 MiB RAM, BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/2 is now RUNNING
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/1 is now RUNNING
25/04/06 14:24:49 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250406142449-0078/0 is now RUNNING
25/04/06 14:24:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f2a344a33cdc, 44107, None)
25/04/06 14:24:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/06 14:24:49 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/06 14:24:49 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/06 14:24:50 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:51542) with ID 0,  ResourceProfileId 0
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35868) with ID 1,  ResourceProfileId 0
25/04/06 14:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:51132) with ID 2,  ResourceProfileId 0
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39177 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.10, 39177, None)
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44275 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44275, None)
25/04/06 14:24:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:45151 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.2, 45151, None)
25/04/06 14:24:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/06 14:24:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:24:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/06 14:24:51 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:24:51 INFO DAGScheduler: Missing parents: List()
25/04/06 14:24:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:24:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/06 14:24:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/06 14:24:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f2a344a33cdc:44107 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/06 14:24:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:24:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/06 14:24:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/06 14:24:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:44275 (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 172.18.0.3 (executor 1) (1/1)
25/04/06 14:24:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/06 14:24:52 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.247 s
25/04/06 14:24:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:24:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/06 14:24:52 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.277049 s
25/04/06 14:24:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f2a344a33cdc:44107 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:44275 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/06 14:24:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:24:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/06 14:24:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/06 14:24:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 14:24:54 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0e3ed39a-a047-4ace-8b5d-2af68b4cf6bc, clientType=HIVECLI]
25/04/06 14:24:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/06 14:24:54 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/06 14:24:54 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/06 14:24:54 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/06 14:24:54 INFO metastore: Connected to metastore.
25/04/06 14:24:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/06 14:24:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/06 14:24:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/06 14:24:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/06 14:24:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/06 14:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/06 14:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/06 14:24:55 INFO CodeGenerator: Code generated in 149.852151 ms
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f2a344a33cdc:44107 (size: 33.7 KiB, free: 366.3 MiB)
25/04/06 14:24:55 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:24:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/06 14:24:55 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/06 14:24:55 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/06 14:24:55 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/06 14:24:55 INFO DAGScheduler: Parents of final stage: List()
25/04/06 14:24:55 INFO DAGScheduler: Missing parents: List()
25/04/06 14:24:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.2 KiB, free 365.7 MiB)
25/04/06 14:24:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f2a344a33cdc:44107 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:24:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/06 14:24:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/06 14:24:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/06 14:24:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/06 14:24:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:39177 (size: 74.7 KiB, free: 366.2 MiB)
25/04/06 14:24:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:39177 (size: 33.7 KiB, free: 366.2 MiB)
25/04/06 14:24:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1982 ms on 172.18.0.10 (executor 0) (1/1)
25/04/06 14:24:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/06 14:24:57 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.006 s
25/04/06 14:24:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/06 14:24:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/06 14:24:57 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.012373 s
25/04/06 14:24:57 INFO FileFormatWriter: Start to commit write Job 09241579-d0a7-41d8-9cc9-f98ed2d942a9.
25/04/06 14:24:57 INFO FileFormatWriter: Write Job 09241579-d0a7-41d8-9cc9-f98ed2d942a9 committed. Elapsed time: 41 ms.
25/04/06 14:24:57 INFO FileFormatWriter: Finished processing stats for write job 09241579-d0a7-41d8-9cc9-f98ed2d942a9.
25/04/06 14:24:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/06 14:24:57 INFO SparkUI: Stopped Spark web UI at http://f2a344a33cdc:4040
25/04/06 14:24:57 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/06 14:24:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/06 14:24:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/06 14:24:57 INFO MemoryStore: MemoryStore cleared
25/04/06 14:24:57 INFO BlockManager: BlockManager stopped
25/04/06 14:24:57 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/06 14:24:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/06 14:24:57 INFO SparkContext: Successfully stopped SparkContext
25/04/06 14:24:57 INFO ShutdownHookManager: Shutdown hook called
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-7329fdb6-c55d-4088-9e3d-856be2185616
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-be92076b-4ff4-495a-b3ee-f01f00e9dcb9/pyspark-28c99f4a-9406-4f32-9abb-7930844c479d
25/04/06 14:24:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-be92076b-4ff4-495a-b3ee-f01f00e9dcb9
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/07 00:10:23 INFO SparkContext: Running Spark version 3.2.2
25/04/07 00:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/07 00:10:23 INFO ResourceUtils: ==============================================================
25/04/07 00:10:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/07 00:10:23 INFO ResourceUtils: ==============================================================
25/04/07 00:10:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/07 00:10:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/07 00:10:23 INFO ResourceProfile: Limiting resource is cpu
25/04/07 00:10:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/07 00:10:23 INFO SecurityManager: Changing view acls to: root
25/04/07 00:10:23 INFO SecurityManager: Changing modify acls to: root
25/04/07 00:10:23 INFO SecurityManager: Changing view acls groups to: 
25/04/07 00:10:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/07 00:10:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/07 00:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 34349.
25/04/07 00:10:24 INFO SparkEnv: Registering MapOutputTracker
25/04/07 00:10:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/07 00:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/07 00:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/07 00:10:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/07 00:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-370f8812-3cca-47bf-87a1-679217bb349f
25/04/07 00:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/07 00:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/07 00:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/07 00:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://cfd5ae002cf8:4040
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/07 00:10:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/07 00:10:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250407001024-0004
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407001024-0004/0 on worker-20250406221201-172.18.0.7-43403 (172.18.0.7:43403) with 4 core(s)
25/04/07 00:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407001024-0004/0 on hostPort 172.18.0.7:43403 with 4 core(s), 1024.0 MiB RAM
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407001024-0004/1 on worker-20250406221201-172.18.0.3-43441 (172.18.0.3:43441) with 4 core(s)
25/04/07 00:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407001024-0004/1 on hostPort 172.18.0.3:43441 with 4 core(s), 1024.0 MiB RAM
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407001024-0004/2 on worker-20250406221201-172.18.0.11-45757 (172.18.0.11:45757) with 4 core(s)
25/04/07 00:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407001024-0004/2 on hostPort 172.18.0.11:45757 with 4 core(s), 1024.0 MiB RAM
25/04/07 00:10:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41755.
25/04/07 00:10:24 INFO NettyBlockTransferService: Server created on cfd5ae002cf8:41755
25/04/07 00:10:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/07 00:10:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, cfd5ae002cf8, 41755, None)
25/04/07 00:10:24 INFO BlockManagerMasterEndpoint: Registering block manager cfd5ae002cf8:41755 with 366.3 MiB RAM, BlockManagerId(driver, cfd5ae002cf8, 41755, None)
25/04/07 00:10:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, cfd5ae002cf8, 41755, None)
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407001024-0004/0 is now RUNNING
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407001024-0004/2 is now RUNNING
25/04/07 00:10:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, cfd5ae002cf8, 41755, None)
25/04/07 00:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407001024-0004/1 is now RUNNING
25/04/07 00:10:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/07 00:10:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/07 00:10:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/07 00:10:26 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/07 00:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.11:54256) with ID 2,  ResourceProfileId 0
25/04/07 00:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:45052) with ID 1,  ResourceProfileId 0
25/04/07 00:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:57558) with ID 0,  ResourceProfileId 0
25/04/07 00:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.11:35003 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.11, 35003, None)
25/04/07 00:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44785 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44785, None)
25/04/07 00:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:35025 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 35025, None)
25/04/07 00:10:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/07 00:10:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 00:10:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/07 00:10:26 INFO DAGScheduler: Parents of final stage: List()
25/04/07 00:10:26 INFO DAGScheduler: Missing parents: List()
25/04/07 00:10:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 00:10:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/07 00:10:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/07 00:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on cfd5ae002cf8:41755 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 00:10:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/07 00:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 00:10:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/07 00:10:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/07 00:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:44785 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 00:10:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 172.18.0.3 (executor 1) (1/1)
25/04/07 00:10:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/07 00:10:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.236 s
25/04/07 00:10:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 00:10:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/07 00:10:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.268087 s
25/04/07 00:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on cfd5ae002cf8:41755 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 00:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:44785 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 00:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 00:10:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/07 00:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 00:10:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/07 00:10:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 00:10:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 00:10:29 INFO metastore: Connected to metastore.
25/04/07 00:10:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/07 00:10:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4c2f9726-1cca-4a80-98aa-de730ceec438, clientType=HIVECLI]
25/04/07 00:10:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/07 00:10:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/07 00:10:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/07 00:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 00:10:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 00:10:30 INFO metastore: Connected to metastore.
25/04/07 00:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 00:10:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/07 00:10:30 INFO metastore: Connected to metastore.
25/04/07 00:10:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/07 00:10:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/07 00:10:30 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/07 00:10:30 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/07 00:10:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 00:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 00:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 00:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 00:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 00:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 00:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 00:10:31 INFO CodeGenerator: Code generated in 143.612642 ms
25/04/07 00:10:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/07 00:10:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/07 00:10:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on cfd5ae002cf8:41755 (size: 33.7 KiB, free: 366.3 MiB)
25/04/07 00:10:31 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/07 00:10:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/07 00:10:31 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/07 00:10:31 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 00:10:31 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/07 00:10:31 INFO DAGScheduler: Parents of final stage: List()
25/04/07 00:10:31 INFO DAGScheduler: Missing parents: List()
25/04/07 00:10:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 00:10:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/07 00:10:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/07 00:10:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on cfd5ae002cf8:41755 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 00:10:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/07 00:10:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 00:10:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/07 00:10:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/07 00:10:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:35025 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 00:10:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:35025 (size: 33.7 KiB, free: 366.2 MiB)
25/04/07 00:10:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1943 ms on 172.18.0.7 (executor 0) (1/1)
25/04/07 00:10:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/07 00:10:33 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.970 s
25/04/07 00:10:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 00:10:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/07 00:10:33 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.978175 s
25/04/07 00:10:33 INFO FileFormatWriter: Start to commit write Job ee7d5e01-526f-43bd-aab5-f8cc05099bd5.
25/04/07 00:10:33 INFO FileFormatWriter: Write Job ee7d5e01-526f-43bd-aab5-f8cc05099bd5 committed. Elapsed time: 36 ms.
25/04/07 00:10:33 INFO FileFormatWriter: Finished processing stats for write job ee7d5e01-526f-43bd-aab5-f8cc05099bd5.
25/04/07 00:10:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/07 00:10:33 INFO SparkUI: Stopped Spark web UI at http://cfd5ae002cf8:4040
25/04/07 00:10:33 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/07 00:10:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/07 00:10:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/07 00:10:33 INFO MemoryStore: MemoryStore cleared
25/04/07 00:10:33 INFO BlockManager: BlockManager stopped
25/04/07 00:10:33 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/07 00:10:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/07 00:10:33 INFO SparkContext: Successfully stopped SparkContext
25/04/07 00:10:33 INFO ShutdownHookManager: Shutdown hook called
25/04/07 00:10:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5fcabf3-98c7-48f4-aafe-f376b2bf60ec/pyspark-fcf77351-165f-4375-9b7e-ab3d5925de4a
25/04/07 00:10:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-e5fcabf3-98c7-48f4-aafe-f376b2bf60ec
25/04/07 00:10:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-17b1daf2-b2b2-4a53-9b43-f1295ff1ea36
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/07 04:10:23 INFO SparkContext: Running Spark version 3.2.2
25/04/07 04:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/07 04:10:23 INFO ResourceUtils: ==============================================================
25/04/07 04:10:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/07 04:10:23 INFO ResourceUtils: ==============================================================
25/04/07 04:10:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/07 04:10:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/07 04:10:23 INFO ResourceProfile: Limiting resource is cpu
25/04/07 04:10:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/07 04:10:23 INFO SecurityManager: Changing view acls to: root
25/04/07 04:10:23 INFO SecurityManager: Changing modify acls to: root
25/04/07 04:10:23 INFO SecurityManager: Changing view acls groups to: 
25/04/07 04:10:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/07 04:10:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/07 04:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 33219.
25/04/07 04:10:24 INFO SparkEnv: Registering MapOutputTracker
25/04/07 04:10:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/07 04:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/07 04:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/07 04:10:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/07 04:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bdcb7c2c-b4f2-4c3c-815a-cb87c22c6805
25/04/07 04:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/07 04:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/07 04:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/07 04:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://cfd5ae002cf8:4040
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/07 04:10:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/07 04:10:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250407041024-0007
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407041024-0007/0 on worker-20250406221201-172.18.0.7-43403 (172.18.0.7:43403) with 4 core(s)
25/04/07 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407041024-0007/0 on hostPort 172.18.0.7:43403 with 4 core(s), 1024.0 MiB RAM
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407041024-0007/1 on worker-20250406221201-172.18.0.3-43441 (172.18.0.3:43441) with 4 core(s)
25/04/07 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407041024-0007/1 on hostPort 172.18.0.3:43441 with 4 core(s), 1024.0 MiB RAM
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407041024-0007/2 on worker-20250406221201-172.18.0.11-45757 (172.18.0.11:45757) with 4 core(s)
25/04/07 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407041024-0007/2 on hostPort 172.18.0.11:45757 with 4 core(s), 1024.0 MiB RAM
25/04/07 04:10:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38921.
25/04/07 04:10:24 INFO NettyBlockTransferService: Server created on cfd5ae002cf8:38921
25/04/07 04:10:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/07 04:10:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, cfd5ae002cf8, 38921, None)
25/04/07 04:10:24 INFO BlockManagerMasterEndpoint: Registering block manager cfd5ae002cf8:38921 with 366.3 MiB RAM, BlockManagerId(driver, cfd5ae002cf8, 38921, None)
25/04/07 04:10:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, cfd5ae002cf8, 38921, None)
25/04/07 04:10:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, cfd5ae002cf8, 38921, None)
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407041024-0007/0 is now RUNNING
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407041024-0007/2 is now RUNNING
25/04/07 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407041024-0007/1 is now RUNNING
25/04/07 04:10:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/07 04:10:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/07 04:10:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/07 04:10:26 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/07 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.11:49796) with ID 2,  ResourceProfileId 0
25/04/07 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:32868) with ID 1,  ResourceProfileId 0
25/04/07 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:43482) with ID 0,  ResourceProfileId 0
25/04/07 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.11:42003 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.11, 42003, None)
25/04/07 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41949 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41949, None)
25/04/07 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:33209 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 33209, None)
25/04/07 04:10:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/07 04:10:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 04:10:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/07 04:10:26 INFO DAGScheduler: Parents of final stage: List()
25/04/07 04:10:26 INFO DAGScheduler: Missing parents: List()
25/04/07 04:10:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 04:10:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/07 04:10:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/07 04:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on cfd5ae002cf8:38921 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 04:10:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/07 04:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 04:10:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/07 04:10:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/07 04:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:33209 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 04:10:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 172.18.0.7 (executor 0) (1/1)
25/04/07 04:10:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/07 04:10:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.235 s
25/04/07 04:10:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 04:10:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/07 04:10:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.268433 s
25/04/07 04:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on cfd5ae002cf8:38921 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 04:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.7:33209 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 04:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 04:10:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/07 04:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 04:10:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/07 04:10:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 04:10:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 04:10:29 INFO metastore: Connected to metastore.
25/04/07 04:10:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/07 04:10:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=6d730c29-cccd-4cf2-bdac-66cca9f60377, clientType=HIVECLI]
25/04/07 04:10:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/07 04:10:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/07 04:10:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/07 04:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 04:10:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 04:10:30 INFO metastore: Connected to metastore.
25/04/07 04:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 04:10:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/07 04:10:30 INFO metastore: Connected to metastore.
25/04/07 04:10:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/07 04:10:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/07 04:10:30 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/07 04:10:30 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/07 04:10:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 04:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 04:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 04:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 04:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 04:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 04:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 04:10:30 INFO CodeGenerator: Code generated in 156.919162 ms
25/04/07 04:10:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/07 04:10:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/07 04:10:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on cfd5ae002cf8:38921 (size: 33.7 KiB, free: 366.3 MiB)
25/04/07 04:10:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/07 04:10:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/07 04:10:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/07 04:10:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 04:10:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/07 04:10:30 INFO DAGScheduler: Parents of final stage: List()
25/04/07 04:10:30 INFO DAGScheduler: Missing parents: List()
25/04/07 04:10:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 04:10:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/07 04:10:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/07 04:10:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on cfd5ae002cf8:38921 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 04:10:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/07 04:10:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 04:10:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/07 04:10:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/07 04:10:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:33209 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 04:10:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:33209 (size: 33.7 KiB, free: 366.2 MiB)
25/04/07 04:10:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 945 ms on 172.18.0.7 (executor 0) (1/1)
25/04/07 04:10:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/07 04:10:31 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.970 s
25/04/07 04:10:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 04:10:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/07 04:10:31 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.977443 s
25/04/07 04:10:31 INFO FileFormatWriter: Start to commit write Job 6447dafe-c83e-4e4c-b215-83b14cf5e1f2.
25/04/07 04:10:31 INFO FileFormatWriter: Write Job 6447dafe-c83e-4e4c-b215-83b14cf5e1f2 committed. Elapsed time: 36 ms.
25/04/07 04:10:31 INFO FileFormatWriter: Finished processing stats for write job 6447dafe-c83e-4e4c-b215-83b14cf5e1f2.
25/04/07 04:10:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/07 04:10:31 INFO SparkUI: Stopped Spark web UI at http://cfd5ae002cf8:4040
25/04/07 04:10:31 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/07 04:10:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/07 04:10:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/07 04:10:31 INFO MemoryStore: MemoryStore cleared
25/04/07 04:10:31 INFO BlockManager: BlockManager stopped
25/04/07 04:10:31 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/07 04:10:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/07 04:10:31 INFO SparkContext: Successfully stopped SparkContext
25/04/07 04:10:31 INFO ShutdownHookManager: Shutdown hook called
25/04/07 04:10:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6aef5440-995d-440c-9c06-436dbd29f1ab
25/04/07 04:10:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6aef5440-995d-440c-9c06-436dbd29f1ab/pyspark-00288898-135d-4c64-8aaf-e4120e973e80
25/04/07 04:10:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-63295ff2-d5e4-438c-9e57-e45b752e94f1
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/07 08:10:23 INFO SparkContext: Running Spark version 3.2.2
25/04/07 08:10:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/07 08:10:23 INFO ResourceUtils: ==============================================================
25/04/07 08:10:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/07 08:10:23 INFO ResourceUtils: ==============================================================
25/04/07 08:10:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/07 08:10:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/07 08:10:23 INFO ResourceProfile: Limiting resource is cpu
25/04/07 08:10:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/07 08:10:23 INFO SecurityManager: Changing view acls to: root
25/04/07 08:10:23 INFO SecurityManager: Changing modify acls to: root
25/04/07 08:10:23 INFO SecurityManager: Changing view acls groups to: 
25/04/07 08:10:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/07 08:10:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/07 08:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 42199.
25/04/07 08:10:24 INFO SparkEnv: Registering MapOutputTracker
25/04/07 08:10:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/07 08:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/07 08:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/07 08:10:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/07 08:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9932a44d-ad7c-40c8-b97e-b2501e9194e3
25/04/07 08:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/07 08:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/07 08:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/07 08:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://cfd5ae002cf8:4040
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/07 08:10:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/07 08:10:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250407081024-0010
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407081024-0010/0 on worker-20250406221201-172.18.0.7-43403 (172.18.0.7:43403) with 4 core(s)
25/04/07 08:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407081024-0010/0 on hostPort 172.18.0.7:43403 with 4 core(s), 1024.0 MiB RAM
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407081024-0010/1 on worker-20250406221201-172.18.0.3-43441 (172.18.0.3:43441) with 4 core(s)
25/04/07 08:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407081024-0010/1 on hostPort 172.18.0.3:43441 with 4 core(s), 1024.0 MiB RAM
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407081024-0010/2 on worker-20250406221201-172.18.0.11-45757 (172.18.0.11:45757) with 4 core(s)
25/04/07 08:10:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34771.
25/04/07 08:10:24 INFO NettyBlockTransferService: Server created on cfd5ae002cf8:34771
25/04/07 08:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407081024-0010/2 on hostPort 172.18.0.11:45757 with 4 core(s), 1024.0 MiB RAM
25/04/07 08:10:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/07 08:10:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, cfd5ae002cf8, 34771, None)
25/04/07 08:10:24 INFO BlockManagerMasterEndpoint: Registering block manager cfd5ae002cf8:34771 with 366.3 MiB RAM, BlockManagerId(driver, cfd5ae002cf8, 34771, None)
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407081024-0010/2 is now RUNNING
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407081024-0010/0 is now RUNNING
25/04/07 08:10:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, cfd5ae002cf8, 34771, None)
25/04/07 08:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407081024-0010/1 is now RUNNING
25/04/07 08:10:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, cfd5ae002cf8, 34771, None)
25/04/07 08:10:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/07 08:10:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/07 08:10:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/07 08:10:26 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/07 08:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:41034) with ID 0,  ResourceProfileId 0
25/04/07 08:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:46284) with ID 1,  ResourceProfileId 0
25/04/07 08:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.11:38724) with ID 2,  ResourceProfileId 0
25/04/07 08:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.11:39025 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.11, 39025, None)
25/04/07 08:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41357 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41357, None)
25/04/07 08:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:43597 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.7, 43597, None)
25/04/07 08:10:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/07 08:10:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 08:10:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/07 08:10:26 INFO DAGScheduler: Parents of final stage: List()
25/04/07 08:10:26 INFO DAGScheduler: Missing parents: List()
25/04/07 08:10:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 08:10:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/07 08:10:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/07 08:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on cfd5ae002cf8:34771 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 08:10:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/07 08:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 08:10:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/07 08:10:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/07 08:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:41357 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 08:10:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1149 ms on 172.18.0.3 (executor 1) (1/1)
25/04/07 08:10:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/07 08:10:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.243 s
25/04/07 08:10:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 08:10:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/07 08:10:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.273112 s
25/04/07 08:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on cfd5ae002cf8:34771 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 08:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:41357 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 08:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 08:10:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/07 08:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 08:10:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/07 08:10:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 08:10:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 08:10:29 INFO metastore: Connected to metastore.
25/04/07 08:10:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/07 08:10:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=fd1f2c9b-3338-4569-9d13-1261e9425cca, clientType=HIVECLI]
25/04/07 08:10:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/07 08:10:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/07 08:10:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/07 08:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 08:10:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 08:10:30 INFO metastore: Connected to metastore.
25/04/07 08:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 08:10:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/07 08:10:30 INFO metastore: Connected to metastore.
25/04/07 08:10:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/07 08:10:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/07 08:10:30 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/07 08:10:30 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/07 08:10:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 08:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 08:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 08:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 08:10:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 08:10:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 08:10:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 08:10:30 INFO CodeGenerator: Code generated in 148.589637 ms
25/04/07 08:10:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/07 08:10:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/07 08:10:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on cfd5ae002cf8:34771 (size: 33.7 KiB, free: 366.3 MiB)
25/04/07 08:10:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/07 08:10:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/07 08:10:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/07 08:10:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 08:10:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/07 08:10:30 INFO DAGScheduler: Parents of final stage: List()
25/04/07 08:10:30 INFO DAGScheduler: Missing parents: List()
25/04/07 08:10:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 08:10:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/07 08:10:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/07 08:10:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on cfd5ae002cf8:34771 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 08:10:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/07 08:10:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 08:10:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/07 08:10:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.11, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/07 08:10:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.11:39025 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 08:10:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.11:39025 (size: 33.7 KiB, free: 366.2 MiB)
25/04/07 08:10:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1954 ms on 172.18.0.11 (executor 2) (1/1)
25/04/07 08:10:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/07 08:10:32 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.979 s
25/04/07 08:10:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 08:10:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/07 08:10:32 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.986838 s
25/04/07 08:10:32 INFO FileFormatWriter: Start to commit write Job 49cf2fcf-b049-427e-be75-537ad0c97f1f.
25/04/07 08:10:32 INFO FileFormatWriter: Write Job 49cf2fcf-b049-427e-be75-537ad0c97f1f committed. Elapsed time: 37 ms.
25/04/07 08:10:32 INFO FileFormatWriter: Finished processing stats for write job 49cf2fcf-b049-427e-be75-537ad0c97f1f.
25/04/07 08:10:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/07 08:10:32 INFO SparkUI: Stopped Spark web UI at http://cfd5ae002cf8:4040
25/04/07 08:10:32 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/07 08:10:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/07 08:10:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/07 08:10:32 INFO MemoryStore: MemoryStore cleared
25/04/07 08:10:32 INFO BlockManager: BlockManager stopped
25/04/07 08:10:32 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/07 08:10:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/07 08:10:32 INFO SparkContext: Successfully stopped SparkContext
25/04/07 08:10:32 INFO ShutdownHookManager: Shutdown hook called
25/04/07 08:10:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc7a89aa-af1c-4e07-91a7-7576cbc1c67f
25/04/07 08:10:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc7a89aa-af1c-4e07-91a7-7576cbc1c67f/pyspark-e8112c38-b4dc-4ca1-823e-3f8a50e12146
25/04/07 08:10:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-d612d150-29be-436e-b48e-c31001cde25c
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/07 12:10:24 INFO SparkContext: Running Spark version 3.2.2
25/04/07 12:10:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/07 12:10:24 INFO ResourceUtils: ==============================================================
25/04/07 12:10:24 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/07 12:10:24 INFO ResourceUtils: ==============================================================
25/04/07 12:10:24 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/07 12:10:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/07 12:10:24 INFO ResourceProfile: Limiting resource is cpu
25/04/07 12:10:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/07 12:10:24 INFO SecurityManager: Changing view acls to: root
25/04/07 12:10:24 INFO SecurityManager: Changing modify acls to: root
25/04/07 12:10:24 INFO SecurityManager: Changing view acls groups to: 
25/04/07 12:10:24 INFO SecurityManager: Changing modify acls groups to: 
25/04/07 12:10:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/07 12:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 44453.
25/04/07 12:10:24 INFO SparkEnv: Registering MapOutputTracker
25/04/07 12:10:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/07 12:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/07 12:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/07 12:10:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/07 12:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-33db65ce-5e56-48b6-bd42-38efca2410d4
25/04/07 12:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/07 12:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/07 12:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/07 12:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c4dafd3c6246:4040
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/07 12:10:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/07 12:10:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250407121024-0002
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407121024-0002/0 on worker-20250407112846-172.18.0.12-35705 (172.18.0.12:35705) with 4 core(s)
25/04/07 12:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407121024-0002/0 on hostPort 172.18.0.12:35705 with 4 core(s), 1024.0 MiB RAM
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407121024-0002/1 on worker-20250407112846-172.18.0.9-33429 (172.18.0.9:33429) with 4 core(s)
25/04/07 12:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407121024-0002/1 on hostPort 172.18.0.9:33429 with 4 core(s), 1024.0 MiB RAM
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250407121024-0002/2 on worker-20250407112846-172.18.0.6-32955 (172.18.0.6:32955) with 4 core(s)
25/04/07 12:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250407121024-0002/2 on hostPort 172.18.0.6:32955 with 4 core(s), 1024.0 MiB RAM
25/04/07 12:10:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39401.
25/04/07 12:10:24 INFO NettyBlockTransferService: Server created on c4dafd3c6246:39401
25/04/07 12:10:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/07 12:10:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c4dafd3c6246, 39401, None)
25/04/07 12:10:24 INFO BlockManagerMasterEndpoint: Registering block manager c4dafd3c6246:39401 with 366.3 MiB RAM, BlockManagerId(driver, c4dafd3c6246, 39401, None)
25/04/07 12:10:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c4dafd3c6246, 39401, None)
25/04/07 12:10:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c4dafd3c6246, 39401, None)
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407121024-0002/1 is now RUNNING
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407121024-0002/0 is now RUNNING
25/04/07 12:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250407121024-0002/2 is now RUNNING
25/04/07 12:10:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/07 12:10:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/07 12:10:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/07 12:10:26 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/07 12:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:58050) with ID 1,  ResourceProfileId 0
25/04/07 12:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:42806) with ID 0,  ResourceProfileId 0
25/04/07 12:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:42776) with ID 2,  ResourceProfileId 0
25/04/07 12:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33209 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 33209, None)
25/04/07 12:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:46171 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.12, 46171, None)
25/04/07 12:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:44909 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 44909, None)
25/04/07 12:10:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/07 12:10:27 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 12:10:27 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/07 12:10:27 INFO DAGScheduler: Parents of final stage: List()
25/04/07 12:10:27 INFO DAGScheduler: Missing parents: List()
25/04/07 12:10:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 12:10:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/07 12:10:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/07 12:10:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c4dafd3c6246:39401 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 12:10:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/07 12:10:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 12:10:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/07 12:10:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/07 12:10:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:33209 (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 12:10:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1171 ms on 172.18.0.9 (executor 1) (1/1)
25/04/07 12:10:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/07 12:10:28 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.262 s
25/04/07 12:10:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 12:10:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/07 12:10:28 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.293029 s
25/04/07 12:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c4dafd3c6246:39401 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 12:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:33209 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/07 12:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 12:10:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/07 12:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/07 12:10:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/07 12:10:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 12:10:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 12:10:29 INFO metastore: Connected to metastore.
25/04/07 12:10:30 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/07 12:10:31 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0f20920c-b283-47bb-830c-639369d9d4e0, clientType=HIVECLI]
25/04/07 12:10:31 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/07 12:10:31 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/07 12:10:31 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/07 12:10:31 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 12:10:31 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/07 12:10:31 INFO metastore: Connected to metastore.
25/04/07 12:10:31 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/07 12:10:31 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/07 12:10:31 INFO metastore: Connected to metastore.
25/04/07 12:10:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/07 12:10:31 INFO FileSourceStrategy: Pushed Filters: 
25/04/07 12:10:31 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/07 12:10:31 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: int, StockCode: int, Quantity: int, CustomerID: int, Country: string ... 1 more field>
25/04/07 12:10:31 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 12:10:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 12:10:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 12:10:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 12:10:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/07 12:10:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/07 12:10:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/07 12:10:31 INFO CodeGenerator: Code generated in 151.782566 ms
25/04/07 12:10:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/07 12:10:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.9 MiB)
25/04/07 12:10:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c4dafd3c6246:39401 (size: 33.7 KiB, free: 366.3 MiB)
25/04/07 12:10:31 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/07 12:10:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/07 12:10:31 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/07 12:10:31 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/07 12:10:31 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/07 12:10:31 INFO DAGScheduler: Parents of final stage: List()
25/04/07 12:10:31 INFO DAGScheduler: Missing parents: List()
25/04/07 12:10:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/07 12:10:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/07 12:10:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/07 12:10:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c4dafd3c6246:39401 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 12:10:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/07 12:10:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/07 12:10:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/07 12:10:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/07 12:10:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:44909 (size: 74.7 KiB, free: 366.2 MiB)
25/04/07 12:10:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:44909 (size: 33.7 KiB, free: 366.2 MiB)
25/04/07 12:10:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2023 ms on 172.18.0.6 (executor 2) (1/1)
25/04/07 12:10:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/07 12:10:33 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.049 s
25/04/07 12:10:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/07 12:10:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/07 12:10:33 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.057011 s
25/04/07 12:10:33 INFO FileFormatWriter: Start to commit write Job 0c1d171d-429a-4a93-8941-fd550a0a1e21.
25/04/07 12:10:33 INFO FileFormatWriter: Write Job 0c1d171d-429a-4a93-8941-fd550a0a1e21 committed. Elapsed time: 38 ms.
25/04/07 12:10:33 INFO FileFormatWriter: Finished processing stats for write job 0c1d171d-429a-4a93-8941-fd550a0a1e21.
25/04/07 12:10:33 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/07 12:10:33 INFO SparkUI: Stopped Spark web UI at http://c4dafd3c6246:4040
25/04/07 12:10:33 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/07 12:10:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/07 12:10:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/07 12:10:33 INFO MemoryStore: MemoryStore cleared
25/04/07 12:10:33 INFO BlockManager: BlockManager stopped
25/04/07 12:10:33 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/07 12:10:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/07 12:10:33 INFO SparkContext: Successfully stopped SparkContext
25/04/07 12:10:34 INFO ShutdownHookManager: Shutdown hook called
25/04/07 12:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ea49a90-788b-4f5b-917d-816aaa789ed7
25/04/07 12:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-ad6ec14c-bb29-476e-9a80-358977002248
25/04/07 12:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ea49a90-788b-4f5b-917d-816aaa789ed7/pyspark-1bf1d5c7-0446-47e3-a8ea-5727785e4a92
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 00:10:21 INFO SparkContext: Running Spark version 3.2.2
25/04/08 00:10:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 00:10:22 INFO ResourceUtils: ==============================================================
25/04/08 00:10:22 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 00:10:22 INFO ResourceUtils: ==============================================================
25/04/08 00:10:22 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 00:10:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 00:10:22 INFO ResourceProfile: Limiting resource is cpu
25/04/08 00:10:22 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 00:10:22 INFO SecurityManager: Changing view acls to: root
25/04/08 00:10:22 INFO SecurityManager: Changing modify acls to: root
25/04/08 00:10:22 INFO SecurityManager: Changing view acls groups to: 
25/04/08 00:10:22 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 00:10:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 00:10:22 INFO Utils: Successfully started service 'sparkDriver' on port 46207.
25/04/08 00:10:22 INFO SparkEnv: Registering MapOutputTracker
25/04/08 00:10:22 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 00:10:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 00:10:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 00:10:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 00:10:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9b6ad91b-ee2d-49c8-9af3-412ce9d65873
25/04/08 00:10:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 00:10:22 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 00:10:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 00:10:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://98d2d932c324:4040
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 00:10:22 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.18:7077 after 22 ms (0 ms spent in bootstraps)
25/04/08 00:10:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408001022-0004
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408001022-0004/0 on worker-20250407214357-172.18.0.4-36701 (172.18.0.4:36701) with 4 core(s)
25/04/08 00:10:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408001022-0004/0 on hostPort 172.18.0.4:36701 with 4 core(s), 1024.0 MiB RAM
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408001022-0004/1 on worker-20250407214357-172.18.0.7-35443 (172.18.0.7:35443) with 4 core(s)
25/04/08 00:10:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408001022-0004/1 on hostPort 172.18.0.7:35443 with 4 core(s), 1024.0 MiB RAM
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408001022-0004/2 on worker-20250407214357-172.18.0.6-37199 (172.18.0.6:37199) with 4 core(s)
25/04/08 00:10:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408001022-0004/2 on hostPort 172.18.0.6:37199 with 4 core(s), 1024.0 MiB RAM
25/04/08 00:10:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33139.
25/04/08 00:10:22 INFO NettyBlockTransferService: Server created on 98d2d932c324:33139
25/04/08 00:10:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 00:10:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 98d2d932c324, 33139, None)
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408001022-0004/1 is now RUNNING
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408001022-0004/2 is now RUNNING
25/04/08 00:10:22 INFO BlockManagerMasterEndpoint: Registering block manager 98d2d932c324:33139 with 366.3 MiB RAM, BlockManagerId(driver, 98d2d932c324, 33139, None)
25/04/08 00:10:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 98d2d932c324, 33139, None)
25/04/08 00:10:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 98d2d932c324, 33139, None)
25/04/08 00:10:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408001022-0004/0 is now RUNNING
25/04/08 00:10:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 00:10:23 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 00:10:23 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 00:10:24 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/08 00:10:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:49806) with ID 2,  ResourceProfileId 0
25/04/08 00:10:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:39312) with ID 1,  ResourceProfileId 0
25/04/08 00:10:24 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:49754) with ID 0,  ResourceProfileId 0
25/04/08 00:10:24 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:38513 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 38513, None)
25/04/08 00:10:24 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:39869 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.7, 39869, None)
25/04/08 00:10:24 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:42489 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.4, 42489, None)
25/04/08 00:10:24 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 00:10:24 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 00:10:24 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 00:10:24 INFO DAGScheduler: Parents of final stage: List()
25/04/08 00:10:24 INFO DAGScheduler: Missing parents: List()
25/04/08 00:10:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 00:10:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 00:10:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/08 00:10:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 98d2d932c324:33139 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 00:10:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 00:10:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 00:10:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 00:10:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 00:10:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.7:39869 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 00:10:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1156 ms on 172.18.0.7 (executor 1) (1/1)
25/04/08 00:10:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 00:10:26 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.246 s
25/04/08 00:10:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 00:10:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 00:10:26 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.280187 s
25/04/08 00:10:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 98d2d932c324:33139 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 00:10:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.7:39869 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 00:10:27 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 00:10:27 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 00:10:27 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 00:10:27 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 00:10:27 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 00:10:27 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 00:10:27 INFO metastore: Connected to metastore.
25/04/08 00:10:28 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 00:10:28 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=14818bb5-e9d4-4c32-8ff2-c683c91fe89a, clientType=HIVECLI]
25/04/08 00:10:28 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 00:10:28 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 00:10:28 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 00:10:28 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 00:10:28 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 00:10:28 INFO metastore: Connected to metastore.
25/04/08 00:10:28 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 00:10:28 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 00:10:28 INFO metastore: Connected to metastore.
25/04/08 00:10:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 42, in <module>
    logs.write.mode("append").insertInto(HIVE_TABLE)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Cannot write incompatible data to table '`default`.`unprocessedlogs`':
- Cannot safely cast 'InvoiceNo': string to int
- Cannot safely cast 'StockCode': string to int
- Cannot safely cast 'Quantity': string to int
- Cannot safely cast 'CustomerID': string to int
25/04/08 00:10:29 INFO SparkContext: Invoking stop() from shutdown hook
25/04/08 00:10:29 INFO SparkUI: Stopped Spark web UI at http://98d2d932c324:4040
25/04/08 00:10:29 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 00:10:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 00:10:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 00:10:29 INFO MemoryStore: MemoryStore cleared
25/04/08 00:10:29 INFO BlockManager: BlockManager stopped
25/04/08 00:10:29 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 00:10:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 00:10:29 INFO SparkContext: Successfully stopped SparkContext
25/04/08 00:10:29 INFO ShutdownHookManager: Shutdown hook called
25/04/08 00:10:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e40c0f2f-eee5-44e8-b33b-db8c3e3665c6
25/04/08 00:10:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e90ed399-debf-4584-8949-58d6f369a27f
25/04/08 00:10:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e40c0f2f-eee5-44e8-b33b-db8c3e3665c6/pyspark-b9bb5c57-f16e-44bf-8df3-9e78764b618d
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 04:10:23 INFO SparkContext: Running Spark version 3.2.2
25/04/08 04:10:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 04:10:24 INFO ResourceUtils: ==============================================================
25/04/08 04:10:24 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 04:10:24 INFO ResourceUtils: ==============================================================
25/04/08 04:10:24 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 04:10:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 04:10:24 INFO ResourceProfile: Limiting resource is cpu
25/04/08 04:10:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 04:10:24 INFO SecurityManager: Changing view acls to: root
25/04/08 04:10:24 INFO SecurityManager: Changing modify acls to: root
25/04/08 04:10:24 INFO SecurityManager: Changing view acls groups to: 
25/04/08 04:10:24 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 04:10:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 04:10:24 INFO Utils: Successfully started service 'sparkDriver' on port 43683.
25/04/08 04:10:24 INFO SparkEnv: Registering MapOutputTracker
25/04/08 04:10:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 04:10:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 04:10:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 04:10:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 04:10:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-669a86fb-3263-4cc6-849c-df8fe19637b6
25/04/08 04:10:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 04:10:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 04:10:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 04:10:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://98d2d932c324:4040
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 04:10:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.18:7077 after 23 ms (0 ms spent in bootstraps)
25/04/08 04:10:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408041024-0007
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408041024-0007/0 on worker-20250407214357-172.18.0.4-36701 (172.18.0.4:36701) with 4 core(s)
25/04/08 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408041024-0007/0 on hostPort 172.18.0.4:36701 with 4 core(s), 1024.0 MiB RAM
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408041024-0007/1 on worker-20250407214357-172.18.0.7-35443 (172.18.0.7:35443) with 4 core(s)
25/04/08 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408041024-0007/1 on hostPort 172.18.0.7:35443 with 4 core(s), 1024.0 MiB RAM
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408041024-0007/2 on worker-20250407214357-172.18.0.6-37199 (172.18.0.6:37199) with 4 core(s)
25/04/08 04:10:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408041024-0007/2 on hostPort 172.18.0.6:37199 with 4 core(s), 1024.0 MiB RAM
25/04/08 04:10:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41291.
25/04/08 04:10:24 INFO NettyBlockTransferService: Server created on 98d2d932c324:41291
25/04/08 04:10:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 04:10:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 98d2d932c324, 41291, None)
25/04/08 04:10:24 INFO BlockManagerMasterEndpoint: Registering block manager 98d2d932c324:41291 with 366.3 MiB RAM, BlockManagerId(driver, 98d2d932c324, 41291, None)
25/04/08 04:10:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 98d2d932c324, 41291, None)
25/04/08 04:10:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 98d2d932c324, 41291, None)
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408041024-0007/1 is now RUNNING
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408041024-0007/0 is now RUNNING
25/04/08 04:10:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408041024-0007/2 is now RUNNING
25/04/08 04:10:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 04:10:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 04:10:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 04:10:26 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/08 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:54240) with ID 0,  ResourceProfileId 0
25/04/08 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:53176) with ID 2,  ResourceProfileId 0
25/04/08 04:10:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:47294) with ID 1,  ResourceProfileId 0
25/04/08 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:35033 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.4, 35033, None)
25/04/08 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:45961 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 45961, None)
25/04/08 04:10:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:46611 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.7, 46611, None)
25/04/08 04:10:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 04:10:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 04:10:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 04:10:26 INFO DAGScheduler: Parents of final stage: List()
25/04/08 04:10:26 INFO DAGScheduler: Missing parents: List()
25/04/08 04:10:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 04:10:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 04:10:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/08 04:10:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 98d2d932c324:41291 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 04:10:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 04:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 04:10:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 04:10:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 04:10:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:35033 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 04:10:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1144 ms on 172.18.0.4 (executor 0) (1/1)
25/04/08 04:10:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 04:10:28 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.234 s
25/04/08 04:10:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 04:10:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 04:10:28 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.264458 s
25/04/08 04:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 98d2d932c324:41291 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 04:10:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.4:35033 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 04:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 04:10:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 04:10:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 04:10:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 04:10:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 04:10:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 04:10:29 INFO metastore: Connected to metastore.
25/04/08 04:10:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 04:10:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e2dd0918-c75e-411f-8e27-61c7783ab119, clientType=HIVECLI]
25/04/08 04:10:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 04:10:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 04:10:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 04:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 04:10:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 04:10:30 INFO metastore: Connected to metastore.
25/04/08 04:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 04:10:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 04:10:30 INFO metastore: Connected to metastore.
25/04/08 04:10:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 42, in <module>
    logs.write.mode("append").insertInto(HIVE_TABLE)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Cannot write incompatible data to table '`default`.`unprocessedlogs`':
- Cannot safely cast 'InvoiceNo': string to int
- Cannot safely cast 'StockCode': string to int
- Cannot safely cast 'Quantity': string to int
- Cannot safely cast 'CustomerID': string to int
25/04/08 04:10:30 INFO SparkContext: Invoking stop() from shutdown hook
25/04/08 04:10:30 INFO SparkUI: Stopped Spark web UI at http://98d2d932c324:4040
25/04/08 04:10:30 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 04:10:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 04:10:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 04:10:30 INFO MemoryStore: MemoryStore cleared
25/04/08 04:10:30 INFO BlockManager: BlockManager stopped
25/04/08 04:10:30 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 04:10:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 04:10:30 INFO SparkContext: Successfully stopped SparkContext
25/04/08 04:10:30 INFO ShutdownHookManager: Shutdown hook called
25/04/08 04:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-2de6a6dc-c9cb-481d-bf4b-b20e5c21491a
25/04/08 04:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-2de6a6dc-c9cb-481d-bf4b-b20e5c21491a/pyspark-332d1635-4e1b-477e-a506-16ea19bdc343
25/04/08 04:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-a18b4e84-8869-4dad-babc-af535ae5396e
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 06:56:42 INFO SparkContext: Running Spark version 3.2.2
25/04/08 06:56:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 06:56:43 INFO ResourceUtils: ==============================================================
25/04/08 06:56:43 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 06:56:43 INFO ResourceUtils: ==============================================================
25/04/08 06:56:43 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 06:56:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 06:56:43 INFO ResourceProfile: Limiting resource is cpu
25/04/08 06:56:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 06:56:43 INFO SecurityManager: Changing view acls to: root
25/04/08 06:56:43 INFO SecurityManager: Changing modify acls to: root
25/04/08 06:56:43 INFO SecurityManager: Changing view acls groups to: 
25/04/08 06:56:43 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 06:56:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 06:56:43 INFO Utils: Successfully started service 'sparkDriver' on port 38255.
25/04/08 06:56:43 INFO SparkEnv: Registering MapOutputTracker
25/04/08 06:56:43 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 06:56:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 06:56:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 06:56:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 06:56:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7de8e2d1-68a9-41b1-b356-d22d5a56ee2c
25/04/08 06:56:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 06:56:43 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 06:56:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 06:56:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://98d2d932c324:4040
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 06:56:43 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.18:7077 after 22 ms (0 ms spent in bootstraps)
25/04/08 06:56:43 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408065643-0009
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408065643-0009/0 on worker-20250407214357-172.18.0.4-36701 (172.18.0.4:36701) with 4 core(s)
25/04/08 06:56:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408065643-0009/0 on hostPort 172.18.0.4:36701 with 4 core(s), 1024.0 MiB RAM
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408065643-0009/1 on worker-20250407214357-172.18.0.7-35443 (172.18.0.7:35443) with 4 core(s)
25/04/08 06:56:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408065643-0009/1 on hostPort 172.18.0.7:35443 with 4 core(s), 1024.0 MiB RAM
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408065643-0009/2 on worker-20250407214357-172.18.0.6-37199 (172.18.0.6:37199) with 4 core(s)
25/04/08 06:56:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408065643-0009/2 on hostPort 172.18.0.6:37199 with 4 core(s), 1024.0 MiB RAM
25/04/08 06:56:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43041.
25/04/08 06:56:43 INFO NettyBlockTransferService: Server created on 98d2d932c324:43041
25/04/08 06:56:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 06:56:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 98d2d932c324, 43041, None)
25/04/08 06:56:43 INFO BlockManagerMasterEndpoint: Registering block manager 98d2d932c324:43041 with 366.3 MiB RAM, BlockManagerId(driver, 98d2d932c324, 43041, None)
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408065643-0009/1 is now RUNNING
25/04/08 06:56:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 98d2d932c324, 43041, None)
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408065643-0009/0 is now RUNNING
25/04/08 06:56:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 98d2d932c324, 43041, None)
25/04/08 06:56:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408065643-0009/2 is now RUNNING
25/04/08 06:56:44 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 06:56:44 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 06:56:44 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 06:56:45 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/08 06:56:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:58052) with ID 2,  ResourceProfileId 0
25/04/08 06:56:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:36630) with ID 0,  ResourceProfileId 0
25/04/08 06:56:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:38720) with ID 1,  ResourceProfileId 0
25/04/08 06:56:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:42967 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.4, 42967, None)
25/04/08 06:56:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:36093 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 36093, None)
25/04/08 06:56:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:40391 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.7, 40391, None)
25/04/08 06:56:46 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 06:56:46 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 06:56:46 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 06:56:46 INFO DAGScheduler: Parents of final stage: List()
25/04/08 06:56:46 INFO DAGScheduler: Missing parents: List()
25/04/08 06:56:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 06:56:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 06:56:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/08 06:56:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 98d2d932c324:43041 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 06:56:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 06:56:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 06:56:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 06:56:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 06:56:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:36093 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 06:56:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1261 ms on 172.18.0.6 (executor 2) (1/1)
25/04/08 06:56:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 06:56:47 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.353 s
25/04/08 06:56:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 06:56:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 06:56:47 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.386198 s
25/04/08 06:56:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 98d2d932c324:43041 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 06:56:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:36093 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 06:56:48 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 06:56:48 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 06:56:48 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 06:56:48 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 06:56:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 06:56:48 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 06:56:49 INFO metastore: Connected to metastore.
25/04/08 06:56:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 06:56:49 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=6f217937-6c67-40ac-8eb2-27835def619b, clientType=HIVECLI]
25/04/08 06:56:49 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 06:56:49 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 06:56:49 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 06:56:49 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 06:56:49 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 06:56:49 INFO metastore: Connected to metastore.
25/04/08 06:56:49 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 06:56:49 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 06:56:49 INFO metastore: Connected to metastore.
25/04/08 06:56:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_unprocessed_logs_into_hive.py", line 42, in <module>
    logs.write.mode("append").insertInto(HIVE_TABLE)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco
pyspark.sql.utils.AnalysisException: Cannot write incompatible data to table '`default`.`unprocessedlogs`':
- Cannot safely cast 'Quantity': string to int
25/04/08 06:56:50 INFO SparkContext: Invoking stop() from shutdown hook
25/04/08 06:56:50 INFO SparkUI: Stopped Spark web UI at http://98d2d932c324:4040
25/04/08 06:56:50 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 06:56:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 06:56:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 06:56:50 INFO MemoryStore: MemoryStore cleared
25/04/08 06:56:50 INFO BlockManager: BlockManager stopped
25/04/08 06:56:50 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 06:56:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 06:56:50 INFO SparkContext: Successfully stopped SparkContext
25/04/08 06:56:50 INFO ShutdownHookManager: Shutdown hook called
25/04/08 06:56:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-6df45497-c147-43d8-ae0d-690e1151022d
25/04/08 06:56:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa934d33-05f3-4198-a114-997ed53cd8d7
25/04/08 06:56:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-6df45497-c147-43d8-ae0d-690e1151022d/pyspark-736b0c8d-e27f-40c1-a1bc-3e18572cb252
Spark job FAILED!
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 07:07:36 INFO SparkContext: Running Spark version 3.2.2
25/04/08 07:07:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 07:07:36 INFO ResourceUtils: ==============================================================
25/04/08 07:07:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 07:07:36 INFO ResourceUtils: ==============================================================
25/04/08 07:07:36 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 07:07:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 07:07:36 INFO ResourceProfile: Limiting resource is cpu
25/04/08 07:07:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 07:07:36 INFO SecurityManager: Changing view acls to: root
25/04/08 07:07:36 INFO SecurityManager: Changing modify acls to: root
25/04/08 07:07:36 INFO SecurityManager: Changing view acls groups to: 
25/04/08 07:07:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 07:07:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 07:07:36 INFO Utils: Successfully started service 'sparkDriver' on port 41915.
25/04/08 07:07:36 INFO SparkEnv: Registering MapOutputTracker
25/04/08 07:07:36 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 07:07:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 07:07:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 07:07:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 07:07:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d2abef0c-925d-4420-ae63-6b1e1e306710
25/04/08 07:07:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 07:07:36 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 07:07:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 07:07:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://98d2d932c324:4040
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 07:07:36 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.18:7077 after 23 ms (0 ms spent in bootstraps)
25/04/08 07:07:36 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408070736-0015
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408070736-0015/0 on worker-20250407214357-172.18.0.4-36701 (172.18.0.4:36701) with 4 core(s)
25/04/08 07:07:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408070736-0015/0 on hostPort 172.18.0.4:36701 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408070736-0015/1 on worker-20250407214357-172.18.0.7-35443 (172.18.0.7:35443) with 4 core(s)
25/04/08 07:07:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408070736-0015/1 on hostPort 172.18.0.7:35443 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408070736-0015/2 on worker-20250407214357-172.18.0.6-37199 (172.18.0.6:37199) with 4 core(s)
25/04/08 07:07:36 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408070736-0015/2 on hostPort 172.18.0.6:37199 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:07:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40453.
25/04/08 07:07:36 INFO NettyBlockTransferService: Server created on 98d2d932c324:40453
25/04/08 07:07:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 07:07:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 98d2d932c324, 40453, None)
25/04/08 07:07:36 INFO BlockManagerMasterEndpoint: Registering block manager 98d2d932c324:40453 with 366.3 MiB RAM, BlockManagerId(driver, 98d2d932c324, 40453, None)
25/04/08 07:07:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 98d2d932c324, 40453, None)
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408070736-0015/1 is now RUNNING
25/04/08 07:07:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 98d2d932c324, 40453, None)
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408070736-0015/2 is now RUNNING
25/04/08 07:07:36 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408070736-0015/0 is now RUNNING
25/04/08 07:07:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 07:07:37 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 07:07:37 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 07:07:38 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/08 07:07:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:58814) with ID 0,  ResourceProfileId 0
25/04/08 07:07:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:33122) with ID 2,  ResourceProfileId 0
25/04/08 07:07:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:53250) with ID 1,  ResourceProfileId 0
25/04/08 07:07:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:34761 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.4, 34761, None)
25/04/08 07:07:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:36177 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 36177, None)
25/04/08 07:07:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:36889 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.7, 36889, None)
25/04/08 07:07:39 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 07:07:39 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 07:07:39 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 07:07:39 INFO DAGScheduler: Parents of final stage: List()
25/04/08 07:07:39 INFO DAGScheduler: Missing parents: List()
25/04/08 07:07:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 07:07:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 07:07:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/08 07:07:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 98d2d932c324:40453 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 07:07:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 07:07:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 07:07:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 07:07:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 07:07:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.4:34761 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 07:07:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1217 ms on 172.18.0.4 (executor 0) (1/1)
25/04/08 07:07:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 07:07:40 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.313 s
25/04/08 07:07:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 07:07:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 07:07:40 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.344783 s
25/04/08 07:07:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 98d2d932c324:40453 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 07:07:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.4:34761 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 07:07:41 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 07:07:41 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 07:07:41 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 07:07:41 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 07:07:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:07:41 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 07:07:41 INFO metastore: Connected to metastore.
25/04/08 07:07:42 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 07:07:42 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0755afc8-004e-42bd-8dca-30ad661f8a0e, clientType=HIVECLI]
25/04/08 07:07:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 07:07:42 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 07:07:42 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 07:07:42 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:07:42 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 07:07:42 INFO metastore: Connected to metastore.
25/04/08 07:07:42 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:07:42 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 07:07:42 INFO metastore: Connected to metastore.
25/04/08 07:07:42 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 07:07:42 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 07:07:42 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 07:07:42 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 07:07:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 07:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 07:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:07:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 07:07:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 07:07:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:07:43 INFO CodeGenerator: Code generated in 171.402692 ms
25/04/08 07:07:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 07:07:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 07:07:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 98d2d932c324:40453 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 07:07:43 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 07:07:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 07:07:43 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 07:07:43 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 07:07:43 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 07:07:43 INFO DAGScheduler: Parents of final stage: List()
25/04/08 07:07:43 INFO DAGScheduler: Missing parents: List()
25/04/08 07:07:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 07:07:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 07:07:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 07:07:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 98d2d932c324:40453 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 07:07:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 07:07:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 07:07:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 07:07:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 07:07:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:36889 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 07:07:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:36889 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 07:07:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2066 ms on 172.18.0.7 (executor 1) (1/1)
25/04/08 07:07:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 07:07:45 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.094 s
25/04/08 07:07:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 07:07:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 07:07:45 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.100707 s
25/04/08 07:07:45 INFO FileFormatWriter: Start to commit write Job 826187fd-62fe-4f47-bc08-58474db457d4.
25/04/08 07:07:45 INFO FileFormatWriter: Write Job 826187fd-62fe-4f47-bc08-58474db457d4 committed. Elapsed time: 39 ms.
25/04/08 07:07:45 INFO FileFormatWriter: Finished processing stats for write job 826187fd-62fe-4f47-bc08-58474db457d4.
25/04/08 07:07:45 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/08 07:07:45 INFO SparkUI: Stopped Spark web UI at http://98d2d932c324:4040
25/04/08 07:07:45 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 07:07:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 07:07:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 07:07:45 INFO MemoryStore: MemoryStore cleared
25/04/08 07:07:45 INFO BlockManager: BlockManager stopped
25/04/08 07:07:45 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 07:07:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 07:07:45 INFO SparkContext: Successfully stopped SparkContext
25/04/08 07:07:46 INFO ShutdownHookManager: Shutdown hook called
25/04/08 07:07:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-61f10d21-378a-4661-b4a1-e50e1806d3a3/pyspark-a859c0e5-a3af-4805-ba94-6a0c6e754304
25/04/08 07:07:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-61f10d21-378a-4661-b4a1-e50e1806d3a3
25/04/08 07:07:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2b20e72-9451-49ac-b835-b3d93d53dc25
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 07:24:05 INFO SparkContext: Running Spark version 3.2.2
25/04/08 07:24:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 07:24:05 INFO ResourceUtils: ==============================================================
25/04/08 07:24:05 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 07:24:05 INFO ResourceUtils: ==============================================================
25/04/08 07:24:05 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 07:24:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 07:24:05 INFO ResourceProfile: Limiting resource is cpu
25/04/08 07:24:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 07:24:05 INFO SecurityManager: Changing view acls to: root
25/04/08 07:24:05 INFO SecurityManager: Changing modify acls to: root
25/04/08 07:24:05 INFO SecurityManager: Changing view acls groups to: 
25/04/08 07:24:05 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 07:24:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 07:24:05 INFO Utils: Successfully started service 'sparkDriver' on port 34385.
25/04/08 07:24:05 INFO SparkEnv: Registering MapOutputTracker
25/04/08 07:24:05 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 07:24:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 07:24:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 07:24:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 07:24:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ebc91007-38fb-4888-8eab-43de8211fc02
25/04/08 07:24:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 07:24:05 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 07:24:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 07:24:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://98d2d932c324:4040
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 07:24:06 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.18:7077 after 23 ms (0 ms spent in bootstraps)
25/04/08 07:24:06 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408072406-0025
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408072406-0025/0 on worker-20250407214357-172.18.0.4-36701 (172.18.0.4:36701) with 4 core(s)
25/04/08 07:24:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408072406-0025/0 on hostPort 172.18.0.4:36701 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408072406-0025/1 on worker-20250407214357-172.18.0.7-35443 (172.18.0.7:35443) with 4 core(s)
25/04/08 07:24:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408072406-0025/1 on hostPort 172.18.0.7:35443 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408072406-0025/2 on worker-20250407214357-172.18.0.6-37199 (172.18.0.6:37199) with 4 core(s)
25/04/08 07:24:06 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408072406-0025/2 on hostPort 172.18.0.6:37199 with 4 core(s), 1024.0 MiB RAM
25/04/08 07:24:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36049.
25/04/08 07:24:06 INFO NettyBlockTransferService: Server created on 98d2d932c324:36049
25/04/08 07:24:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 07:24:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 98d2d932c324, 36049, None)
25/04/08 07:24:06 INFO BlockManagerMasterEndpoint: Registering block manager 98d2d932c324:36049 with 366.3 MiB RAM, BlockManagerId(driver, 98d2d932c324, 36049, None)
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408072406-0025/2 is now RUNNING
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408072406-0025/0 is now RUNNING
25/04/08 07:24:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408072406-0025/1 is now RUNNING
25/04/08 07:24:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 98d2d932c324, 36049, None)
25/04/08 07:24:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 98d2d932c324, 36049, None)
25/04/08 07:24:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 07:24:06 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 07:24:06 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 07:24:07 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/08 07:24:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:52228) with ID 2,  ResourceProfileId 0
25/04/08 07:24:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:40286) with ID 0,  ResourceProfileId 0
25/04/08 07:24:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:55862) with ID 1,  ResourceProfileId 0
25/04/08 07:24:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:39637 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.6, 39637, None)
25/04/08 07:24:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:42343 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.7, 42343, None)
25/04/08 07:24:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:36983 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.4, 36983, None)
25/04/08 07:24:08 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 07:24:08 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 07:24:08 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 07:24:08 INFO DAGScheduler: Parents of final stage: List()
25/04/08 07:24:08 INFO DAGScheduler: Missing parents: List()
25/04/08 07:24:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 07:24:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 07:24:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/08 07:24:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 98d2d932c324:36049 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 07:24:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 07:24:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 07:24:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 07:24:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 07:24:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:39637 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 07:24:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1193 ms on 172.18.0.6 (executor 2) (1/1)
25/04/08 07:24:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 07:24:09 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.298 s
25/04/08 07:24:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 07:24:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 07:24:09 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.332501 s
25/04/08 07:24:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 98d2d932c324:36049 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 07:24:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:39637 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 07:24:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 07:24:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 07:24:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 07:24:11 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 07:24:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:24:11 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 07:24:11 INFO metastore: Connected to metastore.
25/04/08 07:24:11 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 07:24:11 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=46fb8bd0-a3ce-489b-9984-29ed0a661363, clientType=HIVECLI]
25/04/08 07:24:11 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 07:24:11 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 07:24:11 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 07:24:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:24:11 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 07:24:11 INFO metastore: Connected to metastore.
25/04/08 07:24:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 07:24:11 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 07:24:11 INFO metastore: Connected to metastore.
25/04/08 07:24:12 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 07:24:12 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 07:24:12 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 07:24:12 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 07:24:12 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 07:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 07:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:24:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 07:24:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 07:24:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 07:24:12 INFO CodeGenerator: Code generated in 155.208273 ms
25/04/08 07:24:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 07:24:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 07:24:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 98d2d932c324:36049 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 07:24:12 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 07:24:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 07:24:12 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 07:24:12 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 07:24:12 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 07:24:12 INFO DAGScheduler: Parents of final stage: List()
25/04/08 07:24:12 INFO DAGScheduler: Missing parents: List()
25/04/08 07:24:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 07:24:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 07:24:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 07:24:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 98d2d932c324:36049 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 07:24:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 07:24:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 07:24:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 07:24:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 07:24:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:39637 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 07:24:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:39637 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 07:24:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1003 ms on 172.18.0.6 (executor 2) (1/1)
25/04/08 07:24:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 07:24:13 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.028 s
25/04/08 07:24:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 07:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 07:24:13 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.035654 s
25/04/08 07:24:13 INFO FileFormatWriter: Start to commit write Job d7f92783-27f4-452e-a357-1e7daaea7c91.
25/04/08 07:24:13 INFO FileFormatWriter: Write Job d7f92783-27f4-452e-a357-1e7daaea7c91 committed. Elapsed time: 38 ms.
25/04/08 07:24:13 INFO FileFormatWriter: Finished processing stats for write job d7f92783-27f4-452e-a357-1e7daaea7c91.
25/04/08 07:24:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/08 07:24:13 INFO SparkUI: Stopped Spark web UI at http://98d2d932c324:4040
25/04/08 07:24:13 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 07:24:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 07:24:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 07:24:13 INFO MemoryStore: MemoryStore cleared
25/04/08 07:24:13 INFO BlockManager: BlockManager stopped
25/04/08 07:24:13 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 07:24:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 07:24:13 INFO SparkContext: Successfully stopped SparkContext
25/04/08 07:24:13 INFO ShutdownHookManager: Shutdown hook called
25/04/08 07:24:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-992301a6-fc69-454e-9b5e-828522239161/pyspark-b034f685-e0ef-4099-9d0c-ee34f12de0b3
25/04/08 07:24:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-fef87dd3-2d91-4b66-886e-8c4e5826ab30
25/04/08 07:24:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-992301a6-fc69-454e-9b5e-828522239161
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 08:10:25 INFO SparkContext: Running Spark version 3.2.2
25/04/08 08:10:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 08:10:25 INFO ResourceUtils: ==============================================================
25/04/08 08:10:25 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 08:10:25 INFO ResourceUtils: ==============================================================
25/04/08 08:10:25 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 08:10:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 08:10:25 INFO ResourceProfile: Limiting resource is cpu
25/04/08 08:10:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 08:10:25 INFO SecurityManager: Changing view acls to: root
25/04/08 08:10:25 INFO SecurityManager: Changing modify acls to: root
25/04/08 08:10:25 INFO SecurityManager: Changing view acls groups to: 
25/04/08 08:10:25 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 08:10:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 08:10:25 INFO Utils: Successfully started service 'sparkDriver' on port 42507.
25/04/08 08:10:25 INFO SparkEnv: Registering MapOutputTracker
25/04/08 08:10:25 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 08:10:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 08:10:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 08:10:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 08:10:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-91ff5b83-118e-43b4-9f35-2d58c3180014
25/04/08 08:10:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 08:10:25 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 08:10:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 08:10:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 08:10:25 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/08 08:10:25 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408081025-0002
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408081025-0002/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/08 08:10:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408081025-0002/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408081025-0002/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/08 08:10:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408081025-0002/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408081025-0002/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/08 08:10:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408081025-0002/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/08 08:10:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45537.
25/04/08 08:10:25 INFO NettyBlockTransferService: Server created on 3fada93ce917:45537
25/04/08 08:10:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 08:10:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 45537, None)
25/04/08 08:10:25 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:45537 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 45537, None)
25/04/08 08:10:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 45537, None)
25/04/08 08:10:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 45537, None)
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408081025-0002/0 is now RUNNING
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408081025-0002/1 is now RUNNING
25/04/08 08:10:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408081025-0002/2 is now RUNNING
25/04/08 08:10:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 08:10:26 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 08:10:26 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 08:10:27 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/08 08:10:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:56984) with ID 2,  ResourceProfileId 0
25/04/08 08:10:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:52404) with ID 1,  ResourceProfileId 0
25/04/08 08:10:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:49816) with ID 0,  ResourceProfileId 0
25/04/08 08:10:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:41951 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 41951, None)
25/04/08 08:10:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:38935 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 38935, None)
25/04/08 08:10:27 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:35497 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 35497, None)
25/04/08 08:10:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 08:10:27 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 08:10:27 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 08:10:27 INFO DAGScheduler: Parents of final stage: List()
25/04/08 08:10:27 INFO DAGScheduler: Missing parents: List()
25/04/08 08:10:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 08:10:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 08:10:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/08 08:10:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:45537 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 08:10:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 08:10:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 08:10:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 08:10:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 08:10:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:41951 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 08:10:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1143 ms on 172.18.0.12 (executor 2) (1/1)
25/04/08 08:10:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 08:10:29 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.246 s
25/04/08 08:10:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 08:10:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 08:10:29 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.279714 s
25/04/08 08:10:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:45537 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 08:10:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:41951 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 08:10:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 08:10:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 08:10:30 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 08:10:30 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 08:10:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 08:10:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 08:10:30 INFO metastore: Connected to metastore.
25/04/08 08:10:31 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/08 08:10:31 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=231cfd91-6791-4e1c-9a54-a29567507c0c, clientType=HIVECLI]
25/04/08 08:10:31 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 08:10:31 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 08:10:31 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 08:10:31 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 08:10:31 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 08:10:31 INFO metastore: Connected to metastore.
25/04/08 08:10:31 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 08:10:31 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 08:10:31 INFO metastore: Connected to metastore.
25/04/08 08:10:32 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 08:10:32 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 08:10:32 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 08:10:32 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 08:10:32 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 08:10:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 08:10:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 08:10:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 08:10:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 08:10:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 08:10:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 08:10:32 INFO CodeGenerator: Code generated in 144.905085 ms
25/04/08 08:10:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 08:10:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 08:10:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:45537 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 08:10:32 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 08:10:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 08:10:32 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 08:10:32 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 08:10:32 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 08:10:32 INFO DAGScheduler: Parents of final stage: List()
25/04/08 08:10:32 INFO DAGScheduler: Missing parents: List()
25/04/08 08:10:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 08:10:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 08:10:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 08:10:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:45537 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 08:10:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 08:10:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 08:10:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 08:10:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 08:10:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:38935 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 08:10:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:38935 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 08:10:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1916 ms on 172.18.0.3 (executor 1) (1/1)
25/04/08 08:10:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 08:10:34 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.940 s
25/04/08 08:10:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 08:10:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 08:10:34 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.947762 s
25/04/08 08:10:34 INFO FileFormatWriter: Start to commit write Job 0c110752-9284-4937-8a56-7f23e4ab7fc5.
25/04/08 08:10:34 INFO FileFormatWriter: Write Job 0c110752-9284-4937-8a56-7f23e4ab7fc5 committed. Elapsed time: 37 ms.
25/04/08 08:10:34 INFO FileFormatWriter: Finished processing stats for write job 0c110752-9284-4937-8a56-7f23e4ab7fc5.
25/04/08 08:10:34 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/08 08:10:34 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/08 08:10:34 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 08:10:34 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 08:10:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 08:10:34 INFO MemoryStore: MemoryStore cleared
25/04/08 08:10:34 INFO BlockManager: BlockManager stopped
25/04/08 08:10:34 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 08:10:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 08:10:34 INFO SparkContext: Successfully stopped SparkContext
25/04/08 08:10:34 INFO ShutdownHookManager: Shutdown hook called
25/04/08 08:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-66ae7170-8a7b-4cbb-b82e-d079c562ec6b/pyspark-a18b02d4-6cea-4452-8b2d-610dcea4cc9a
25/04/08 08:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-d47f4cd1-9271-4ead-be3d-52e73ca4d0e8
25/04/08 08:10:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-66ae7170-8a7b-4cbb-b82e-d079c562ec6b
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 14:33:23 INFO SparkContext: Running Spark version 3.2.2
25/04/08 14:33:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 14:33:23 INFO ResourceUtils: ==============================================================
25/04/08 14:33:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 14:33:23 INFO ResourceUtils: ==============================================================
25/04/08 14:33:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 14:33:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 14:33:23 INFO ResourceProfile: Limiting resource is cpu
25/04/08 14:33:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 14:33:23 INFO SecurityManager: Changing view acls to: root
25/04/08 14:33:23 INFO SecurityManager: Changing modify acls to: root
25/04/08 14:33:23 INFO SecurityManager: Changing view acls groups to: 
25/04/08 14:33:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 14:33:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 14:33:23 INFO Utils: Successfully started service 'sparkDriver' on port 36373.
25/04/08 14:33:23 INFO SparkEnv: Registering MapOutputTracker
25/04/08 14:33:23 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 14:33:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 14:33:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 14:33:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 14:33:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2384236b-119b-412a-9355-e65356d4e8cd
25/04/08 14:33:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 14:33:23 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 14:33:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 14:33:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 14:33:23 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 21 ms (0 ms spent in bootstraps)
25/04/08 14:33:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408143323-0005
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408143323-0005/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/08 14:33:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408143323-0005/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408143323-0005/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/08 14:33:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408143323-0005/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408143323-0005/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/08 14:33:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408143323-0005/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/08 14:33:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39965.
25/04/08 14:33:23 INFO NettyBlockTransferService: Server created on 3fada93ce917:39965
25/04/08 14:33:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 14:33:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39965, None)
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408143323-0005/0 is now RUNNING
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408143323-0005/1 is now RUNNING
25/04/08 14:33:23 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39965 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39965, None)
25/04/08 14:33:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39965, None)
25/04/08 14:33:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39965, None)
25/04/08 14:33:23 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408143323-0005/2 is now RUNNING
25/04/08 14:33:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 14:33:24 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 14:33:24 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 14:33:25 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/08 14:33:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:33576) with ID 2,  ResourceProfileId 0
25/04/08 14:33:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:57544) with ID 0,  ResourceProfileId 0
25/04/08 14:33:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40336) with ID 1,  ResourceProfileId 0
25/04/08 14:33:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:40237 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 40237, None)
25/04/08 14:33:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:42497 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 42497, None)
25/04/08 14:33:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40909 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40909, None)
25/04/08 14:33:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 14:33:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 14:33:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 14:33:26 INFO DAGScheduler: Parents of final stage: List()
25/04/08 14:33:26 INFO DAGScheduler: Missing parents: List()
25/04/08 14:33:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 14:33:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 14:33:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/08 14:33:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39965 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 14:33:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 14:33:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 14:33:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 14:33:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 14:33:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:40909 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 14:33:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1214 ms on 172.18.0.3 (executor 1) (1/1)
25/04/08 14:33:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 14:33:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.317 s
25/04/08 14:33:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 14:33:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 14:33:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.352501 s
25/04/08 14:33:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39965 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 14:33:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:40909 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 14:33:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 14:33:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 14:33:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 14:33:28 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 14:33:28 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 14:33:28 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 14:33:28 INFO metastore: Connected to metastore.
25/04/08 14:33:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/08 14:33:29 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5e8cfc2b-17cb-4227-ada4-5a7301c02554, clientType=HIVECLI]
25/04/08 14:33:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 14:33:29 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 14:33:29 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 14:33:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 14:33:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 14:33:29 INFO metastore: Connected to metastore.
25/04/08 14:33:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 14:33:29 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 14:33:29 INFO metastore: Connected to metastore.
25/04/08 14:33:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 14:33:29 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 14:33:29 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 14:33:29 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 14:33:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 14:33:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 14:33:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 14:33:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 14:33:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 14:33:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 14:33:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 14:33:30 INFO CodeGenerator: Code generated in 156.789835 ms
25/04/08 14:33:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 14:33:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 14:33:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39965 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 14:33:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 14:33:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 14:33:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 14:33:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 14:33:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 14:33:30 INFO DAGScheduler: Parents of final stage: List()
25/04/08 14:33:30 INFO DAGScheduler: Missing parents: List()
25/04/08 14:33:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 14:33:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 14:33:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 14:33:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39965 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 14:33:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 14:33:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 14:33:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 14:33:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 14:33:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:42497 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 14:33:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:42497 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 14:33:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2154 ms on 172.18.0.9 (executor 0) (1/1)
25/04/08 14:33:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 14:33:32 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.181 s
25/04/08 14:33:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 14:33:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 14:33:32 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.189154 s
25/04/08 14:33:32 INFO FileFormatWriter: Start to commit write Job 68322994-7918-4245-9b82-7a49e587abe2.
25/04/08 14:33:32 INFO FileFormatWriter: Write Job 68322994-7918-4245-9b82-7a49e587abe2 committed. Elapsed time: 38 ms.
25/04/08 14:33:32 INFO FileFormatWriter: Finished processing stats for write job 68322994-7918-4245-9b82-7a49e587abe2.
25/04/08 14:33:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/08 14:33:32 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/08 14:33:32 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 14:33:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 14:33:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 14:33:32 INFO MemoryStore: MemoryStore cleared
25/04/08 14:33:32 INFO BlockManager: BlockManager stopped
25/04/08 14:33:32 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 14:33:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 14:33:32 INFO SparkContext: Successfully stopped SparkContext
25/04/08 14:33:32 INFO ShutdownHookManager: Shutdown hook called
25/04/08 14:33:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed162d8b-0f76-4cce-8875-c00ee31b0303
25/04/08 14:33:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e256640-7285-46bf-bed2-575404d04ef8/pyspark-1e47a147-8077-44be-8543-5d98c58b0808
25/04/08 14:33:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-3e256640-7285-46bf-bed2-575404d04ef8
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 16:11:13 INFO SparkContext: Running Spark version 3.2.2
25/04/08 16:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 16:11:13 INFO ResourceUtils: ==============================================================
25/04/08 16:11:13 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 16:11:13 INFO ResourceUtils: ==============================================================
25/04/08 16:11:13 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 16:11:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 16:11:13 INFO ResourceProfile: Limiting resource is cpu
25/04/08 16:11:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 16:11:13 INFO SecurityManager: Changing view acls to: root
25/04/08 16:11:13 INFO SecurityManager: Changing modify acls to: root
25/04/08 16:11:13 INFO SecurityManager: Changing view acls groups to: 
25/04/08 16:11:13 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 16:11:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 16:11:13 INFO Utils: Successfully started service 'sparkDriver' on port 32999.
25/04/08 16:11:13 INFO SparkEnv: Registering MapOutputTracker
25/04/08 16:11:13 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 16:11:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 16:11:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 16:11:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 16:11:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c75dfa45-3158-44a4-89a5-6096cb25e1e4
25/04/08 16:11:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 16:11:13 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 16:11:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 16:11:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 16:11:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/08 16:11:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408161114-0008
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408161114-0008/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/08 16:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408161114-0008/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408161114-0008/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/08 16:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408161114-0008/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408161114-0008/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/08 16:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408161114-0008/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/08 16:11:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39877.
25/04/08 16:11:14 INFO NettyBlockTransferService: Server created on 3fada93ce917:39877
25/04/08 16:11:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 16:11:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39877, None)
25/04/08 16:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39877 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39877, None)
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408161114-0008/0 is now RUNNING
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408161114-0008/2 is now RUNNING
25/04/08 16:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408161114-0008/1 is now RUNNING
25/04/08 16:11:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39877, None)
25/04/08 16:11:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39877, None)
25/04/08 16:11:14 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 16:11:14 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 16:11:14 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 16:11:15 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/08 16:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:35412) with ID 2,  ResourceProfileId 0
25/04/08 16:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:46486) with ID 0,  ResourceProfileId 0
25/04/08 16:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:38472) with ID 1,  ResourceProfileId 0
25/04/08 16:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:41209 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 41209, None)
25/04/08 16:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38863 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 38863, None)
25/04/08 16:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:37853 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 37853, None)
25/04/08 16:11:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 16:11:16 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 16:11:16 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 16:11:16 INFO DAGScheduler: Parents of final stage: List()
25/04/08 16:11:16 INFO DAGScheduler: Missing parents: List()
25/04/08 16:11:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 16:11:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 16:11:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/08 16:11:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39877 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 16:11:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 16:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 16:11:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 16:11:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 16:11:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:38863 (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 16:11:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1158 ms on 172.18.0.9 (executor 0) (1/1)
25/04/08 16:11:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 16:11:17 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.262 s
25/04/08 16:11:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 16:11:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 16:11:17 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.294598 s
25/04/08 16:11:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39877 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 16:11:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:38863 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/08 16:11:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 16:11:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 16:11:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 16:11:18 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 16:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 16:11:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 16:11:18 INFO metastore: Connected to metastore.
25/04/08 16:11:19 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/08 16:11:19 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d4056155-8852-44e3-a7d1-76b88fe9ac38, clientType=HIVECLI]
25/04/08 16:11:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 16:11:19 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 16:11:19 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 16:11:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 16:11:19 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 16:11:19 INFO metastore: Connected to metastore.
25/04/08 16:11:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 16:11:19 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 16:11:19 INFO metastore: Connected to metastore.
25/04/08 16:11:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 16:11:19 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 16:11:19 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 16:11:19 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 16:11:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 16:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 16:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 16:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 16:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 16:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 16:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 16:11:19 INFO CodeGenerator: Code generated in 150.437327 ms
25/04/08 16:11:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 16:11:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 16:11:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39877 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 16:11:19 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 16:11:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 16:11:20 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 16:11:20 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 16:11:20 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 16:11:20 INFO DAGScheduler: Parents of final stage: List()
25/04/08 16:11:20 INFO DAGScheduler: Missing parents: List()
25/04/08 16:11:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 16:11:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 16:11:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 16:11:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39877 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 16:11:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 16:11:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 16:11:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 16:11:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 16:11:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:38863 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 16:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:38863 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 16:11:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 959 ms on 172.18.0.9 (executor 0) (1/1)
25/04/08 16:11:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 16:11:21 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.983 s
25/04/08 16:11:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 16:11:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 16:11:21 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 0.990321 s
25/04/08 16:11:21 INFO FileFormatWriter: Start to commit write Job 6f37baed-9a6e-482c-bee0-f9190113cacf.
25/04/08 16:11:21 INFO FileFormatWriter: Write Job 6f37baed-9a6e-482c-bee0-f9190113cacf committed. Elapsed time: 37 ms.
25/04/08 16:11:21 INFO FileFormatWriter: Finished processing stats for write job 6f37baed-9a6e-482c-bee0-f9190113cacf.
25/04/08 16:11:21 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/08 16:11:21 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/08 16:11:21 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 16:11:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 16:11:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 16:11:21 INFO MemoryStore: MemoryStore cleared
25/04/08 16:11:21 INFO BlockManager: BlockManager stopped
25/04/08 16:11:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 16:11:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 16:11:21 INFO SparkContext: Successfully stopped SparkContext
25/04/08 16:11:21 INFO ShutdownHookManager: Shutdown hook called
25/04/08 16:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-e04c798a-5e7a-468c-9efb-0a51db57f3a8/pyspark-ecb90d5c-eb51-4bed-ac43-f842800d59fe
25/04/08 16:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c59abbc2-b362-41e4-adf3-5f20e9e2b734
25/04/08 16:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-e04c798a-5e7a-468c-9efb-0a51db57f3a8
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/08 20:11:12 INFO SparkContext: Running Spark version 3.2.2
25/04/08 20:11:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/08 20:11:12 INFO ResourceUtils: ==============================================================
25/04/08 20:11:12 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/08 20:11:12 INFO ResourceUtils: ==============================================================
25/04/08 20:11:12 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/08 20:11:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/08 20:11:12 INFO ResourceProfile: Limiting resource is cpu
25/04/08 20:11:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/08 20:11:12 INFO SecurityManager: Changing view acls to: root
25/04/08 20:11:12 INFO SecurityManager: Changing modify acls to: root
25/04/08 20:11:12 INFO SecurityManager: Changing view acls groups to: 
25/04/08 20:11:12 INFO SecurityManager: Changing modify acls groups to: 
25/04/08 20:11:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/08 20:11:12 INFO Utils: Successfully started service 'sparkDriver' on port 44671.
25/04/08 20:11:12 INFO SparkEnv: Registering MapOutputTracker
25/04/08 20:11:12 INFO SparkEnv: Registering BlockManagerMaster
25/04/08 20:11:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/08 20:11:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/08 20:11:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/08 20:11:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2aa97ab5-ac40-461c-8682-0c0cccff9170
25/04/08 20:11:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/08 20:11:12 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/08 20:11:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/08 20:11:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/08 20:11:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 21 ms (0 ms spent in bootstraps)
25/04/08 20:11:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250408201112-0011
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408201112-0011/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/08 20:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408201112-0011/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408201112-0011/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/08 20:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408201112-0011/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250408201112-0011/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/08 20:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250408201112-0011/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/08 20:11:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34013.
25/04/08 20:11:12 INFO NettyBlockTransferService: Server created on 3fada93ce917:34013
25/04/08 20:11:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/08 20:11:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 34013, None)
25/04/08 20:11:12 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:34013 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 34013, None)
25/04/08 20:11:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 34013, None)
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408201112-0011/0 is now RUNNING
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408201112-0011/2 is now RUNNING
25/04/08 20:11:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 34013, None)
25/04/08 20:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250408201112-0011/1 is now RUNNING
25/04/08 20:11:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/08 20:11:13 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/08 20:11:13 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/08 20:11:14 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
25/04/08 20:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:35448) with ID 0,  ResourceProfileId 0
25/04/08 20:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:55514) with ID 1,  ResourceProfileId 0
25/04/08 20:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:51696) with ID 2,  ResourceProfileId 0
25/04/08 20:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:36991 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 36991, None)
25/04/08 20:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:37117 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 37117, None)
25/04/08 20:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:46059 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 46059, None)
25/04/08 20:11:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/08 20:11:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 20:11:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/08 20:11:15 INFO DAGScheduler: Parents of final stage: List()
25/04/08 20:11:15 INFO DAGScheduler: Missing parents: List()
25/04/08 20:11:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 20:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/08 20:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/08 20:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:34013 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 20:11:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/08 20:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 20:11:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/08 20:11:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/08 20:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:37117 (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 20:11:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1160 ms on 172.18.0.9 (executor 0) (1/1)
25/04/08 20:11:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/08 20:11:16 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.253 s
25/04/08 20:11:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 20:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/08 20:11:16 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.284024 s
25/04/08 20:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:34013 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 20:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:37117 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/08 20:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 20:11:17 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/08 20:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/08 20:11:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/08 20:11:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 20:11:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 20:11:17 INFO metastore: Connected to metastore.
25/04/08 20:11:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/08 20:11:18 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=6fde8e9a-1d04-4518-b28e-8bc0f1f80640, clientType=HIVECLI]
25/04/08 20:11:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/08 20:11:18 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/08 20:11:18 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/08 20:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 20:11:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/08 20:11:18 INFO metastore: Connected to metastore.
25/04/08 20:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/08 20:11:18 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/08 20:11:18 INFO metastore: Connected to metastore.
25/04/08 20:11:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/08 20:11:18 INFO FileSourceStrategy: Pushed Filters: 
25/04/08 20:11:18 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/08 20:11:18 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/08 20:11:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 20:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 20:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 20:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 20:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/08 20:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/08 20:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/08 20:11:18 INFO CodeGenerator: Code generated in 157.262507 ms
25/04/08 20:11:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/08 20:11:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/08 20:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:34013 (size: 33.8 KiB, free: 366.3 MiB)
25/04/08 20:11:18 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/08 20:11:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/08 20:11:18 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/08 20:11:18 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/08 20:11:18 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/08 20:11:18 INFO DAGScheduler: Parents of final stage: List()
25/04/08 20:11:18 INFO DAGScheduler: Missing parents: List()
25/04/08 20:11:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/08 20:11:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/08 20:11:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/08 20:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:34013 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 20:11:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/08 20:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/08 20:11:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/08 20:11:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/08 20:11:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:46059 (size: 74.7 KiB, free: 366.2 MiB)
25/04/08 20:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:46059 (size: 33.8 KiB, free: 366.2 MiB)
25/04/08 20:11:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1992 ms on 172.18.0.3 (executor 1) (1/1)
25/04/08 20:11:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/08 20:11:20 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.018 s
25/04/08 20:11:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/08 20:11:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/08 20:11:20 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.026297 s
25/04/08 20:11:20 INFO FileFormatWriter: Start to commit write Job 60480cce-f400-4c41-8bfa-da67e9c1e2fb.
25/04/08 20:11:20 INFO FileFormatWriter: Write Job 60480cce-f400-4c41-8bfa-da67e9c1e2fb committed. Elapsed time: 38 ms.
25/04/08 20:11:20 INFO FileFormatWriter: Finished processing stats for write job 60480cce-f400-4c41-8bfa-da67e9c1e2fb.
25/04/08 20:11:20 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/08 20:11:21 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/08 20:11:21 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/08 20:11:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/08 20:11:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/08 20:11:21 INFO MemoryStore: MemoryStore cleared
25/04/08 20:11:21 INFO BlockManager: BlockManager stopped
25/04/08 20:11:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/08 20:11:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/08 20:11:21 INFO SparkContext: Successfully stopped SparkContext
25/04/08 20:11:21 INFO ShutdownHookManager: Shutdown hook called
25/04/08 20:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-b57f8dff-555f-41e7-8232-414add3d3082
25/04/08 20:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1399e82-011f-407a-9cfe-4a8e75486a1d
25/04/08 20:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-b57f8dff-555f-41e7-8232-414add3d3082/pyspark-fd22ae84-a819-4d81-95c6-8768c3e303c8
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 00:11:12 INFO SparkContext: Running Spark version 3.2.2
25/04/09 00:11:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 00:11:12 INFO ResourceUtils: ==============================================================
25/04/09 00:11:12 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 00:11:12 INFO ResourceUtils: ==============================================================
25/04/09 00:11:12 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 00:11:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 00:11:12 INFO ResourceProfile: Limiting resource is cpu
25/04/09 00:11:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 00:11:12 INFO SecurityManager: Changing view acls to: root
25/04/09 00:11:12 INFO SecurityManager: Changing modify acls to: root
25/04/09 00:11:12 INFO SecurityManager: Changing view acls groups to: 
25/04/09 00:11:12 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 00:11:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 00:11:12 INFO Utils: Successfully started service 'sparkDriver' on port 32769.
25/04/09 00:11:12 INFO SparkEnv: Registering MapOutputTracker
25/04/09 00:11:12 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 00:11:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 00:11:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 00:11:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 00:11:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-90c67fe8-e21a-4509-a39a-fdef0788d46c
25/04/09 00:11:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 00:11:12 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 00:11:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 00:11:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 00:11:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 00:11:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 00:11:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409001112-0016
25/04/09 00:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409001112-0016/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 00:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409001112-0016/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 00:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409001112-0016/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 00:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409001112-0016/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 00:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409001112-0016/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 00:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409001112-0016/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 00:11:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36157.
25/04/09 00:11:12 INFO NettyBlockTransferService: Server created on 3fada93ce917:36157
25/04/09 00:11:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 00:11:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 36157, None)
25/04/09 00:11:13 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:36157 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 36157, None)
25/04/09 00:11:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 36157, None)
25/04/09 00:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409001112-0016/2 is now RUNNING
25/04/09 00:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409001112-0016/1 is now RUNNING
25/04/09 00:11:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 36157, None)
25/04/09 00:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409001112-0016/0 is now RUNNING
25/04/09 00:11:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 00:11:13 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 00:11:13 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 00:11:14 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/09 00:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:57718) with ID 2,  ResourceProfileId 0
25/04/09 00:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:60528) with ID 1,  ResourceProfileId 0
25/04/09 00:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:39642) with ID 0,  ResourceProfileId 0
25/04/09 00:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:35831 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 35831, None)
25/04/09 00:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43795 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43795, None)
25/04/09 00:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38351 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 38351, None)
25/04/09 00:11:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 00:11:14 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 00:11:14 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 00:11:14 INFO DAGScheduler: Parents of final stage: List()
25/04/09 00:11:14 INFO DAGScheduler: Missing parents: List()
25/04/09 00:11:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 00:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 00:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 00:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:36157 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 00:11:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 00:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 00:11:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 00:11:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 00:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:38351 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 00:11:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1168 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 00:11:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 00:11:16 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.261 s
25/04/09 00:11:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 00:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 00:11:16 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.292949 s
25/04/09 00:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:36157 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 00:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:38351 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 00:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 00:11:17 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 00:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 00:11:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 00:11:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 00:11:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 00:11:17 INFO metastore: Connected to metastore.
25/04/09 00:11:18 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 00:11:18 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=24bd9ed4-a21e-492c-aa0d-a65126b3dc26, clientType=HIVECLI]
25/04/09 00:11:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 00:11:18 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 00:11:18 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 00:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 00:11:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 00:11:18 INFO metastore: Connected to metastore.
25/04/09 00:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 00:11:18 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 00:11:18 INFO metastore: Connected to metastore.
25/04/09 00:11:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 00:11:18 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 00:11:18 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 00:11:18 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 00:11:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 00:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 00:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 00:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 00:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 00:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 00:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 00:11:18 INFO CodeGenerator: Code generated in 154.942094 ms
25/04/09 00:11:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 00:11:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 00:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:36157 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 00:11:18 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 00:11:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 00:11:18 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 00:11:18 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 00:11:18 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 00:11:18 INFO DAGScheduler: Parents of final stage: List()
25/04/09 00:11:18 INFO DAGScheduler: Missing parents: List()
25/04/09 00:11:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 00:11:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 00:11:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 00:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:36157 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 00:11:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 00:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 00:11:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 00:11:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 00:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:38351 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 00:11:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:38351 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 00:11:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 984 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 00:11:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 00:11:19 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.010 s
25/04/09 00:11:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 00:11:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 00:11:19 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.018511 s
25/04/09 00:11:19 INFO FileFormatWriter: Start to commit write Job 3210eae9-9011-4abe-896e-65260bfe5cee.
25/04/09 00:11:19 INFO FileFormatWriter: Write Job 3210eae9-9011-4abe-896e-65260bfe5cee committed. Elapsed time: 36 ms.
25/04/09 00:11:19 INFO FileFormatWriter: Finished processing stats for write job 3210eae9-9011-4abe-896e-65260bfe5cee.
25/04/09 00:11:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 00:11:19 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 00:11:19 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 00:11:19 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 00:11:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 00:11:20 INFO MemoryStore: MemoryStore cleared
25/04/09 00:11:20 INFO BlockManager: BlockManager stopped
25/04/09 00:11:20 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 00:11:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 00:11:20 INFO SparkContext: Successfully stopped SparkContext
25/04/09 00:11:20 INFO ShutdownHookManager: Shutdown hook called
25/04/09 00:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-fae9fbd7-a1e4-413c-9350-4755d3e12a48
25/04/09 00:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b9d33d0-d522-4e1d-84f7-032fa31691e5
25/04/09 00:11:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b9d33d0-d522-4e1d-84f7-032fa31691e5/pyspark-2084884f-0546-46bf-bf83-2fd70f7629a4
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 04:11:12 INFO SparkContext: Running Spark version 3.2.2
25/04/09 04:11:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 04:11:12 INFO ResourceUtils: ==============================================================
25/04/09 04:11:12 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 04:11:12 INFO ResourceUtils: ==============================================================
25/04/09 04:11:12 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 04:11:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 04:11:12 INFO ResourceProfile: Limiting resource is cpu
25/04/09 04:11:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 04:11:12 INFO SecurityManager: Changing view acls to: root
25/04/09 04:11:12 INFO SecurityManager: Changing modify acls to: root
25/04/09 04:11:12 INFO SecurityManager: Changing view acls groups to: 
25/04/09 04:11:12 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 04:11:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 04:11:12 INFO Utils: Successfully started service 'sparkDriver' on port 40683.
25/04/09 04:11:12 INFO SparkEnv: Registering MapOutputTracker
25/04/09 04:11:12 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 04:11:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 04:11:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 04:11:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 04:11:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2e1c5a40-224e-4b2f-a766-43d09c0bf7bf
25/04/09 04:11:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 04:11:12 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 04:11:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 04:11:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 04:11:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 04:11:12 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 04:11:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409041112-0019
25/04/09 04:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409041112-0019/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 04:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409041112-0019/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 04:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409041112-0019/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 04:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409041112-0019/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 04:11:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409041112-0019/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 04:11:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409041112-0019/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 04:11:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46179.
25/04/09 04:11:12 INFO NettyBlockTransferService: Server created on 3fada93ce917:46179
25/04/09 04:11:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 04:11:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 46179, None)
25/04/09 04:11:13 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:46179 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 46179, None)
25/04/09 04:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409041112-0019/0 is now RUNNING
25/04/09 04:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409041112-0019/1 is now RUNNING
25/04/09 04:11:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 46179, None)
25/04/09 04:11:13 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409041112-0019/2 is now RUNNING
25/04/09 04:11:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 46179, None)
25/04/09 04:11:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 04:11:13 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 04:11:13 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 04:11:14 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.
25/04/09 04:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:57666) with ID 1,  ResourceProfileId 0
25/04/09 04:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:51242) with ID 2,  ResourceProfileId 0
25/04/09 04:11:14 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:43576) with ID 0,  ResourceProfileId 0
25/04/09 04:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42945 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42945, None)
25/04/09 04:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:38717 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 38717, None)
25/04/09 04:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:39807 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 39807, None)
25/04/09 04:11:15 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 04:11:15 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 04:11:15 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 04:11:15 INFO DAGScheduler: Parents of final stage: List()
25/04/09 04:11:15 INFO DAGScheduler: Missing parents: List()
25/04/09 04:11:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 04:11:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 04:11:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 04:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:46179 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 04:11:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 04:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 04:11:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 04:11:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 04:11:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:39807 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 04:11:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1139 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 04:11:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 04:11:16 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.229 s
25/04/09 04:11:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 04:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 04:11:16 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.259597 s
25/04/09 04:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:46179 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 04:11:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:39807 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 04:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 04:11:17 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 04:11:17 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 04:11:17 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 04:11:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 04:11:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 04:11:17 INFO metastore: Connected to metastore.
25/04/09 04:11:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 04:11:18 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f60e36e2-ed12-4397-8adc-49a113850b5b, clientType=HIVECLI]
25/04/09 04:11:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 04:11:18 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 04:11:18 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 04:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 04:11:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 04:11:18 INFO metastore: Connected to metastore.
25/04/09 04:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 04:11:18 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 04:11:18 INFO metastore: Connected to metastore.
25/04/09 04:11:18 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 04:11:18 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 04:11:18 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 04:11:18 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 04:11:18 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 04:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 04:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 04:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 04:11:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 04:11:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 04:11:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 04:11:18 INFO CodeGenerator: Code generated in 147.149192 ms
25/04/09 04:11:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 04:11:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 04:11:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:46179 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 04:11:18 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 04:11:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 04:11:18 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 04:11:18 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 04:11:18 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 04:11:18 INFO DAGScheduler: Parents of final stage: List()
25/04/09 04:11:18 INFO DAGScheduler: Missing parents: List()
25/04/09 04:11:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 04:11:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 04:11:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 04:11:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:46179 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 04:11:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 04:11:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 04:11:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 04:11:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 04:11:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:38717 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 04:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:38717 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 04:11:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1962 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 04:11:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 04:11:20 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.988 s
25/04/09 04:11:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 04:11:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 04:11:20 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.995591 s
25/04/09 04:11:20 INFO FileFormatWriter: Start to commit write Job 35b95193-e494-4b5c-bc16-6d703a0df50e.
25/04/09 04:11:20 INFO FileFormatWriter: Write Job 35b95193-e494-4b5c-bc16-6d703a0df50e committed. Elapsed time: 36 ms.
25/04/09 04:11:20 INFO FileFormatWriter: Finished processing stats for write job 35b95193-e494-4b5c-bc16-6d703a0df50e.
25/04/09 04:11:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 04:11:20 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 04:11:20 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 04:11:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 04:11:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 04:11:21 INFO MemoryStore: MemoryStore cleared
25/04/09 04:11:21 INFO BlockManager: BlockManager stopped
25/04/09 04:11:21 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 04:11:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 04:11:21 INFO SparkContext: Successfully stopped SparkContext
25/04/09 04:11:21 INFO ShutdownHookManager: Shutdown hook called
25/04/09 04:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-79fd9823-b7ec-44b5-9635-1e659aa73684
25/04/09 04:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-8065ddb3-1bb0-4e04-bfc4-7d522a656db7/pyspark-67bc97b3-56d6-45ef-bf2e-75312b0f6b20
25/04/09 04:11:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-8065ddb3-1bb0-4e04-bfc4-7d522a656db7
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 08:11:13 INFO SparkContext: Running Spark version 3.2.2
25/04/09 08:11:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 08:11:13 INFO ResourceUtils: ==============================================================
25/04/09 08:11:13 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 08:11:13 INFO ResourceUtils: ==============================================================
25/04/09 08:11:13 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 08:11:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 08:11:13 INFO ResourceProfile: Limiting resource is cpu
25/04/09 08:11:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 08:11:13 INFO SecurityManager: Changing view acls to: root
25/04/09 08:11:13 INFO SecurityManager: Changing modify acls to: root
25/04/09 08:11:13 INFO SecurityManager: Changing view acls groups to: 
25/04/09 08:11:13 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 08:11:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 08:11:13 INFO Utils: Successfully started service 'sparkDriver' on port 40001.
25/04/09 08:11:13 INFO SparkEnv: Registering MapOutputTracker
25/04/09 08:11:13 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 08:11:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 08:11:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 08:11:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 08:11:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8149e1fe-d6d4-4045-9c31-be65e6506c96
25/04/09 08:11:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 08:11:13 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 08:11:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 08:11:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 08:11:13 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 08:11:14 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 08:11:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409081114-0022
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409081114-0022/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 08:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409081114-0022/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409081114-0022/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 08:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409081114-0022/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409081114-0022/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 08:11:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409081114-0022/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 08:11:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45133.
25/04/09 08:11:14 INFO NettyBlockTransferService: Server created on 3fada93ce917:45133
25/04/09 08:11:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 08:11:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 45133, None)
25/04/09 08:11:14 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:45133 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 45133, None)
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409081114-0022/0 is now RUNNING
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409081114-0022/2 is now RUNNING
25/04/09 08:11:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 45133, None)
25/04/09 08:11:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409081114-0022/1 is now RUNNING
25/04/09 08:11:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 45133, None)
25/04/09 08:11:14 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 08:11:14 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 08:11:14 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 08:11:15 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/09 08:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:36028) with ID 1,  ResourceProfileId 0
25/04/09 08:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:43514) with ID 2,  ResourceProfileId 0
25/04/09 08:11:15 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:43106) with ID 0,  ResourceProfileId 0
25/04/09 08:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:45605 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 45605, None)
25/04/09 08:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:45687 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 45687, None)
25/04/09 08:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:39033 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 39033, None)
25/04/09 08:11:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 08:11:16 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 08:11:16 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 08:11:16 INFO DAGScheduler: Parents of final stage: List()
25/04/09 08:11:16 INFO DAGScheduler: Missing parents: List()
25/04/09 08:11:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 08:11:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 08:11:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 08:11:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:45133 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 08:11:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 08:11:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 08:11:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 08:11:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 08:11:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:45605 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 08:11:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1168 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 08:11:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 08:11:17 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.260 s
25/04/09 08:11:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 08:11:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 08:11:17 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.295057 s
25/04/09 08:11:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:45133 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 08:11:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:45605 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 08:11:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 08:11:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 08:11:18 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 08:11:18 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 08:11:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 08:11:18 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 08:11:18 INFO metastore: Connected to metastore.
25/04/09 08:11:19 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 08:11:19 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5432e104-52c3-4ce1-9634-9339de5a74cf, clientType=HIVECLI]
25/04/09 08:11:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 08:11:19 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 08:11:19 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 08:11:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 08:11:19 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 08:11:19 INFO metastore: Connected to metastore.
25/04/09 08:11:19 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 08:11:19 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 08:11:19 INFO metastore: Connected to metastore.
25/04/09 08:11:19 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 08:11:19 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 08:11:19 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 08:11:19 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 08:11:19 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 08:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 08:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 08:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 08:11:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 08:11:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 08:11:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 08:11:20 INFO CodeGenerator: Code generated in 156.25788 ms
25/04/09 08:11:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 08:11:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 08:11:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:45133 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 08:11:20 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 08:11:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 08:11:20 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 08:11:20 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 08:11:20 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 08:11:20 INFO DAGScheduler: Parents of final stage: List()
25/04/09 08:11:20 INFO DAGScheduler: Missing parents: List()
25/04/09 08:11:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 08:11:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 08:11:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 08:11:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:45133 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 08:11:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 08:11:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 08:11:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 08:11:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 08:11:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:45687 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 08:11:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:45687 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 08:11:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1989 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 08:11:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 08:11:22 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.013 s
25/04/09 08:11:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 08:11:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 08:11:22 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.020413 s
25/04/09 08:11:22 INFO FileFormatWriter: Start to commit write Job 39357c9e-f6cc-45cc-a65e-d448ec2eb7c9.
25/04/09 08:11:22 INFO FileFormatWriter: Write Job 39357c9e-f6cc-45cc-a65e-d448ec2eb7c9 committed. Elapsed time: 38 ms.
25/04/09 08:11:22 INFO FileFormatWriter: Finished processing stats for write job 39357c9e-f6cc-45cc-a65e-d448ec2eb7c9.
25/04/09 08:11:22 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
25/04/09 08:11:22 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 08:11:22 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 08:11:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 08:11:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 08:11:22 INFO MemoryStore: MemoryStore cleared
25/04/09 08:11:22 INFO BlockManager: BlockManager stopped
25/04/09 08:11:22 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 08:11:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 08:11:22 INFO SparkContext: Successfully stopped SparkContext
25/04/09 08:11:22 INFO ShutdownHookManager: Shutdown hook called
25/04/09 08:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-33f202fc-f490-4f76-a56e-94be2ad217dc/pyspark-91700cfc-b303-49a3-b2da-4cac6d19ee02
25/04/09 08:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-33f202fc-f490-4f76-a56e-94be2ad217dc
25/04/09 08:11:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-f39d22fc-db7d-4403-bcd4-65a7522aff7d
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:04:55 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:04:55 INFO ResourceUtils: ==============================================================
25/04/09 10:04:55 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:04:55 INFO ResourceUtils: ==============================================================
25/04/09 10:04:55 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:04:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:04:55 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:04:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:04:55 INFO SecurityManager: Changing view acls to: root
25/04/09 10:04:55 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:04:55 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:04:55 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:04:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:04:55 INFO Utils: Successfully started service 'sparkDriver' on port 46881.
25/04/09 10:04:55 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:04:55 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:04:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:04:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:04:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:04:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0523b0d-84e7-4d1a-adcc-07ed21b2b24f
25/04/09 10:04:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:04:55 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:04:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:04:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:04:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:04:56 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 24 ms (0 ms spent in bootstraps)
25/04/09 10:04:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409100456-0030
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100456-0030/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:04:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100456-0030/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100456-0030/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:04:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100456-0030/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100456-0030/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:04:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100456-0030/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:04:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42267.
25/04/09 10:04:56 INFO NettyBlockTransferService: Server created on 3fada93ce917:42267
25/04/09 10:04:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:04:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 42267, None)
25/04/09 10:04:56 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:42267 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 42267, None)
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100456-0030/2 is now RUNNING
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100456-0030/0 is now RUNNING
25/04/09 10:04:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100456-0030/1 is now RUNNING
25/04/09 10:04:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 42267, None)
25/04/09 10:04:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 42267, None)
25/04/09 10:04:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:04:56 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:04:56 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:04:57 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 10:04:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:57224) with ID 0,  ResourceProfileId 0
25/04/09 10:04:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:56684) with ID 1,  ResourceProfileId 0
25/04/09 10:04:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:42534) with ID 2,  ResourceProfileId 0
25/04/09 10:04:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44691 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44691, None)
25/04/09 10:04:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33233 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 33233, None)
25/04/09 10:04:58 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:37327 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 37327, None)
25/04/09 10:04:58 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:04:58 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:04:58 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:04:58 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:04:58 INFO DAGScheduler: Missing parents: List()
25/04/09 10:04:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:04:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:04:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:04:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:42267 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:04:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:04:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:04:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:04:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:04:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:33233 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:04:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1220 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:04:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:04:59 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.316 s
25/04/09 10:04:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:04:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:04:59 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.349104 s
25/04/09 10:04:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:42267 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:04:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:33233 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:05:00 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:05:00 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:05:00 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:05:01 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:05:01 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:05:01 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:05:01 INFO metastore: Connected to metastore.
25/04/09 10:05:01 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:05:01 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e5dba077-ebc1-407a-b769-0a9eb57cc147, clientType=HIVECLI]
25/04/09 10:05:01 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:05:01 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:05:01 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:05:01 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:05:01 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:05:01 INFO metastore: Connected to metastore.
25/04/09 10:05:01 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:05:01 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:05:01 INFO metastore: Connected to metastore.
25/04/09 10:05:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:05:02 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:05:02 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:05:02 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:05:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:05:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:05:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:05:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:05:02 INFO CodeGenerator: Code generated in 162.869619 ms
25/04/09 10:05:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:05:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:05:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:42267 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:05:02 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:05:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:05:02 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:05:02 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:05:02 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:05:02 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:05:02 INFO DAGScheduler: Missing parents: List()
25/04/09 10:05:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:05:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:05:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:05:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:42267 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:05:02 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:05:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:05:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:05:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:05:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:37327 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:05:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:37327 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:05:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2139 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:05:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:05:04 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.166 s
25/04/09 10:05:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:05:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:05:04 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.174029 s
25/04/09 10:05:04 INFO FileFormatWriter: Start to commit write Job 98e630e1-bb6b-4aac-888f-276a57950a3f.
25/04/09 10:05:04 INFO FileFormatWriter: Write Job 98e630e1-bb6b-4aac-888f-276a57950a3f committed. Elapsed time: 43 ms.
25/04/09 10:05:04 INFO FileFormatWriter: Finished processing stats for write job 98e630e1-bb6b-4aac-888f-276a57950a3f.
25/04/09 10:05:04 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:05:04 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:05:04 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:05:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:05:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:05:04 INFO MemoryStore: MemoryStore cleared
25/04/09 10:05:04 INFO BlockManager: BlockManager stopped
25/04/09 10:05:04 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:05:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:05:04 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:05:04 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:05:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-852c637d-5e74-4374-9122-6282f7f6444a/pyspark-afba66d3-2834-4396-b93c-c16ba3f08519
25/04/09 10:05:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-4385d8a1-6500-41ee-afc7-12780fd2dcbe
25/04/09 10:05:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-852c637d-5e74-4374-9122-6282f7f6444a
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:09:23 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:09:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:09:23 INFO ResourceUtils: ==============================================================
25/04/09 10:09:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:09:23 INFO ResourceUtils: ==============================================================
25/04/09 10:09:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:09:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:09:23 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:09:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:09:24 INFO SecurityManager: Changing view acls to: root
25/04/09 10:09:24 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:09:24 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:09:24 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:09:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:09:24 INFO Utils: Successfully started service 'sparkDriver' on port 46387.
25/04/09 10:09:24 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:09:24 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:09:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:09:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:09:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:09:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ce69421-c0e2-4ccc-be2d-bdcb4bdb7ac7
25/04/09 10:09:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:09:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:09:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:09:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:09:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 10:09:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409100924-0032
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100924-0032/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:09:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100924-0032/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100924-0032/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:09:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100924-0032/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409100924-0032/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:09:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409100924-0032/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:09:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33043.
25/04/09 10:09:24 INFO NettyBlockTransferService: Server created on 3fada93ce917:33043
25/04/09 10:09:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:09:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 33043, None)
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100924-0032/0 is now RUNNING
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100924-0032/2 is now RUNNING
25/04/09 10:09:24 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:33043 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 33043, None)
25/04/09 10:09:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409100924-0032/1 is now RUNNING
25/04/09 10:09:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 33043, None)
25/04/09 10:09:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 33043, None)
25/04/09 10:09:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:09:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:09:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:09:26 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 10:09:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:56396) with ID 2,  ResourceProfileId 0
25/04/09 10:09:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:54822) with ID 1,  ResourceProfileId 0
25/04/09 10:09:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:56528) with ID 0,  ResourceProfileId 0
25/04/09 10:09:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:35127 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 35127, None)
25/04/09 10:09:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:41119 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 41119, None)
25/04/09 10:09:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40463 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40463, None)
25/04/09 10:09:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:09:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:09:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:09:26 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:09:26 INFO DAGScheduler: Missing parents: List()
25/04/09 10:09:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:09:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:09:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:09:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:33043 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:09:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:09:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:09:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:09:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:09:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:35127 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:09:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1232 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:09:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:09:28 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.326 s
25/04/09 10:09:28 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:09:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:09:28 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.361012 s
25/04/09 10:09:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:33043 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:09:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:35127 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:09:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:09:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:09:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:09:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:09:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:09:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:09:29 INFO metastore: Connected to metastore.
25/04/09 10:09:30 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:09:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=d6777e33-0063-42b5-adfc-0f8f3971eb8a, clientType=HIVECLI]
25/04/09 10:09:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:09:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:09:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:09:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:09:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:09:30 INFO metastore: Connected to metastore.
25/04/09 10:09:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:09:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:09:30 INFO metastore: Connected to metastore.
25/04/09 10:09:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:09:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:09:30 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:09:30 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:09:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:09:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:09:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:09:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:09:31 INFO CodeGenerator: Code generated in 162.211681 ms
25/04/09 10:09:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:09:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:09:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:33043 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:09:31 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:09:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:09:31 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:09:31 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:09:31 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:09:31 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:09:31 INFO DAGScheduler: Missing parents: List()
25/04/09 10:09:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:09:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:09:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:09:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:33043 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:09:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:09:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:09:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:09:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:09:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:41119 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:09:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:41119 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:09:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2108 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:09:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:09:33 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.133 s
25/04/09 10:09:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:09:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:09:33 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.141230 s
25/04/09 10:09:33 INFO FileFormatWriter: Start to commit write Job 551b6d80-690d-4bcd-9feb-c873844b0689.
25/04/09 10:09:33 INFO FileFormatWriter: Write Job 551b6d80-690d-4bcd-9feb-c873844b0689 committed. Elapsed time: 38 ms.
25/04/09 10:09:33 INFO FileFormatWriter: Finished processing stats for write job 551b6d80-690d-4bcd-9feb-c873844b0689.
25/04/09 10:09:33 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
25/04/09 10:09:33 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:09:33 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:09:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:09:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:09:33 INFO MemoryStore: MemoryStore cleared
25/04/09 10:09:33 INFO BlockManager: BlockManager stopped
25/04/09 10:09:33 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:09:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:09:33 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:09:33 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:09:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b6aa55f-c7b0-4368-8864-0095e22d3f37
25/04/09 10:09:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-2fbc62c0-53d9-40b9-b242-bbf61828e002/pyspark-05ab607e-e4d6-4ca0-875e-a1808ffc3837
25/04/09 10:09:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-2fbc62c0-53d9-40b9-b242-bbf61828e002
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:10:20 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:10:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:10:20 INFO ResourceUtils: ==============================================================
25/04/09 10:10:20 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:10:20 INFO ResourceUtils: ==============================================================
25/04/09 10:10:20 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:10:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:10:20 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:10:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:10:20 INFO SecurityManager: Changing view acls to: root
25/04/09 10:10:20 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:10:20 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:10:20 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:10:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:10:20 INFO Utils: Successfully started service 'sparkDriver' on port 43807.
25/04/09 10:10:20 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:10:20 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:10:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:10:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:10:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:10:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-27ef69c6-6ee8-4410-a35b-8eba1fa5f84b
25/04/09 10:10:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:10:20 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:10:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:10:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:10:21 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 25 ms (0 ms spent in bootstraps)
25/04/09 10:10:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409101021-0034
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101021-0034/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:10:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101021-0034/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101021-0034/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:10:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101021-0034/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101021-0034/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:10:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101021-0034/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:10:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45275.
25/04/09 10:10:21 INFO NettyBlockTransferService: Server created on 3fada93ce917:45275
25/04/09 10:10:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:10:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 45275, None)
25/04/09 10:10:21 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:45275 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 45275, None)
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101021-0034/2 is now RUNNING
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101021-0034/1 is now RUNNING
25/04/09 10:10:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 45275, None)
25/04/09 10:10:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101021-0034/0 is now RUNNING
25/04/09 10:10:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 45275, None)
25/04/09 10:10:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:10:21 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:10:21 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:10:23 INFO InMemoryFileIndex: It took 72 ms to list leaf files for 1 paths.
25/04/09 10:10:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:56004) with ID 1,  ResourceProfileId 0
25/04/09 10:10:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:56760) with ID 0,  ResourceProfileId 0
25/04/09 10:10:23 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:34934) with ID 2,  ResourceProfileId 0
25/04/09 10:10:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:34977 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 34977, None)
25/04/09 10:10:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43477 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43477, None)
25/04/09 10:10:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:38137 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 38137, None)
25/04/09 10:10:23 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:10:23 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:10:23 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:10:23 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:10:23 INFO DAGScheduler: Missing parents: List()
25/04/09 10:10:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:10:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:10:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:10:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:45275 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:10:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:10:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:10:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:10:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:10:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:38137 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:10:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1252 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:10:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:10:25 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.365 s
25/04/09 10:10:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:10:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:10:25 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.403447 s
25/04/09 10:10:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:45275 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:10:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:38137 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:10:26 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:10:26 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:10:26 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:10:26 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:10:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:10:26 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:10:26 INFO metastore: Connected to metastore.
25/04/09 10:10:27 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:10:27 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=20070bcf-c651-4fc7-b0d0-eab55a682917, clientType=HIVECLI]
25/04/09 10:10:27 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:10:27 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:10:27 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:10:27 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:10:27 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:10:27 INFO metastore: Connected to metastore.
25/04/09 10:10:27 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:10:27 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:10:27 INFO metastore: Connected to metastore.
25/04/09 10:10:27 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:10:27 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:10:27 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:10:27 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:10:27 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:10:27 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:10:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:10:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:10:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:10:27 INFO CodeGenerator: Code generated in 155.156515 ms
25/04/09 10:10:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:10:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:10:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:45275 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:10:27 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:10:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:10:27 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:10:27 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:10:27 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:10:27 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:10:27 INFO DAGScheduler: Missing parents: List()
25/04/09 10:10:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:10:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:10:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:10:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:45275 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:10:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:10:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:10:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:10:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:10:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:34977 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:10:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:34977 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:10:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2185 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:10:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:10:30 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.212 s
25/04/09 10:10:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:10:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:10:30 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.219946 s
25/04/09 10:10:30 INFO FileFormatWriter: Start to commit write Job f83c0842-a479-4643-bd96-214d3a12d83d.
25/04/09 10:10:30 INFO FileFormatWriter: Write Job f83c0842-a479-4643-bd96-214d3a12d83d committed. Elapsed time: 39 ms.
25/04/09 10:10:30 INFO FileFormatWriter: Finished processing stats for write job f83c0842-a479-4643-bd96-214d3a12d83d.
25/04/09 10:10:30 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:10:30 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:10:30 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:10:30 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:10:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:10:30 INFO MemoryStore: MemoryStore cleared
25/04/09 10:10:30 INFO BlockManager: BlockManager stopped
25/04/09 10:10:30 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:10:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:10:30 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:10:30 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-e81354de-dcf2-4d12-a9c9-50ce6c39ea35
25/04/09 10:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2c0edab-4b7f-4e87-8c48-5f3b2c1ae885/pyspark-66e0a1c4-2540-47d7-91d3-7fbc05108c5e
25/04/09 10:10:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2c0edab-4b7f-4e87-8c48-5f3b2c1ae885
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:11:57 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:11:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:11:57 INFO ResourceUtils: ==============================================================
25/04/09 10:11:57 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:11:57 INFO ResourceUtils: ==============================================================
25/04/09 10:11:57 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:11:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:11:57 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:11:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:11:57 INFO SecurityManager: Changing view acls to: root
25/04/09 10:11:57 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:11:57 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:11:57 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:11:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:11:58 INFO Utils: Successfully started service 'sparkDriver' on port 37409.
25/04/09 10:11:58 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:11:58 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:11:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:11:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:11:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:11:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3dc02a7-eab4-4a70-887c-c61bef560e1a
25/04/09 10:11:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:11:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:11:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:11:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:11:58 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:11:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409101158-0036
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101158-0036/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:11:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101158-0036/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101158-0036/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:11:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101158-0036/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101158-0036/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:11:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101158-0036/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:11:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39913.
25/04/09 10:11:58 INFO NettyBlockTransferService: Server created on 3fada93ce917:39913
25/04/09 10:11:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:11:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39913, None)
25/04/09 10:11:58 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39913 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39913, None)
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101158-0036/2 is now RUNNING
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101158-0036/1 is now RUNNING
25/04/09 10:11:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101158-0036/0 is now RUNNING
25/04/09 10:11:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39913, None)
25/04/09 10:11:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39913, None)
25/04/09 10:11:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:11:59 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:11:59 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:12:00 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/09 10:12:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:34574) with ID 0,  ResourceProfileId 0
25/04/09 10:12:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:33994) with ID 1,  ResourceProfileId 0
25/04/09 10:12:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:50396) with ID 2,  ResourceProfileId 0
25/04/09 10:12:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:39931 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 39931, None)
25/04/09 10:12:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43453 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43453, None)
25/04/09 10:12:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:39827 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 39827, None)
25/04/09 10:12:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:12:00 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:12:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:12:00 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:12:00 INFO DAGScheduler: Missing parents: List()
25/04/09 10:12:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:12:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:12:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:12:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39913 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:12:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:12:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:12:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:12:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:12:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:43453 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:12:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1203 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:12:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:12:02 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.292 s
25/04/09 10:12:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:12:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:12:02 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.324058 s
25/04/09 10:12:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39913 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:12:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:43453 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:12:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:12:03 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:12:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:12:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:12:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:12:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:12:03 INFO metastore: Connected to metastore.
25/04/09 10:12:04 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:12:04 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=66235e09-ae91-4398-90b2-101440eae7be, clientType=HIVECLI]
25/04/09 10:12:04 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:12:04 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:12:04 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:12:04 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:12:04 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:12:04 INFO metastore: Connected to metastore.
25/04/09 10:12:04 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:12:04 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:12:04 INFO metastore: Connected to metastore.
25/04/09 10:12:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:12:04 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:12:04 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:12:04 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:12:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:12:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:12:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:12:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:12:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:12:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:12:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:12:04 INFO CodeGenerator: Code generated in 157.844968 ms
25/04/09 10:12:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:12:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:12:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39913 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:12:04 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:12:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:12:04 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:12:04 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:12:04 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:12:04 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:12:04 INFO DAGScheduler: Missing parents: List()
25/04/09 10:12:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:12:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:12:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:12:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39913 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:12:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:12:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:12:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:12:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:12:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:43453 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:12:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:43453 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:12:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1005 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:12:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:12:05 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.031 s
25/04/09 10:12:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:12:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:12:05 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.038971 s
25/04/09 10:12:05 INFO FileFormatWriter: Start to commit write Job 1e538627-413a-4088-a83d-b39f8c4aba5b.
25/04/09 10:12:05 INFO FileFormatWriter: Write Job 1e538627-413a-4088-a83d-b39f8c4aba5b committed. Elapsed time: 40 ms.
25/04/09 10:12:05 INFO FileFormatWriter: Finished processing stats for write job 1e538627-413a-4088-a83d-b39f8c4aba5b.
25/04/09 10:12:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:12:05 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:12:05 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:12:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:12:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:12:06 INFO MemoryStore: MemoryStore cleared
25/04/09 10:12:06 INFO BlockManager: BlockManager stopped
25/04/09 10:12:06 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:12:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:12:06 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:12:06 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:12:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b8a0c1e-e12d-4a81-a4b8-7cc8d158efc1
25/04/09 10:12:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-0b8a0c1e-e12d-4a81-a4b8-7cc8d158efc1/pyspark-8e23f91b-c166-4da8-88de-cffd0029ef3f
25/04/09 10:12:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-b8fe5312-b49e-495e-bf3c-3e187c6606bd
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:15:23 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:15:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:15:23 INFO ResourceUtils: ==============================================================
25/04/09 10:15:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:15:23 INFO ResourceUtils: ==============================================================
25/04/09 10:15:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:15:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:15:23 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:15:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:15:23 INFO SecurityManager: Changing view acls to: root
25/04/09 10:15:23 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:15:23 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:15:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:15:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:15:23 INFO Utils: Successfully started service 'sparkDriver' on port 43549.
25/04/09 10:15:23 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:15:23 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:15:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:15:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:15:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:15:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6c88b5fe-4c6d-40d9-93a6-17fa088f1748
25/04/09 10:15:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:15:23 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:15:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:15:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:15:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:15:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409101524-0038
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101524-0038/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:15:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101524-0038/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101524-0038/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:15:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101524-0038/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101524-0038/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:15:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101524-0038/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:15:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42361.
25/04/09 10:15:24 INFO NettyBlockTransferService: Server created on 3fada93ce917:42361
25/04/09 10:15:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:15:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 42361, None)
25/04/09 10:15:24 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:42361 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 42361, None)
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101524-0038/2 is now RUNNING
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101524-0038/0 is now RUNNING
25/04/09 10:15:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 42361, None)
25/04/09 10:15:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101524-0038/1 is now RUNNING
25/04/09 10:15:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 42361, None)
25/04/09 10:15:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:15:24 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:15:24 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:15:26 INFO InMemoryFileIndex: It took 75 ms to list leaf files for 1 paths.
25/04/09 10:15:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:39462) with ID 1,  ResourceProfileId 0
25/04/09 10:15:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:58876) with ID 0,  ResourceProfileId 0
25/04/09 10:15:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:56724) with ID 2,  ResourceProfileId 0
25/04/09 10:15:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41681 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 41681, None)
25/04/09 10:15:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:45297 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 45297, None)
25/04/09 10:15:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:34639 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 34639, None)
25/04/09 10:15:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:15:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:15:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:15:26 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:15:26 INFO DAGScheduler: Missing parents: List()
25/04/09 10:15:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:15:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:15:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:15:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:42361 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:15:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:15:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:15:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:15:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:15:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:34639 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:15:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1225 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:15:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:15:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.322 s
25/04/09 10:15:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:15:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:15:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.356834 s
25/04/09 10:15:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:42361 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:15:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:34639 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:15:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:15:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:15:29 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:15:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:15:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:15:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:15:29 INFO metastore: Connected to metastore.
25/04/09 10:15:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:15:30 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=a6b5a782-33b4-4842-b47c-ea85fe98a112, clientType=HIVECLI]
25/04/09 10:15:30 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:15:30 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:15:30 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:15:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:15:30 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:15:30 INFO metastore: Connected to metastore.
25/04/09 10:15:30 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:15:30 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:15:30 INFO metastore: Connected to metastore.
25/04/09 10:15:30 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:15:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:15:30 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:15:30 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:15:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:15:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:15:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:15:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:15:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:15:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:15:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:15:30 INFO CodeGenerator: Code generated in 164.111337 ms
25/04/09 10:15:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:15:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:15:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:42361 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:15:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:15:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:15:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:15:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:15:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:15:30 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:15:30 INFO DAGScheduler: Missing parents: List()
25/04/09 10:15:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:15:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:15:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:15:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:42361 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:15:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:15:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:15:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:15:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:15:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:34639 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:15:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:34639 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:15:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1035 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:15:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:15:31 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.062 s
25/04/09 10:15:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:15:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:15:31 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.069694 s
25/04/09 10:15:31 INFO FileFormatWriter: Start to commit write Job b4f753c7-91ed-42e7-90f2-316c8ae88202.
25/04/09 10:15:31 INFO FileFormatWriter: Write Job b4f753c7-91ed-42e7-90f2-316c8ae88202 committed. Elapsed time: 39 ms.
25/04/09 10:15:31 INFO FileFormatWriter: Finished processing stats for write job b4f753c7-91ed-42e7-90f2-316c8ae88202.
25/04/09 10:15:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:15:31 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:15:31 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:15:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:15:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:15:31 INFO MemoryStore: MemoryStore cleared
25/04/09 10:15:31 INFO BlockManager: BlockManager stopped
25/04/09 10:15:31 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:15:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:15:31 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:15:32 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:15:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1496d46-8e61-48f5-84e4-cf85df3712ec
25/04/09 10:15:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-2aedeaab-ba2c-4a0d-b64b-7bcfde62f56f
25/04/09 10:15:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-2aedeaab-ba2c-4a0d-b64b-7bcfde62f56f/pyspark-83fb9100-b117-4160-ab8c-9b99e47caa43
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:17:19 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:17:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:17:19 INFO ResourceUtils: ==============================================================
25/04/09 10:17:19 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:17:19 INFO ResourceUtils: ==============================================================
25/04/09 10:17:19 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:17:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:17:19 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:17:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:17:19 INFO SecurityManager: Changing view acls to: root
25/04/09 10:17:19 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:17:19 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:17:19 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:17:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:17:19 INFO Utils: Successfully started service 'sparkDriver' on port 44659.
25/04/09 10:17:19 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:17:19 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:17:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:17:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:17:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:17:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54831277-07c0-4617-b29c-ba9b843cd7c1
25/04/09 10:17:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:17:20 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:17:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:17:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:17:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 24 ms (0 ms spent in bootstraps)
25/04/09 10:17:20 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409101720-0040
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101720-0040/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:17:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101720-0040/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101720-0040/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:17:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101720-0040/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409101720-0040/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:17:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409101720-0040/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:17:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34103.
25/04/09 10:17:20 INFO NettyBlockTransferService: Server created on 3fada93ce917:34103
25/04/09 10:17:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:17:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 34103, None)
25/04/09 10:17:20 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:34103 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 34103, None)
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101720-0040/0 is now RUNNING
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101720-0040/2 is now RUNNING
25/04/09 10:17:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 34103, None)
25/04/09 10:17:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409101720-0040/1 is now RUNNING
25/04/09 10:17:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 34103, None)
25/04/09 10:17:20 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:17:20 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:17:21 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:17:22 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
25/04/09 10:17:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:51964) with ID 2,  ResourceProfileId 0
25/04/09 10:17:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40020) with ID 1,  ResourceProfileId 0
25/04/09 10:17:22 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:48092) with ID 0,  ResourceProfileId 0
25/04/09 10:17:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:46389 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 46389, None)
25/04/09 10:17:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:34891 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 34891, None)
25/04/09 10:17:22 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:43569 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 43569, None)
25/04/09 10:17:22 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:17:22 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:17:22 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:17:22 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:17:22 INFO DAGScheduler: Missing parents: List()
25/04/09 10:17:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:17:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:17:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:17:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:34103 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:17:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:17:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:17:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:17:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:17:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:46389 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:17:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1260 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:17:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:17:24 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.360 s
25/04/09 10:17:24 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:17:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:17:24 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.393768 s
25/04/09 10:17:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:34103 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:17:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:46389 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:17:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:17:25 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:17:25 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:17:25 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:17:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:17:25 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:17:25 INFO metastore: Connected to metastore.
25/04/09 10:17:26 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:17:26 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b693eb25-85ec-4810-8a75-7d0f5600bc18, clientType=HIVECLI]
25/04/09 10:17:26 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:17:26 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:17:26 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:17:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:17:26 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:17:26 INFO metastore: Connected to metastore.
25/04/09 10:17:26 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:17:26 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:17:26 INFO metastore: Connected to metastore.
25/04/09 10:17:26 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:17:26 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:17:26 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:17:26 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:17:26 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:17:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:17:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:17:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:17:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:17:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:17:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:17:26 INFO CodeGenerator: Code generated in 163.174559 ms
25/04/09 10:17:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:17:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:17:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:34103 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:17:26 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:17:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:17:27 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:17:27 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:17:27 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:17:27 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:17:27 INFO DAGScheduler: Missing parents: List()
25/04/09 10:17:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:17:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:17:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:17:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:34103 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:17:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:17:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:17:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:17:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:17:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:34891 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:17:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:34891 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:17:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2162 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:17:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:17:29 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.190 s
25/04/09 10:17:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:17:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:17:29 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.196713 s
25/04/09 10:17:29 INFO FileFormatWriter: Start to commit write Job 2f55cb6d-ef80-4046-826d-b42a035b8d04.
25/04/09 10:17:29 INFO FileFormatWriter: Write Job 2f55cb6d-ef80-4046-826d-b42a035b8d04 committed. Elapsed time: 44 ms.
25/04/09 10:17:29 INFO FileFormatWriter: Finished processing stats for write job 2f55cb6d-ef80-4046-826d-b42a035b8d04.
25/04/09 10:17:29 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:17:29 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:17:29 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:17:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:17:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:17:29 INFO MemoryStore: MemoryStore cleared
25/04/09 10:17:29 INFO BlockManager: BlockManager stopped
25/04/09 10:17:29 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:17:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:17:29 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:17:29 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:17:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-b579cca7-8504-4542-a431-7f69d3b7ad53
25/04/09 10:17:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e05b8df3-4995-4b83-9cb2-e542ee0995e1
25/04/09 10:17:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-e05b8df3-4995-4b83-9cb2-e542ee0995e1/pyspark-b322f6cf-7092-442f-bb88-0ec02aa6e612
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:20:48 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:20:48 INFO ResourceUtils: ==============================================================
25/04/09 10:20:48 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:20:48 INFO ResourceUtils: ==============================================================
25/04/09 10:20:48 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:20:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:20:48 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:20:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:20:48 INFO SecurityManager: Changing view acls to: root
25/04/09 10:20:48 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:20:48 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:20:48 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:20:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:20:48 INFO Utils: Successfully started service 'sparkDriver' on port 45435.
25/04/09 10:20:48 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:20:48 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:20:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:20:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:20:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:20:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c186bd3c-024f-46d2-a21e-5830b6be1ee9
25/04/09 10:20:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:20:48 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:20:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:20:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:20:48 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 25 ms (0 ms spent in bootstraps)
25/04/09 10:20:48 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409102048-0043
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102048-0043/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:20:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102048-0043/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102048-0043/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:20:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102048-0043/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102048-0043/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:20:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102048-0043/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:20:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44271.
25/04/09 10:20:48 INFO NettyBlockTransferService: Server created on 3fada93ce917:44271
25/04/09 10:20:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:20:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 44271, None)
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102048-0043/1 is now RUNNING
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102048-0043/0 is now RUNNING
25/04/09 10:20:48 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:44271 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 44271, None)
25/04/09 10:20:48 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102048-0043/2 is now RUNNING
25/04/09 10:20:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 44271, None)
25/04/09 10:20:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 44271, None)
25/04/09 10:20:49 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:20:49 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:20:49 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:20:50 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
25/04/09 10:20:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:59816) with ID 1,  ResourceProfileId 0
25/04/09 10:20:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:32874) with ID 2,  ResourceProfileId 0
25/04/09 10:20:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:42502) with ID 0,  ResourceProfileId 0
25/04/09 10:20:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:37063 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 37063, None)
25/04/09 10:20:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:39553 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 39553, None)
25/04/09 10:20:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:45811 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 45811, None)
25/04/09 10:20:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:20:51 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:20:51 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:20:51 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:20:51 INFO DAGScheduler: Missing parents: List()
25/04/09 10:20:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:20:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:20:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:20:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:44271 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:20:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:20:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:20:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:20:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:20:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:39553 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:20:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1222 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:20:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:20:52 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.323 s
25/04/09 10:20:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:20:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:20:52 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.359028 s
25/04/09 10:20:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:44271 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:20:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:39553 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:20:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:20:53 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:20:53 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:20:53 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:20:53 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:20:53 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:20:54 INFO metastore: Connected to metastore.
25/04/09 10:20:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:20:54 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=ce519875-0b30-4387-aa35-90cc2d632765, clientType=HIVECLI]
25/04/09 10:20:54 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:20:54 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:20:54 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:20:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:20:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:20:54 INFO metastore: Connected to metastore.
25/04/09 10:20:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:20:54 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:20:54 INFO metastore: Connected to metastore.
25/04/09 10:20:54 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:20:54 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:20:54 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:20:54 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:20:54 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:20:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:20:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:20:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:20:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:20:55 INFO CodeGenerator: Code generated in 158.647942 ms
25/04/09 10:20:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:20:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:20:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:44271 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:20:55 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:20:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:20:55 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:20:55 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:20:55 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:20:55 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:20:55 INFO DAGScheduler: Missing parents: List()
25/04/09 10:20:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:20:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:20:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:20:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:44271 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:20:55 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:20:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:20:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:20:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:20:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:37063 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:20:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:37063 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:20:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2157 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:20:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:20:57 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.184 s
25/04/09 10:20:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:20:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:20:57 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.190164 s
25/04/09 10:20:57 INFO FileFormatWriter: Start to commit write Job 94338ea5-ee67-4a80-9cad-dd70d5b16ec8.
25/04/09 10:20:57 INFO FileFormatWriter: Write Job 94338ea5-ee67-4a80-9cad-dd70d5b16ec8 committed. Elapsed time: 41 ms.
25/04/09 10:20:57 INFO FileFormatWriter: Finished processing stats for write job 94338ea5-ee67-4a80-9cad-dd70d5b16ec8.
25/04/09 10:20:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:20:57 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:20:57 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:20:57 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:20:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:20:57 INFO MemoryStore: MemoryStore cleared
25/04/09 10:20:57 INFO BlockManager: BlockManager stopped
25/04/09 10:20:57 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:20:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:20:57 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:20:57 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-24ab6376-cad5-46f9-8dd1-6e6dc0700abe
25/04/09 10:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-e221fda3-6c17-431d-b4bf-04069fd58849/pyspark-cdefca4d-a98f-4d15-97d1-04bb59d73a20
25/04/09 10:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-e221fda3-6c17-431d-b4bf-04069fd58849
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:22:26 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:22:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:22:27 INFO ResourceUtils: ==============================================================
25/04/09 10:22:27 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:22:27 INFO ResourceUtils: ==============================================================
25/04/09 10:22:27 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:22:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:22:27 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:22:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:22:27 INFO SecurityManager: Changing view acls to: root
25/04/09 10:22:27 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:22:27 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:22:27 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:22:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:22:27 INFO Utils: Successfully started service 'sparkDriver' on port 41461.
25/04/09 10:22:27 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:22:27 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:22:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:22:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:22:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:22:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-27dd8e89-45e7-4dfd-8bab-d22e76cc4ae3
25/04/09 10:22:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:22:27 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:22:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:22:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:22:27 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 24 ms (0 ms spent in bootstraps)
25/04/09 10:22:27 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409102227-0045
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102227-0045/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:22:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102227-0045/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102227-0045/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:22:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102227-0045/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102227-0045/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:22:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102227-0045/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:22:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40937.
25/04/09 10:22:27 INFO NettyBlockTransferService: Server created on 3fada93ce917:40937
25/04/09 10:22:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:22:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 40937, None)
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102227-0045/2 is now RUNNING
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102227-0045/0 is now RUNNING
25/04/09 10:22:27 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:40937 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 40937, None)
25/04/09 10:22:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102227-0045/1 is now RUNNING
25/04/09 10:22:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 40937, None)
25/04/09 10:22:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 40937, None)
25/04/09 10:22:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:22:28 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:22:28 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:22:29 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/09 10:22:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:51146) with ID 0,  ResourceProfileId 0
25/04/09 10:22:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42048) with ID 1,  ResourceProfileId 0
25/04/09 10:22:29 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:53862) with ID 2,  ResourceProfileId 0
25/04/09 10:22:29 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38707 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 38707, None)
25/04/09 10:22:29 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:37769 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 37769, None)
25/04/09 10:22:29 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:40135 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 40135, None)
25/04/09 10:22:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:22:30 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:22:30 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:22:30 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:22:30 INFO DAGScheduler: Missing parents: List()
25/04/09 10:22:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:22:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:22:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:22:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:40937 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:22:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:22:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:22:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:22:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:22:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:38707 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:22:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1193 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:22:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:22:31 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.292 s
25/04/09 10:22:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:22:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:22:31 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.325358 s
25/04/09 10:22:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:40937 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:22:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:38707 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:22:32 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:22:32 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:22:32 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:22:32 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:22:32 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:22:32 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:22:32 INFO metastore: Connected to metastore.
25/04/09 10:22:33 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:22:33 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e64ea908-36dc-4247-aec6-55e55876af62, clientType=HIVECLI]
25/04/09 10:22:33 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:22:33 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:22:33 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:22:33 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:22:33 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:22:33 INFO metastore: Connected to metastore.
25/04/09 10:22:33 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:22:33 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:22:33 INFO metastore: Connected to metastore.
25/04/09 10:22:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:22:33 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:22:33 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:22:33 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:22:33 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:22:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:22:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:22:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:22:34 INFO CodeGenerator: Code generated in 165.767429 ms
25/04/09 10:22:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:22:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:22:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:40937 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:22:34 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:22:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:22:34 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:22:34 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:22:34 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:22:34 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:22:34 INFO DAGScheduler: Missing parents: List()
25/04/09 10:22:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:22:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:22:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:22:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:40937 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:22:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:22:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:22:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:22:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:22:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:40135 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:22:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:40135 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:22:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2101 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:22:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:22:36 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.127 s
25/04/09 10:22:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:22:36 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.134739 s
25/04/09 10:22:36 INFO FileFormatWriter: Start to commit write Job 27aaf135-e49b-466a-b11e-e2ea749ded1e.
25/04/09 10:22:36 INFO FileFormatWriter: Write Job 27aaf135-e49b-466a-b11e-e2ea749ded1e committed. Elapsed time: 40 ms.
25/04/09 10:22:36 INFO FileFormatWriter: Finished processing stats for write job 27aaf135-e49b-466a-b11e-e2ea749ded1e.
25/04/09 10:22:36 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:22:36 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:22:36 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:22:36 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:22:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:22:36 INFO MemoryStore: MemoryStore cleared
25/04/09 10:22:36 INFO BlockManager: BlockManager stopped
25/04/09 10:22:36 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:22:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:22:36 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:22:36 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:22:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-be0469cc-2a4d-4dc1-8ecb-c8921c7b6fa0/pyspark-f2e3e27f-d709-4309-b95b-41dc65617533
25/04/09 10:22:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-9dc219fe-d32c-4b62-a65f-8c6828725e7c
25/04/09 10:22:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-be0469cc-2a4d-4dc1-8ecb-c8921c7b6fa0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:24:49 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:24:49 INFO ResourceUtils: ==============================================================
25/04/09 10:24:49 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:24:49 INFO ResourceUtils: ==============================================================
25/04/09 10:24:49 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:24:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:24:49 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:24:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:24:49 INFO SecurityManager: Changing view acls to: root
25/04/09 10:24:49 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:24:49 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:24:49 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:24:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:24:49 INFO Utils: Successfully started service 'sparkDriver' on port 32835.
25/04/09 10:24:49 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:24:49 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:24:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:24:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:24:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:24:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-18f2f685-a952-4cb9-a4f0-2ca55f6501c2
25/04/09 10:24:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:24:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:24:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:24:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:24:50 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 10:24:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409102450-0047
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102450-0047/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:24:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102450-0047/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102450-0047/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:24:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102450-0047/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102450-0047/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:24:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102450-0047/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:24:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33181.
25/04/09 10:24:50 INFO NettyBlockTransferService: Server created on 3fada93ce917:33181
25/04/09 10:24:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:24:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 33181, None)
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102450-0047/2 is now RUNNING
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102450-0047/0 is now RUNNING
25/04/09 10:24:50 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:33181 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 33181, None)
25/04/09 10:24:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102450-0047/1 is now RUNNING
25/04/09 10:24:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 33181, None)
25/04/09 10:24:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 33181, None)
25/04/09 10:24:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:24:50 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:24:50 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:24:51 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 10:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:59712) with ID 0,  ResourceProfileId 0
25/04/09 10:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:40310) with ID 2,  ResourceProfileId 0
25/04/09 10:24:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:50052) with ID 1,  ResourceProfileId 0
25/04/09 10:24:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:32929 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 32929, None)
25/04/09 10:24:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:42613 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 42613, None)
25/04/09 10:24:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40935 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40935, None)
25/04/09 10:24:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:24:52 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:24:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:24:52 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:24:52 INFO DAGScheduler: Missing parents: List()
25/04/09 10:24:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:24:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:24:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:24:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:33181 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:24:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:24:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:24:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:24:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:24:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:32929 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:24:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1157 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:24:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:24:53 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.258 s
25/04/09 10:24:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:24:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:24:53 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.291996 s
25/04/09 10:24:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:33181 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:24:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:32929 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:24:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:24:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:24:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:24:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:24:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:24:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:24:54 INFO metastore: Connected to metastore.
25/04/09 10:24:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:24:55 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1b5b38e6-1a40-4bb3-b89c-12fea21d6b62, clientType=HIVECLI]
25/04/09 10:24:55 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:24:55 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:24:55 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:24:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:24:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:24:55 INFO metastore: Connected to metastore.
25/04/09 10:24:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:24:55 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:24:55 INFO metastore: Connected to metastore.
25/04/09 10:24:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:24:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:24:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:24:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:24:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:24:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:24:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:24:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:24:56 INFO CodeGenerator: Code generated in 148.481648 ms
25/04/09 10:24:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:24:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:24:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:33181 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:24:56 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:24:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:24:56 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:24:56 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:24:56 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:24:56 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:24:56 INFO DAGScheduler: Missing parents: List()
25/04/09 10:24:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:24:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:24:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:24:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:33181 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:24:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:24:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:24:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:24:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:24:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:42613 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:24:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:42613 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:24:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1997 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:24:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:24:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.022 s
25/04/09 10:24:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:24:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:24:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.029538 s
25/04/09 10:24:58 INFO FileFormatWriter: Start to commit write Job dbb70ab3-a137-4df0-abd0-1a44647205a1.
25/04/09 10:24:58 INFO FileFormatWriter: Write Job dbb70ab3-a137-4df0-abd0-1a44647205a1 committed. Elapsed time: 39 ms.
25/04/09 10:24:58 INFO FileFormatWriter: Finished processing stats for write job dbb70ab3-a137-4df0-abd0-1a44647205a1.
25/04/09 10:24:58 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:24:58 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:24:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:24:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:24:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:24:58 INFO MemoryStore: MemoryStore cleared
25/04/09 10:24:58 INFO BlockManager: BlockManager stopped
25/04/09 10:24:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:24:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:24:58 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:24:58 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ded78ab-4031-4010-8696-f4724d0c69e0/pyspark-b2cdb947-59f4-4801-90d1-53cb751c7e0e
25/04/09 10:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-7a87cadc-da91-49e3-b445-fc111b41d9dc
25/04/09 10:24:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ded78ab-4031-4010-8696-f4724d0c69e0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:27:41 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:27:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:27:42 INFO ResourceUtils: ==============================================================
25/04/09 10:27:42 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:27:42 INFO ResourceUtils: ==============================================================
25/04/09 10:27:42 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:27:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:27:42 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:27:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:27:42 INFO SecurityManager: Changing view acls to: root
25/04/09 10:27:42 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:27:42 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:27:42 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:27:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:27:42 INFO Utils: Successfully started service 'sparkDriver' on port 32897.
25/04/09 10:27:42 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:27:42 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:27:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:27:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:27:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:27:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2f1ba170-07d5-4f2c-a5b3-501f425522bb
25/04/09 10:27:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:27:42 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:27:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:27:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:27:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 10:27:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409102742-0049
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102742-0049/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:27:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102742-0049/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102742-0049/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:27:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102742-0049/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102742-0049/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:27:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102742-0049/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:27:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44035.
25/04/09 10:27:42 INFO NettyBlockTransferService: Server created on 3fada93ce917:44035
25/04/09 10:27:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:27:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 44035, None)
25/04/09 10:27:42 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:44035 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 44035, None)
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102742-0049/1 is now RUNNING
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102742-0049/2 is now RUNNING
25/04/09 10:27:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102742-0049/0 is now RUNNING
25/04/09 10:27:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 44035, None)
25/04/09 10:27:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 44035, None)
25/04/09 10:27:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:27:43 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:27:43 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:27:44 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/09 10:27:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:40184) with ID 2,  ResourceProfileId 0
25/04/09 10:27:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:43700) with ID 0,  ResourceProfileId 0
25/04/09 10:27:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58440) with ID 1,  ResourceProfileId 0
25/04/09 10:27:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:40029 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 40029, None)
25/04/09 10:27:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:42941 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 42941, None)
25/04/09 10:27:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:38957 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 38957, None)
25/04/09 10:27:44 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:27:44 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:27:44 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:27:44 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:27:44 INFO DAGScheduler: Missing parents: List()
25/04/09 10:27:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:27:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:27:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:27:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:44035 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:27:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:27:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:27:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:27:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:27:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:40029 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:27:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1192 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:27:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:27:46 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.287 s
25/04/09 10:27:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:27:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:27:46 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.322360 s
25/04/09 10:27:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:44035 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:27:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:40029 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:27:47 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:27:47 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:27:47 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:27:47 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:27:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:27:47 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:27:47 INFO metastore: Connected to metastore.
25/04/09 10:27:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:27:48 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=73be2d14-1565-4393-84ae-ff93cd7819e6, clientType=HIVECLI]
25/04/09 10:27:48 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:27:48 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:27:48 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:27:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:27:48 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:27:48 INFO metastore: Connected to metastore.
25/04/09 10:27:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:27:48 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:27:48 INFO metastore: Connected to metastore.
25/04/09 10:27:48 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:27:48 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:27:48 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:27:48 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:27:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:27:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:27:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:27:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:27:48 INFO CodeGenerator: Code generated in 159.110752 ms
25/04/09 10:27:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:27:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:27:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:44035 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:27:48 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:27:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:27:49 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:27:49 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:27:49 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:27:49 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:27:49 INFO DAGScheduler: Missing parents: List()
25/04/09 10:27:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:27:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:27:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:27:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:44035 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:27:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:27:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:27:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:27:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:27:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:42941 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:27:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:42941 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:27:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2073 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:27:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:27:51 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.100 s
25/04/09 10:27:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:27:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:27:51 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.108180 s
25/04/09 10:27:51 INFO FileFormatWriter: Start to commit write Job 0fb28496-4d32-47e0-bcc2-abb9e78e6555.
25/04/09 10:27:51 INFO FileFormatWriter: Write Job 0fb28496-4d32-47e0-bcc2-abb9e78e6555 committed. Elapsed time: 40 ms.
25/04/09 10:27:51 INFO FileFormatWriter: Finished processing stats for write job 0fb28496-4d32-47e0-bcc2-abb9e78e6555.
25/04/09 10:27:51 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:27:51 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:27:51 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:27:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:27:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:27:51 INFO MemoryStore: MemoryStore cleared
25/04/09 10:27:51 INFO BlockManager: BlockManager stopped
25/04/09 10:27:51 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:27:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:27:51 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:27:51 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:27:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-50d33a98-046d-4372-90a0-b48617358680
25/04/09 10:27:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-e54c127e-282d-42c7-a817-e743aab4847e
25/04/09 10:27:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-50d33a98-046d-4372-90a0-b48617358680/pyspark-32b57b76-5ddb-40ff-adf8-245ad3c75143
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:29:18 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:29:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:29:18 INFO ResourceUtils: ==============================================================
25/04/09 10:29:18 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:29:18 INFO ResourceUtils: ==============================================================
25/04/09 10:29:18 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:29:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:29:18 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:29:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:29:18 INFO SecurityManager: Changing view acls to: root
25/04/09 10:29:18 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:29:18 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:29:18 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:29:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:29:18 INFO Utils: Successfully started service 'sparkDriver' on port 46547.
25/04/09 10:29:19 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:29:19 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:29:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:29:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:29:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:29:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2c612796-9ccc-4489-a2c4-d0704bc0d51a
25/04/09 10:29:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:29:19 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:29:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:29:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:29:19 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 21 ms (0 ms spent in bootstraps)
25/04/09 10:29:19 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409102919-0051
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102919-0051/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:29:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102919-0051/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102919-0051/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:29:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102919-0051/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409102919-0051/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:29:19 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409102919-0051/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:29:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39365.
25/04/09 10:29:19 INFO NettyBlockTransferService: Server created on 3fada93ce917:39365
25/04/09 10:29:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:29:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39365, None)
25/04/09 10:29:19 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39365 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39365, None)
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102919-0051/0 is now RUNNING
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102919-0051/1 is now RUNNING
25/04/09 10:29:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39365, None)
25/04/09 10:29:19 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409102919-0051/2 is now RUNNING
25/04/09 10:29:19 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39365, None)
25/04/09 10:29:19 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:29:19 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:29:19 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:29:21 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
25/04/09 10:29:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:33162) with ID 1,  ResourceProfileId 0
25/04/09 10:29:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:47440) with ID 2,  ResourceProfileId 0
25/04/09 10:29:21 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:51248) with ID 0,  ResourceProfileId 0
25/04/09 10:29:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:38045 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 38045, None)
25/04/09 10:29:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:36255 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 36255, None)
25/04/09 10:29:21 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41317 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 41317, None)
25/04/09 10:29:21 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:29:21 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:29:21 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:29:21 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:29:21 INFO DAGScheduler: Missing parents: List()
25/04/09 10:29:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:29:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:29:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:29:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39365 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:29:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:29:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:29:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:29:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:29:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:38045 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:29:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1201 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:29:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:29:22 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.295 s
25/04/09 10:29:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:29:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:29:22 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.329244 s
25/04/09 10:29:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39365 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:29:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:38045 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:29:24 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:29:24 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:29:24 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:29:24 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:29:24 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:29:24 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:29:24 INFO metastore: Connected to metastore.
25/04/09 10:29:24 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:29:25 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=3e1f13b0-fd1e-4cc0-91cd-e9063601be7d, clientType=HIVECLI]
25/04/09 10:29:25 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:29:25 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:29:25 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:29:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:29:25 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:29:25 INFO metastore: Connected to metastore.
25/04/09 10:29:25 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:29:25 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:29:25 INFO metastore: Connected to metastore.
25/04/09 10:29:25 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:29:25 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:29:25 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:29:25 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:29:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:29:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:29:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:29:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:29:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:29:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:29:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:29:25 INFO CodeGenerator: Code generated in 166.941561 ms
25/04/09 10:29:25 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:29:25 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:29:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39365 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:29:25 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:29:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:29:25 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:29:25 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:29:25 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:29:25 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:29:25 INFO DAGScheduler: Missing parents: List()
25/04/09 10:29:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:29:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:29:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:29:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39365 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:29:25 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:29:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:29:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:29:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:29:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:36255 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:29:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:36255 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:29:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2131 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:29:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:29:27 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.158 s
25/04/09 10:29:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:29:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:29:27 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.165619 s
25/04/09 10:29:27 INFO FileFormatWriter: Start to commit write Job e60e8988-fb9a-4e47-8043-a1276f734009.
25/04/09 10:29:27 INFO FileFormatWriter: Write Job e60e8988-fb9a-4e47-8043-a1276f734009 committed. Elapsed time: 40 ms.
25/04/09 10:29:27 INFO FileFormatWriter: Finished processing stats for write job e60e8988-fb9a-4e47-8043-a1276f734009.
25/04/09 10:29:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:29:27 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:29:27 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:29:27 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:29:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:29:28 INFO MemoryStore: MemoryStore cleared
25/04/09 10:29:28 INFO BlockManager: BlockManager stopped
25/04/09 10:29:28 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:29:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:29:28 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:29:28 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:29:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-e51ef9b8-bf80-41aa-b75c-8811f95890ee
25/04/09 10:29:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7f70663-1fef-4a4d-8819-d08cae39e273
25/04/09 10:29:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7f70663-1fef-4a4d-8819-d08cae39e273/pyspark-7e79a08c-da34-42ff-985f-a226365fb1ab
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:31:59 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:31:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:31:59 INFO ResourceUtils: ==============================================================
25/04/09 10:31:59 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:31:59 INFO ResourceUtils: ==============================================================
25/04/09 10:31:59 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:31:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:31:59 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:31:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:31:59 INFO SecurityManager: Changing view acls to: root
25/04/09 10:31:59 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:31:59 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:31:59 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:31:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:32:00 INFO Utils: Successfully started service 'sparkDriver' on port 37873.
25/04/09 10:32:00 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:32:00 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:32:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:32:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:32:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:32:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cd50c3c0-7e58-430d-b66b-f13f0d640188
25/04/09 10:32:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:32:00 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:32:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:32:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:32:00 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:32:00 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409103200-0053
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103200-0053/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:32:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103200-0053/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103200-0053/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:32:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103200-0053/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103200-0053/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:32:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103200-0053/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:32:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41189.
25/04/09 10:32:00 INFO NettyBlockTransferService: Server created on 3fada93ce917:41189
25/04/09 10:32:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:32:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 41189, None)
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103200-0053/0 is now RUNNING
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103200-0053/2 is now RUNNING
25/04/09 10:32:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103200-0053/1 is now RUNNING
25/04/09 10:32:00 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:41189 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 41189, None)
25/04/09 10:32:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 41189, None)
25/04/09 10:32:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 41189, None)
25/04/09 10:32:00 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:32:00 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:32:00 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:32:02 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/09 10:32:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:32876) with ID 2,  ResourceProfileId 0
25/04/09 10:32:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:53686) with ID 1,  ResourceProfileId 0
25/04/09 10:32:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:51302) with ID 0,  ResourceProfileId 0
25/04/09 10:32:02 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:33791 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 33791, None)
25/04/09 10:32:02 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33163 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33163, None)
25/04/09 10:32:02 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:34189 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 34189, None)
25/04/09 10:32:02 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:32:02 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:32:02 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:32:02 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:32:02 INFO DAGScheduler: Missing parents: List()
25/04/09 10:32:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:32:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:32:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:32:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:41189 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:32:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:32:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:32:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:32:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:32:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:33791 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:32:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1142 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:32:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:32:03 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.235 s
25/04/09 10:32:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:32:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:32:03 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.266502 s
25/04/09 10:32:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:41189 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:32:04 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:33791 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:32:04 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:32:04 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:32:05 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:32:05 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:32:05 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:32:05 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:32:05 INFO metastore: Connected to metastore.
25/04/09 10:32:05 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:32:05 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=986f2389-8d70-40b6-bf04-9462ef41829a, clientType=HIVECLI]
25/04/09 10:32:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:32:05 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:32:05 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:32:05 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:32:05 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:32:05 INFO metastore: Connected to metastore.
25/04/09 10:32:05 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:32:05 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:32:05 INFO metastore: Connected to metastore.
25/04/09 10:32:06 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:32:06 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:32:06 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:32:06 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:32:06 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:32:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:32:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:32:06 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:32:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:32:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:32:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:32:06 INFO CodeGenerator: Code generated in 152.654826 ms
25/04/09 10:32:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:32:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:32:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:41189 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:32:06 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:32:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:32:06 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:32:06 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:32:06 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:32:06 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:32:06 INFO DAGScheduler: Missing parents: List()
25/04/09 10:32:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:32:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:32:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:32:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:41189 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:32:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:32:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:32:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:32:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:32:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:34189 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:32:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:34189 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:32:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2013 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:32:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:32:08 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.039 s
25/04/09 10:32:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:32:08 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.046273 s
25/04/09 10:32:08 INFO FileFormatWriter: Start to commit write Job e207c577-5a7c-47fa-b2f9-84e521a7f523.
25/04/09 10:32:08 INFO FileFormatWriter: Write Job e207c577-5a7c-47fa-b2f9-84e521a7f523 committed. Elapsed time: 40 ms.
25/04/09 10:32:08 INFO FileFormatWriter: Finished processing stats for write job e207c577-5a7c-47fa-b2f9-84e521a7f523.
25/04/09 10:32:08 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:32:08 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:32:08 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:32:08 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:32:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:32:08 INFO MemoryStore: MemoryStore cleared
25/04/09 10:32:08 INFO BlockManager: BlockManager stopped
25/04/09 10:32:08 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:32:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:32:08 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:32:08 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-65ad4de8-68a8-4742-8c31-d6951d874bf9/pyspark-dede5dae-ca2e-4eb3-b419-6f82da567340
25/04/09 10:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-65ad4de8-68a8-4742-8c31-d6951d874bf9
25/04/09 10:32:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-b17aab9e-8113-409b-97be-cf675c5fdad2
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:34:34 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:34:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:34:34 INFO ResourceUtils: ==============================================================
25/04/09 10:34:34 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:34:34 INFO ResourceUtils: ==============================================================
25/04/09 10:34:34 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:34:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:34:34 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:34:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:34:34 INFO SecurityManager: Changing view acls to: root
25/04/09 10:34:34 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:34:34 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:34:34 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:34:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:34:34 INFO Utils: Successfully started service 'sparkDriver' on port 42637.
25/04/09 10:34:34 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:34:34 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:34:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:34:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:34:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:34:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dadd6529-9543-4c0a-9eb5-32fa191238b2
25/04/09 10:34:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:34:34 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:34:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:34:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:34:35 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 25 ms (0 ms spent in bootstraps)
25/04/09 10:34:35 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409103435-0055
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103435-0055/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:34:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103435-0055/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103435-0055/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:34:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103435-0055/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103435-0055/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:34:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103435-0055/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:34:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35791.
25/04/09 10:34:35 INFO NettyBlockTransferService: Server created on 3fada93ce917:35791
25/04/09 10:34:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:34:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 35791, None)
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103435-0055/0 is now RUNNING
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103435-0055/2 is now RUNNING
25/04/09 10:34:35 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:35791 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 35791, None)
25/04/09 10:34:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 35791, None)
25/04/09 10:34:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103435-0055/1 is now RUNNING
25/04/09 10:34:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 35791, None)
25/04/09 10:34:35 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:34:35 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:34:35 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:34:37 INFO InMemoryFileIndex: It took 67 ms to list leaf files for 1 paths.
25/04/09 10:34:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:34828) with ID 1,  ResourceProfileId 0
25/04/09 10:34:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:46550) with ID 2,  ResourceProfileId 0
25/04/09 10:34:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:48178) with ID 0,  ResourceProfileId 0
25/04/09 10:34:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:40837 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 40837, None)
25/04/09 10:34:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41985 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41985, None)
25/04/09 10:34:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:45735 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 45735, None)
25/04/09 10:34:37 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:34:37 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:34:37 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:34:37 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:34:37 INFO DAGScheduler: Missing parents: List()
25/04/09 10:34:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:34:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:34:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:34:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:35791 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:34:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:34:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:34:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:34:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:34:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:45735 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:34:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1261 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:34:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:34:39 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.356 s
25/04/09 10:34:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:34:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:34:39 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.391940 s
25/04/09 10:34:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:35791 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:34:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:45735 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:34:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:34:40 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:34:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:34:40 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:34:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:34:40 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:34:40 INFO metastore: Connected to metastore.
25/04/09 10:34:41 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:34:41 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=637447e3-2e7f-4df8-b389-61786cf91d5b, clientType=HIVECLI]
25/04/09 10:34:41 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:34:41 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:34:41 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:34:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:34:41 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:34:41 INFO metastore: Connected to metastore.
25/04/09 10:34:41 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:34:41 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:34:41 INFO metastore: Connected to metastore.
25/04/09 10:34:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:34:41 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:34:41 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:34:41 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:34:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:34:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:34:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:34:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:34:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:34:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:34:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:34:41 INFO CodeGenerator: Code generated in 164.50817 ms
25/04/09 10:34:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:34:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:34:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:35791 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:34:41 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:34:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:34:41 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:34:41 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:34:41 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:34:41 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:34:41 INFO DAGScheduler: Missing parents: List()
25/04/09 10:34:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:34:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:34:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:34:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:35791 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:34:42 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:34:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:34:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:34:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:34:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:45735 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:34:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:45735 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:34:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1055 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:34:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:34:43 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.082 s
25/04/09 10:34:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:34:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:34:43 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.089182 s
25/04/09 10:34:43 INFO FileFormatWriter: Start to commit write Job f73db105-ef5d-4bac-a3e3-c6f189767fbc.
25/04/09 10:34:43 INFO FileFormatWriter: Write Job f73db105-ef5d-4bac-a3e3-c6f189767fbc committed. Elapsed time: 44 ms.
25/04/09 10:34:43 INFO FileFormatWriter: Finished processing stats for write job f73db105-ef5d-4bac-a3e3-c6f189767fbc.
25/04/09 10:34:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:34:43 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:34:43 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:34:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:34:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:34:43 INFO MemoryStore: MemoryStore cleared
25/04/09 10:34:43 INFO BlockManager: BlockManager stopped
25/04/09 10:34:43 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:34:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:34:43 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:34:43 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:34:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-02bf8ec6-1c9a-4965-a56b-a04c076d23d1
25/04/09 10:34:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-dbe2f6a9-e257-48d0-b244-82e3a7ec5479
25/04/09 10:34:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-02bf8ec6-1c9a-4965-a56b-a04c076d23d1/pyspark-3ccb5685-597b-4845-be8c-5af0bda8e53e
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:37:02 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:37:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:37:02 INFO ResourceUtils: ==============================================================
25/04/09 10:37:02 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:37:02 INFO ResourceUtils: ==============================================================
25/04/09 10:37:02 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:37:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:37:02 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:37:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:37:02 INFO SecurityManager: Changing view acls to: root
25/04/09 10:37:02 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:37:02 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:37:02 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:37:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:37:02 INFO Utils: Successfully started service 'sparkDriver' on port 36891.
25/04/09 10:37:02 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:37:02 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:37:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:37:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:37:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:37:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a000158a-39e1-4d04-a011-d49067169df8
25/04/09 10:37:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:37:02 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:37:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:37:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:37:03 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:37:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409103703-0057
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103703-0057/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103703-0057/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103703-0057/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103703-0057/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409103703-0057/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409103703-0057/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:37:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39893.
25/04/09 10:37:03 INFO NettyBlockTransferService: Server created on 3fada93ce917:39893
25/04/09 10:37:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:37:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39893, None)
25/04/09 10:37:03 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39893 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39893, None)
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103703-0057/0 is now RUNNING
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103703-0057/2 is now RUNNING
25/04/09 10:37:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39893, None)
25/04/09 10:37:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39893, None)
25/04/09 10:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409103703-0057/1 is now RUNNING
25/04/09 10:37:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:37:03 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:37:03 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:37:04 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 10:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:32812) with ID 2,  ResourceProfileId 0
25/04/09 10:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:58512) with ID 0,  ResourceProfileId 0
25/04/09 10:37:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:41832) with ID 1,  ResourceProfileId 0
25/04/09 10:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:37591 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 37591, None)
25/04/09 10:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:43395 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 43395, None)
25/04/09 10:37:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:39549 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 39549, None)
25/04/09 10:37:05 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:37:05 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:37:05 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:37:05 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:37:05 INFO DAGScheduler: Missing parents: List()
25/04/09 10:37:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:37:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:37:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:37:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39893 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:37:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:37:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:37:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:37:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:37:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:37591 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:37:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1215 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:37:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:37:06 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.311 s
25/04/09 10:37:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:37:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:37:06 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.344480 s
25/04/09 10:37:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39893 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:37:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:37591 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:37:07 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:37:07 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:37:08 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:37:08 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:37:08 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:37:08 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:37:08 INFO metastore: Connected to metastore.
25/04/09 10:37:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:37:08 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=52499de9-6e45-4fce-8243-ac5f360aeeaa, clientType=HIVECLI]
25/04/09 10:37:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:37:08 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:37:08 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:37:08 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:37:08 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:37:08 INFO metastore: Connected to metastore.
25/04/09 10:37:08 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:37:08 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:37:08 INFO metastore: Connected to metastore.
25/04/09 10:37:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:37:09 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:37:09 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:37:09 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:37:09 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:37:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:37:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:37:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:37:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:37:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:37:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:37:09 INFO CodeGenerator: Code generated in 157.999838 ms
25/04/09 10:37:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:37:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:37:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39893 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:37:09 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:37:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:37:09 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:37:09 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:37:09 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:37:09 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:37:09 INFO DAGScheduler: Missing parents: List()
25/04/09 10:37:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:37:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:37:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:37:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39893 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:37:09 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:37:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:37:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:37:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:37:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:39549 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:37:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:39549 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:37:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2103 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:37:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:37:11 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.131 s
25/04/09 10:37:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:37:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:37:11 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.138999 s
25/04/09 10:37:11 INFO FileFormatWriter: Start to commit write Job 36ebf6f8-5eaa-47c2-ab5e-b0a68d3cdc14.
25/04/09 10:37:11 INFO FileFormatWriter: Write Job 36ebf6f8-5eaa-47c2-ab5e-b0a68d3cdc14 committed. Elapsed time: 41 ms.
25/04/09 10:37:11 INFO FileFormatWriter: Finished processing stats for write job 36ebf6f8-5eaa-47c2-ab5e-b0a68d3cdc14.
25/04/09 10:37:11 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:37:11 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:37:11 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:37:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:37:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:37:11 INFO MemoryStore: MemoryStore cleared
25/04/09 10:37:11 INFO BlockManager: BlockManager stopped
25/04/09 10:37:11 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:37:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:37:11 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:37:12 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-eebca62c-b578-490b-9dac-397427e4bdd4
25/04/09 10:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-946eb905-193b-482a-a61e-873449638233/pyspark-a8ab5dce-3ec0-4f67-afce-6627e75ce2b1
25/04/09 10:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-946eb905-193b-482a-a61e-873449638233
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:41:29 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:41:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:41:29 INFO ResourceUtils: ==============================================================
25/04/09 10:41:29 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:41:29 INFO ResourceUtils: ==============================================================
25/04/09 10:41:29 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:41:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:41:29 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:41:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:41:29 INFO SecurityManager: Changing view acls to: root
25/04/09 10:41:29 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:41:29 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:41:29 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:41:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:41:29 INFO Utils: Successfully started service 'sparkDriver' on port 38905.
25/04/09 10:41:29 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:41:29 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:41:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:41:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:41:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:41:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-851a2c25-dbeb-4940-920a-7254470e95ac
25/04/09 10:41:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:41:29 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:41:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:41:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:41:29 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:41:29 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:41:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409104130-0060
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104130-0060/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:41:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104130-0060/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104130-0060/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:41:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104130-0060/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104130-0060/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:41:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104130-0060/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:41:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42503.
25/04/09 10:41:30 INFO NettyBlockTransferService: Server created on 3fada93ce917:42503
25/04/09 10:41:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:41:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 42503, None)
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104130-0060/0 is now RUNNING
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104130-0060/2 is now RUNNING
25/04/09 10:41:30 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104130-0060/1 is now RUNNING
25/04/09 10:41:30 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:42503 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 42503, None)
25/04/09 10:41:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 42503, None)
25/04/09 10:41:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 42503, None)
25/04/09 10:41:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:41:30 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:41:30 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:41:31 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/09 10:41:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:60534) with ID 2,  ResourceProfileId 0
25/04/09 10:41:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:56740) with ID 0,  ResourceProfileId 0
25/04/09 10:41:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:42228) with ID 1,  ResourceProfileId 0
25/04/09 10:41:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:39979 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 39979, None)
25/04/09 10:41:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44701 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44701, None)
25/04/09 10:41:32 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:36177 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 36177, None)
25/04/09 10:41:32 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:41:32 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:41:32 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:41:32 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:41:32 INFO DAGScheduler: Missing parents: List()
25/04/09 10:41:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:41:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:41:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:41:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:42503 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:41:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:41:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:41:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:41:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:41:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:36177 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:41:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1183 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:41:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:41:33 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.278 s
25/04/09 10:41:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:41:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:41:33 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.309614 s
25/04/09 10:41:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:42503 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:41:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:36177 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:41:34 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:41:34 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:41:34 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:41:34 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:41:34 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:41:34 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:41:34 INFO metastore: Connected to metastore.
25/04/09 10:41:35 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:41:35 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e6607b27-2adf-4143-ba04-2f23f2e9d100, clientType=HIVECLI]
25/04/09 10:41:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:41:35 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:41:35 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:41:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:41:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:41:35 INFO metastore: Connected to metastore.
25/04/09 10:41:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:41:35 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:41:35 INFO metastore: Connected to metastore.
25/04/09 10:41:35 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:41:35 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:41:35 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:41:35 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:41:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:41:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:41:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:41:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:41:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:41:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:41:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:41:36 INFO CodeGenerator: Code generated in 160.633232 ms
25/04/09 10:41:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:41:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:41:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:42503 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:41:36 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:41:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:41:36 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:41:36 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:41:36 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:41:36 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:41:36 INFO DAGScheduler: Missing parents: List()
25/04/09 10:41:36 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:41:36 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:41:36 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:41:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:42503 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:41:36 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:41:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:41:36 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:41:36 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:41:36 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:36177 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:41:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:36177 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:41:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1047 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:41:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:41:37 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.075 s
25/04/09 10:41:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:41:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:41:37 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.083358 s
25/04/09 10:41:37 INFO FileFormatWriter: Start to commit write Job b5c95ffb-834d-4b6a-b671-3eaa4c17d4cd.
25/04/09 10:41:37 INFO FileFormatWriter: Write Job b5c95ffb-834d-4b6a-b671-3eaa4c17d4cd committed. Elapsed time: 42 ms.
25/04/09 10:41:37 INFO FileFormatWriter: Finished processing stats for write job b5c95ffb-834d-4b6a-b671-3eaa4c17d4cd.
25/04/09 10:41:37 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:41:37 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:41:37 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:41:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:41:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:41:37 INFO MemoryStore: MemoryStore cleared
25/04/09 10:41:37 INFO BlockManager: BlockManager stopped
25/04/09 10:41:37 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:41:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:41:37 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:41:37 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:41:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-611bc81a-d56e-468c-92b5-3a8e57ec9021/pyspark-4a0e15ee-4de9-4f8d-b47a-e22d81788e7f
25/04/09 10:41:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-872ff83c-848f-470e-954f-dd93a7d96bbe
25/04/09 10:41:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-611bc81a-d56e-468c-92b5-3a8e57ec9021
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:43:04 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:43:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:43:04 INFO ResourceUtils: ==============================================================
25/04/09 10:43:04 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:43:04 INFO ResourceUtils: ==============================================================
25/04/09 10:43:04 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:43:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:43:04 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:43:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:43:04 INFO SecurityManager: Changing view acls to: root
25/04/09 10:43:04 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:43:04 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:43:04 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:43:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:43:04 INFO Utils: Successfully started service 'sparkDriver' on port 45585.
25/04/09 10:43:04 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:43:04 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:43:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:43:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:43:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:43:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-30895308-4475-46af-a605-f7cb3ebf631b
25/04/09 10:43:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:43:04 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:43:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:43:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:43:05 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:43:05 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409104305-0062
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104305-0062/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:43:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104305-0062/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104305-0062/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:43:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104305-0062/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104305-0062/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:43:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104305-0062/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:43:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34537.
25/04/09 10:43:05 INFO NettyBlockTransferService: Server created on 3fada93ce917:34537
25/04/09 10:43:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:43:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 34537, None)
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104305-0062/0 is now RUNNING
25/04/09 10:43:05 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:34537 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 34537, None)
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104305-0062/2 is now RUNNING
25/04/09 10:43:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104305-0062/1 is now RUNNING
25/04/09 10:43:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 34537, None)
25/04/09 10:43:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 34537, None)
25/04/09 10:43:05 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:43:05 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:43:05 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:43:07 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/09 10:43:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:50548) with ID 1,  ResourceProfileId 0
25/04/09 10:43:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:33598) with ID 0,  ResourceProfileId 0
25/04/09 10:43:07 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:34530) with ID 2,  ResourceProfileId 0
25/04/09 10:43:07 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33637 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33637, None)
25/04/09 10:43:07 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:42313 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 42313, None)
25/04/09 10:43:07 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38247 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 38247, None)
25/04/09 10:43:07 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:43:07 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:43:07 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:43:07 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:43:07 INFO DAGScheduler: Missing parents: List()
25/04/09 10:43:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:43:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:43:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:43:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:34537 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:43:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:43:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:43:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:43:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:43:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:33637 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:43:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1215 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:43:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:43:08 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.314 s
25/04/09 10:43:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:43:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:43:08 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.348562 s
25/04/09 10:43:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:34537 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:43:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:33637 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:43:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:43:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:43:10 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:43:10 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:43:10 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:43:10 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:43:10 INFO metastore: Connected to metastore.
25/04/09 10:43:10 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
25/04/09 10:43:11 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=40dd8c0c-b7ca-4e20-b123-2d96112b3b05, clientType=HIVECLI]
25/04/09 10:43:11 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:43:11 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:43:11 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:43:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:43:11 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:43:11 INFO metastore: Connected to metastore.
25/04/09 10:43:11 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:43:11 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:43:11 INFO metastore: Connected to metastore.
25/04/09 10:43:11 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:43:11 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:43:11 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:43:11 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:43:11 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:43:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:43:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:43:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:43:11 INFO CodeGenerator: Code generated in 167.570128 ms
25/04/09 10:43:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:43:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:43:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:34537 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:43:11 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:43:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:43:11 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:43:11 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:43:11 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:43:11 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:43:11 INFO DAGScheduler: Missing parents: List()
25/04/09 10:43:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:43:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:43:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:43:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:34537 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:43:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:43:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:43:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:43:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:43:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:38247 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:43:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:38247 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:43:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2159 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:43:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:43:13 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.190 s
25/04/09 10:43:13 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:43:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:43:13 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.198428 s
25/04/09 10:43:13 INFO FileFormatWriter: Start to commit write Job d8531929-78d8-4361-94ea-f02de04b8730.
25/04/09 10:43:13 INFO FileFormatWriter: Write Job d8531929-78d8-4361-94ea-f02de04b8730 committed. Elapsed time: 39 ms.
25/04/09 10:43:13 INFO FileFormatWriter: Finished processing stats for write job d8531929-78d8-4361-94ea-f02de04b8730.
25/04/09 10:43:13 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:43:13 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:43:13 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:43:13 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:43:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:43:14 INFO MemoryStore: MemoryStore cleared
25/04/09 10:43:14 INFO BlockManager: BlockManager stopped
25/04/09 10:43:14 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:43:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:43:14 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:43:14 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:43:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-3769aff6-5ec5-420d-ae34-cc6919048c59
25/04/09 10:43:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-0f6cd07a-08b5-493d-b131-9977cfffb76d
25/04/09 10:43:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-3769aff6-5ec5-420d-ae34-cc6919048c59/pyspark-d27de944-ad69-4966-8009-d264895253bb
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:45:57 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:45:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:45:57 INFO ResourceUtils: ==============================================================
25/04/09 10:45:57 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:45:57 INFO ResourceUtils: ==============================================================
25/04/09 10:45:57 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:45:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:45:57 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:45:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:45:57 INFO SecurityManager: Changing view acls to: root
25/04/09 10:45:57 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:45:57 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:45:57 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:45:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:45:57 INFO Utils: Successfully started service 'sparkDriver' on port 37757.
25/04/09 10:45:57 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:45:57 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:45:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:45:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:45:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:45:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aff9469d-9496-4d3f-9e6c-72e4856555cc
25/04/09 10:45:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:45:58 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:45:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:45:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:45:58 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:45:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409104558-0064
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104558-0064/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:45:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104558-0064/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104558-0064/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:45:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104558-0064/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104558-0064/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:45:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104558-0064/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:45:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33731.
25/04/09 10:45:58 INFO NettyBlockTransferService: Server created on 3fada93ce917:33731
25/04/09 10:45:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:45:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 33731, None)
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104558-0064/1 is now RUNNING
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104558-0064/0 is now RUNNING
25/04/09 10:45:58 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:33731 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 33731, None)
25/04/09 10:45:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104558-0064/2 is now RUNNING
25/04/09 10:45:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 33731, None)
25/04/09 10:45:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 33731, None)
25/04/09 10:45:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:45:58 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:45:58 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:46:00 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 10:46:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:55588) with ID 0,  ResourceProfileId 0
25/04/09 10:46:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:56244) with ID 2,  ResourceProfileId 0
25/04/09 10:46:00 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:56084) with ID 1,  ResourceProfileId 0
25/04/09 10:46:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:39595 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 39595, None)
25/04/09 10:46:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:36607 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 36607, None)
25/04/09 10:46:00 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:44291 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 44291, None)
25/04/09 10:46:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:46:00 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:46:00 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:46:00 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:46:00 INFO DAGScheduler: Missing parents: List()
25/04/09 10:46:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:46:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:46:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:46:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:33731 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:46:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:46:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:46:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:46:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:46:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:39595 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:46:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1168 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:46:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:46:01 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.264 s
25/04/09 10:46:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:46:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:46:01 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.298170 s
25/04/09 10:46:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:33731 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:46:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:39595 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:46:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:46:03 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:46:03 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:46:03 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:46:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:46:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:46:03 INFO metastore: Connected to metastore.
25/04/09 10:46:03 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:46:03 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=2f9feb8a-99c4-4321-aa22-9df296a9dbb6, clientType=HIVECLI]
25/04/09 10:46:03 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:46:03 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:46:03 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:46:03 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:46:03 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:46:03 INFO metastore: Connected to metastore.
25/04/09 10:46:04 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:46:04 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:46:04 INFO metastore: Connected to metastore.
25/04/09 10:46:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:46:04 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:46:04 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:46:04 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:46:04 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:46:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:46:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:46:04 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:46:04 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:46:04 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:46:04 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:46:04 INFO CodeGenerator: Code generated in 159.526458 ms
25/04/09 10:46:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:46:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:46:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:33731 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:46:04 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:46:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:46:04 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:46:04 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:46:04 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:46:04 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:46:04 INFO DAGScheduler: Missing parents: List()
25/04/09 10:46:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:46:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:46:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:46:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:33731 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:46:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:46:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:46:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:46:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:46:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:39595 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:46:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:39595 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:46:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1014 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:46:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:46:05 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.040 s
25/04/09 10:46:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:46:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:46:05 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.047625 s
25/04/09 10:46:05 INFO FileFormatWriter: Start to commit write Job 58d0445c-b4a8-4e8a-b807-d272a93facc4.
25/04/09 10:46:05 INFO FileFormatWriter: Write Job 58d0445c-b4a8-4e8a-b807-d272a93facc4 committed. Elapsed time: 40 ms.
25/04/09 10:46:05 INFO FileFormatWriter: Finished processing stats for write job 58d0445c-b4a8-4e8a-b807-d272a93facc4.
25/04/09 10:46:05 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:46:05 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:46:05 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:46:05 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:46:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:46:05 INFO MemoryStore: MemoryStore cleared
25/04/09 10:46:05 INFO BlockManager: BlockManager stopped
25/04/09 10:46:05 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:46:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:46:05 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:46:06 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:46:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c20a7d6f-093b-4f7c-b81b-a814e08fb8d7
25/04/09 10:46:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c20a7d6f-093b-4f7c-b81b-a814e08fb8d7/pyspark-a9faaa99-1c83-4296-bd8c-142b61386f17
25/04/09 10:46:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-65506853-7411-4b50-902c-ea912dcfbd36
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:49:23 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:49:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:49:23 INFO ResourceUtils: ==============================================================
25/04/09 10:49:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:49:23 INFO ResourceUtils: ==============================================================
25/04/09 10:49:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:49:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:49:23 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:49:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:49:23 INFO SecurityManager: Changing view acls to: root
25/04/09 10:49:23 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:49:23 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:49:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:49:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:49:23 INFO Utils: Successfully started service 'sparkDriver' on port 45289.
25/04/09 10:49:23 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:49:23 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:49:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:49:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:49:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:49:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-16ee562a-4056-4dd0-a443-307e08164430
25/04/09 10:49:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:49:23 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:49:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:49:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:49:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 10:49:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409104924-0066
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104924-0066/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:49:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104924-0066/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104924-0066/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:49:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104924-0066/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409104924-0066/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:49:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409104924-0066/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:49:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44047.
25/04/09 10:49:24 INFO NettyBlockTransferService: Server created on 3fada93ce917:44047
25/04/09 10:49:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:49:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 44047, None)
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104924-0066/0 is now RUNNING
25/04/09 10:49:24 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:44047 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 44047, None)
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104924-0066/2 is now RUNNING
25/04/09 10:49:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409104924-0066/1 is now RUNNING
25/04/09 10:49:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 44047, None)
25/04/09 10:49:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 44047, None)
25/04/09 10:49:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:49:24 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:49:24 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:49:26 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/09 10:49:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58336) with ID 1,  ResourceProfileId 0
25/04/09 10:49:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:38434) with ID 0,  ResourceProfileId 0
25/04/09 10:49:26 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:36120) with ID 2,  ResourceProfileId 0
25/04/09 10:49:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:39751 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 39751, None)
25/04/09 10:49:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:39723 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 39723, None)
25/04/09 10:49:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:45687 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 45687, None)
25/04/09 10:49:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:49:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:49:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:49:26 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:49:26 INFO DAGScheduler: Missing parents: List()
25/04/09 10:49:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:49:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:49:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:49:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:44047 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:49:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:49:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:49:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:49:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:49:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:39751 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:49:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1151 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 10:49:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:49:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.256 s
25/04/09 10:49:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:49:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:49:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.288864 s
25/04/09 10:49:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:44047 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:49:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:39751 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:49:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:49:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:49:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:49:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:49:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:49:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:49:29 INFO metastore: Connected to metastore.
25/04/09 10:49:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:49:29 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=5f5365c2-5b23-41c3-8752-62c7ccc41ec4, clientType=HIVECLI]
25/04/09 10:49:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:49:29 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:49:29 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:49:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:49:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:49:29 INFO metastore: Connected to metastore.
25/04/09 10:49:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:49:29 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:49:29 INFO metastore: Connected to metastore.
25/04/09 10:49:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:49:29 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:49:29 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:49:29 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:49:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:49:30 INFO CodeGenerator: Code generated in 144.124957 ms
25/04/09 10:49:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:49:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:49:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:44047 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:49:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:49:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:49:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:49:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:49:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:49:30 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:49:30 INFO DAGScheduler: Missing parents: List()
25/04/09 10:49:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:49:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:49:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:49:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:44047 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:49:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:49:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:49:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:49:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:49:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:45687 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:49:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:45687 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:49:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1959 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:49:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:49:32 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.986 s
25/04/09 10:49:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:49:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:49:32 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.994055 s
25/04/09 10:49:32 INFO FileFormatWriter: Start to commit write Job 344b238a-b29d-4f99-a5b3-d487df09590e.
25/04/09 10:49:32 INFO FileFormatWriter: Write Job 344b238a-b29d-4f99-a5b3-d487df09590e committed. Elapsed time: 36 ms.
25/04/09 10:49:32 INFO FileFormatWriter: Finished processing stats for write job 344b238a-b29d-4f99-a5b3-d487df09590e.
25/04/09 10:49:32 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:49:32 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:49:32 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:49:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:49:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:49:32 INFO MemoryStore: MemoryStore cleared
25/04/09 10:49:32 INFO BlockManager: BlockManager stopped
25/04/09 10:49:32 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:49:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:49:32 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:49:32 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:49:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-0917517a-92af-4017-987d-228341862204
25/04/09 10:49:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-590f8461-277b-43f2-b411-daa866953ccf
25/04/09 10:49:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-590f8461-277b-43f2-b411-daa866953ccf/pyspark-b6e7dcca-01ac-4c23-a8d2-accb530ee137
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:50:41 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:50:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:50:41 INFO ResourceUtils: ==============================================================
25/04/09 10:50:41 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:50:41 INFO ResourceUtils: ==============================================================
25/04/09 10:50:41 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:50:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:50:41 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:50:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:50:41 INFO SecurityManager: Changing view acls to: root
25/04/09 10:50:41 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:50:41 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:50:41 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:50:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:50:41 INFO Utils: Successfully started service 'sparkDriver' on port 46057.
25/04/09 10:50:41 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:50:41 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:50:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:50:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:50:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:50:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-56657995-1be1-45a3-af8f-08e376399942
25/04/09 10:50:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:50:41 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:50:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:50:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:50:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:50:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409105042-0068
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105042-0068/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:50:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105042-0068/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105042-0068/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:50:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105042-0068/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105042-0068/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:50:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105042-0068/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:50:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39095.
25/04/09 10:50:42 INFO NettyBlockTransferService: Server created on 3fada93ce917:39095
25/04/09 10:50:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:50:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39095, None)
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105042-0068/0 is now RUNNING
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105042-0068/1 is now RUNNING
25/04/09 10:50:42 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39095 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39095, None)
25/04/09 10:50:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105042-0068/2 is now RUNNING
25/04/09 10:50:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39095, None)
25/04/09 10:50:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39095, None)
25/04/09 10:50:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:50:42 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:50:42 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:50:43 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/09 10:50:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:43522) with ID 0,  ResourceProfileId 0
25/04/09 10:50:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:52592) with ID 2,  ResourceProfileId 0
25/04/09 10:50:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:39436) with ID 1,  ResourceProfileId 0
25/04/09 10:50:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:36481 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 36481, None)
25/04/09 10:50:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:42069 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 42069, None)
25/04/09 10:50:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40223 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40223, None)
25/04/09 10:50:44 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:50:44 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:50:44 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:50:44 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:50:44 INFO DAGScheduler: Missing parents: List()
25/04/09 10:50:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:50:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:50:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 10:50:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39095 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:50:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:50:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:50:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:50:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:50:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:36481 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:50:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1182 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:50:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:50:45 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.278 s
25/04/09 10:50:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:50:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:50:45 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.310732 s
25/04/09 10:50:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39095 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:50:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:36481 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 10:50:46 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:50:46 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:50:46 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:50:47 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:50:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:50:47 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:50:47 INFO metastore: Connected to metastore.
25/04/09 10:50:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:50:47 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4c826186-07b3-41b0-93ac-53095c426d99, clientType=HIVECLI]
25/04/09 10:50:47 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:50:47 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:50:47 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:50:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:50:47 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:50:47 INFO metastore: Connected to metastore.
25/04/09 10:50:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:50:47 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:50:47 INFO metastore: Connected to metastore.
25/04/09 10:50:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:50:47 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:50:48 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:50:48 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:50:48 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:50:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:50:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:50:48 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:50:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:50:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:50:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:50:48 INFO CodeGenerator: Code generated in 160.832223 ms
25/04/09 10:50:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:50:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:50:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39095 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:50:48 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:50:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:50:48 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:50:48 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:50:48 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:50:48 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:50:48 INFO DAGScheduler: Missing parents: List()
25/04/09 10:50:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:50:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:50:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:50:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39095 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:50:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:50:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:50:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:50:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:50:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:42069 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:50:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:42069 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:50:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2067 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:50:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:50:50 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.094 s
25/04/09 10:50:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:50:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:50:50 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.099981 s
25/04/09 10:50:50 INFO FileFormatWriter: Start to commit write Job 469a5e08-0749-49e8-b723-a92dd8b67c46.
25/04/09 10:50:50 INFO FileFormatWriter: Write Job 469a5e08-0749-49e8-b723-a92dd8b67c46 committed. Elapsed time: 41 ms.
25/04/09 10:50:50 INFO FileFormatWriter: Finished processing stats for write job 469a5e08-0749-49e8-b723-a92dd8b67c46.
25/04/09 10:50:50 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:50:50 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:50:50 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:50:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:50:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:50:50 INFO MemoryStore: MemoryStore cleared
25/04/09 10:50:50 INFO BlockManager: BlockManager stopped
25/04/09 10:50:50 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:50:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:50:50 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:50:51 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:50:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-4413bee0-534f-46ed-a351-51f865bcba44
25/04/09 10:50:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-750d91df-1f32-4959-b30a-3ef27e4ca709
25/04/09 10:50:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-4413bee0-534f-46ed-a351-51f865bcba44/pyspark-d96ae427-0382-4d51-af60-6e189a33c7ed
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:54:45 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:54:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:54:45 INFO ResourceUtils: ==============================================================
25/04/09 10:54:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:54:45 INFO ResourceUtils: ==============================================================
25/04/09 10:54:45 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:54:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:54:45 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:54:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:54:45 INFO SecurityManager: Changing view acls to: root
25/04/09 10:54:45 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:54:45 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:54:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:54:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:54:45 INFO Utils: Successfully started service 'sparkDriver' on port 33531.
25/04/09 10:54:45 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:54:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:54:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:54:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:54:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:54:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-027c5154-d1fc-4ea7-b208-dbffb8fdc6a3
25/04/09 10:54:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:54:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:54:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:54:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:54:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:54:46 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 10:54:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409105446-0070
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105446-0070/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:54:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105446-0070/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105446-0070/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:54:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105446-0070/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105446-0070/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:54:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105446-0070/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:54:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35239.
25/04/09 10:54:46 INFO NettyBlockTransferService: Server created on 3fada93ce917:35239
25/04/09 10:54:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:54:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 35239, None)
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105446-0070/0 is now RUNNING
25/04/09 10:54:46 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:35239 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 35239, None)
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105446-0070/1 is now RUNNING
25/04/09 10:54:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105446-0070/2 is now RUNNING
25/04/09 10:54:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 35239, None)
25/04/09 10:54:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 35239, None)
25/04/09 10:54:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:54:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:54:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:54:47 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
25/04/09 10:54:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:50166) with ID 1,  ResourceProfileId 0
25/04/09 10:54:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:39914) with ID 0,  ResourceProfileId 0
25/04/09 10:54:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:40840) with ID 2,  ResourceProfileId 0
25/04/09 10:54:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:44245 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 44245, None)
25/04/09 10:54:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:36191 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 36191, None)
25/04/09 10:54:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:36991 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 36991, None)
25/04/09 10:54:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:54:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:54:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:54:48 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:54:48 INFO DAGScheduler: Missing parents: List()
25/04/09 10:54:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:54:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:54:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:54:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:35239 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:54:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:54:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:54:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:54:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:54:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:36991 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:54:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1147 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:54:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:54:49 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.237 s
25/04/09 10:54:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:54:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:54:49 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.267449 s
25/04/09 10:54:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:35239 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:54:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:36991 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:54:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:54:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:54:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:54:50 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:54:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:54:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:54:50 INFO metastore: Connected to metastore.
25/04/09 10:54:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:54:51 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=e79578bd-bc9a-471d-9ff7-b69186ff9e32, clientType=HIVECLI]
25/04/09 10:54:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:54:51 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:54:51 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:54:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:54:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:54:51 INFO metastore: Connected to metastore.
25/04/09 10:54:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:54:51 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:54:51 INFO metastore: Connected to metastore.
25/04/09 10:54:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:54:51 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:54:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:54:51 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:54:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:54:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:54:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:54:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:54:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:54:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:54:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:54:51 INFO CodeGenerator: Code generated in 150.209713 ms
25/04/09 10:54:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:54:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:54:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:35239 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:54:51 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:54:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:54:52 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:54:52 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:54:52 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:54:52 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:54:52 INFO DAGScheduler: Missing parents: List()
25/04/09 10:54:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:54:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:54:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:54:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:35239 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:54:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:54:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:54:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:54:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:54:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:44245 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:54:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:44245 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:54:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2038 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:54:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:54:54 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.064 s
25/04/09 10:54:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:54:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:54:54 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.071926 s
25/04/09 10:54:54 INFO FileFormatWriter: Start to commit write Job e45e13b3-2b47-42ef-ad4d-60ecbf9b9988.
25/04/09 10:54:54 INFO FileFormatWriter: Write Job e45e13b3-2b47-42ef-ad4d-60ecbf9b9988 committed. Elapsed time: 41 ms.
25/04/09 10:54:54 INFO FileFormatWriter: Finished processing stats for write job e45e13b3-2b47-42ef-ad4d-60ecbf9b9988.
25/04/09 10:54:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:54:54 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:54:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:54:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:54:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:54:54 INFO MemoryStore: MemoryStore cleared
25/04/09 10:54:54 INFO BlockManager: BlockManager stopped
25/04/09 10:54:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:54:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:54:54 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:54:54 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:54:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce3eb773-b0d4-404e-b7e4-fa18890984b0/pyspark-2eb7e0ee-db5e-458d-b9f7-5d3d280720e4
25/04/09 10:54:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-b705ee48-e502-4d57-b945-d6f3579cae9b
25/04/09 10:54:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce3eb773-b0d4-404e-b7e4-fa18890984b0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 10:56:00 INFO SparkContext: Running Spark version 3.2.2
25/04/09 10:56:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 10:56:00 INFO ResourceUtils: ==============================================================
25/04/09 10:56:00 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 10:56:00 INFO ResourceUtils: ==============================================================
25/04/09 10:56:00 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 10:56:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 10:56:00 INFO ResourceProfile: Limiting resource is cpu
25/04/09 10:56:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 10:56:00 INFO SecurityManager: Changing view acls to: root
25/04/09 10:56:00 INFO SecurityManager: Changing modify acls to: root
25/04/09 10:56:00 INFO SecurityManager: Changing view acls groups to: 
25/04/09 10:56:00 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 10:56:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 10:56:00 INFO Utils: Successfully started service 'sparkDriver' on port 45977.
25/04/09 10:56:00 INFO SparkEnv: Registering MapOutputTracker
25/04/09 10:56:01 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 10:56:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 10:56:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 10:56:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 10:56:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b5c83421-22e3-4a22-867a-1c89df29c978
25/04/09 10:56:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 10:56:01 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 10:56:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 10:56:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 10:56:01 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 10:56:01 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409105601-0072
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105601-0072/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 10:56:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105601-0072/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105601-0072/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 10:56:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105601-0072/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409105601-0072/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 10:56:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409105601-0072/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 10:56:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36785.
25/04/09 10:56:01 INFO NettyBlockTransferService: Server created on 3fada93ce917:36785
25/04/09 10:56:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 10:56:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 36785, None)
25/04/09 10:56:01 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:36785 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 36785, None)
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105601-0072/1 is now RUNNING
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105601-0072/2 is now RUNNING
25/04/09 10:56:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 36785, None)
25/04/09 10:56:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409105601-0072/0 is now RUNNING
25/04/09 10:56:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 36785, None)
25/04/09 10:56:01 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 10:56:01 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 10:56:01 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 10:56:03 INFO InMemoryFileIndex: It took 68 ms to list leaf files for 1 paths.
25/04/09 10:56:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:53346) with ID 0,  ResourceProfileId 0
25/04/09 10:56:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:42072) with ID 2,  ResourceProfileId 0
25/04/09 10:56:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:47670) with ID 1,  ResourceProfileId 0
25/04/09 10:56:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:40425 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 40425, None)
25/04/09 10:56:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:39115 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 39115, None)
25/04/09 10:56:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:44583 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 44583, None)
25/04/09 10:56:03 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 10:56:03 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:56:03 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 10:56:03 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:56:03 INFO DAGScheduler: Missing parents: List()
25/04/09 10:56:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:56:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 10:56:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 10:56:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:36785 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:56:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 10:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:56:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 10:56:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 10:56:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:40425 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:56:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1179 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 10:56:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 10:56:04 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.273 s
25/04/09 10:56:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:56:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 10:56:04 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.302984 s
25/04/09 10:56:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:36785 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:56:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:40425 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 10:56:06 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:56:06 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 10:56:06 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 10:56:06 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 10:56:06 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:56:06 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:56:06 INFO metastore: Connected to metastore.
25/04/09 10:56:06 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 10:56:06 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=89fc95b6-6ad9-4123-b5be-c1633ea91c45, clientType=HIVECLI]
25/04/09 10:56:06 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 10:56:06 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 10:56:06 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 10:56:06 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:56:06 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 10:56:06 INFO metastore: Connected to metastore.
25/04/09 10:56:06 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 10:56:06 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 10:56:06 INFO metastore: Connected to metastore.
25/04/09 10:56:07 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 10:56:07 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 10:56:07 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 10:56:07 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 10:56:07 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:56:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:56:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:56:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:56:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 10:56:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 10:56:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 10:56:07 INFO CodeGenerator: Code generated in 156.979335 ms
25/04/09 10:56:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 10:56:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 10:56:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:36785 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 10:56:07 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:56:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 10:56:07 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 10:56:07 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 10:56:07 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 10:56:07 INFO DAGScheduler: Parents of final stage: List()
25/04/09 10:56:07 INFO DAGScheduler: Missing parents: List()
25/04/09 10:56:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 10:56:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 10:56:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 10:56:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:36785 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:56:07 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 10:56:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 10:56:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 10:56:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.12, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 10:56:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.12:39115 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 10:56:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.12:39115 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 10:56:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2117 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 10:56:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 10:56:09 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.146 s
25/04/09 10:56:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 10:56:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 10:56:09 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.153029 s
25/04/09 10:56:09 INFO FileFormatWriter: Start to commit write Job 34c749d2-3097-4fca-ae3e-a012d53d10d6.
25/04/09 10:56:09 INFO FileFormatWriter: Write Job 34c749d2-3097-4fca-ae3e-a012d53d10d6 committed. Elapsed time: 40 ms.
25/04/09 10:56:09 INFO FileFormatWriter: Finished processing stats for write job 34c749d2-3097-4fca-ae3e-a012d53d10d6.
25/04/09 10:56:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 10:56:09 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 10:56:09 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 10:56:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 10:56:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 10:56:09 INFO MemoryStore: MemoryStore cleared
25/04/09 10:56:09 INFO BlockManager: BlockManager stopped
25/04/09 10:56:09 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 10:56:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 10:56:09 INFO SparkContext: Successfully stopped SparkContext
25/04/09 10:56:10 INFO ShutdownHookManager: Shutdown hook called
25/04/09 10:56:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc905c22-6195-4ebd-bf98-de77e4b67ab6
25/04/09 10:56:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc905c22-6195-4ebd-bf98-de77e4b67ab6/pyspark-64ecc135-fbc1-4521-abc4-2d8fadc44c39
25/04/09 10:56:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-29a5a177-f4a2-4d27-b6e0-01ca3adf7912
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 11:05:08 INFO SparkContext: Running Spark version 3.2.2
25/04/09 11:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 11:05:08 INFO ResourceUtils: ==============================================================
25/04/09 11:05:08 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 11:05:08 INFO ResourceUtils: ==============================================================
25/04/09 11:05:08 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 11:05:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 11:05:08 INFO ResourceProfile: Limiting resource is cpu
25/04/09 11:05:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 11:05:08 INFO SecurityManager: Changing view acls to: root
25/04/09 11:05:08 INFO SecurityManager: Changing modify acls to: root
25/04/09 11:05:08 INFO SecurityManager: Changing view acls groups to: 
25/04/09 11:05:08 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 11:05:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 11:05:08 INFO Utils: Successfully started service 'sparkDriver' on port 37115.
25/04/09 11:05:08 INFO SparkEnv: Registering MapOutputTracker
25/04/09 11:05:08 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 11:05:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 11:05:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 11:05:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 11:05:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-11b98ac3-e13b-4a73-bee5-27812418cb31
25/04/09 11:05:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 11:05:08 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 11:05:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 11:05:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 11:05:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 11:05:09 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409110509-0076
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110509-0076/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 11:05:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110509-0076/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110509-0076/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 11:05:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110509-0076/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110509-0076/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 11:05:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110509-0076/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:05:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45799.
25/04/09 11:05:09 INFO NettyBlockTransferService: Server created on 3fada93ce917:45799
25/04/09 11:05:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 11:05:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 45799, None)
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110509-0076/0 is now RUNNING
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110509-0076/2 is now RUNNING
25/04/09 11:05:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110509-0076/1 is now RUNNING
25/04/09 11:05:09 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:45799 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 45799, None)
25/04/09 11:05:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 45799, None)
25/04/09 11:05:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 45799, None)
25/04/09 11:05:09 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 11:05:09 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 11:05:09 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 11:05:10 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/09 11:05:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:34892) with ID 0,  ResourceProfileId 0
25/04/09 11:05:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:48474) with ID 1,  ResourceProfileId 0
25/04/09 11:05:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:52174) with ID 2,  ResourceProfileId 0
25/04/09 11:05:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41117 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 41117, None)
25/04/09 11:05:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:41735 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 41735, None)
25/04/09 11:05:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:41735 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 41735, None)
25/04/09 11:05:11 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 11:05:11 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:05:11 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 11:05:11 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:05:11 INFO DAGScheduler: Missing parents: List()
25/04/09 11:05:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:05:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 11:05:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 11:05:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:45799 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:05:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 11:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:05:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 11:05:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 11:05:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:41117 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:05:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1201 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 11:05:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 11:05:12 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.294 s
25/04/09 11:05:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:05:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 11:05:12 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.325274 s
25/04/09 11:05:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:45799 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:05:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:41117 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:05:13 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:05:13 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 11:05:13 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:05:14 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 11:05:14 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:05:14 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:05:14 INFO metastore: Connected to metastore.
25/04/09 11:05:14 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 11:05:14 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f39f1273-03ed-4612-8ddc-4ed297c08d52, clientType=HIVECLI]
25/04/09 11:05:14 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 11:05:14 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 11:05:14 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 11:05:14 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:05:14 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:05:14 INFO metastore: Connected to metastore.
25/04/09 11:05:14 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:05:14 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 11:05:14 INFO metastore: Connected to metastore.
25/04/09 11:05:14 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 11:05:14 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 11:05:14 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 11:05:14 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 11:05:15 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:05:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:05:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:05:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:05:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:05:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:05:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:05:15 INFO CodeGenerator: Code generated in 154.902897 ms
25/04/09 11:05:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 11:05:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 11:05:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:45799 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 11:05:15 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:05:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 11:05:15 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:05:15 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:05:15 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 11:05:15 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:05:15 INFO DAGScheduler: Missing parents: List()
25/04/09 11:05:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:05:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 11:05:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 11:05:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:45799 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:05:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 11:05:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:05:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 11:05:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 11:05:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:41117 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:05:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:41117 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 11:05:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1040 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 11:05:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 11:05:16 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.067 s
25/04/09 11:05:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:05:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 11:05:16 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.074968 s
25/04/09 11:05:16 INFO FileFormatWriter: Start to commit write Job b9b08f05-b42a-438a-83ba-f4fd3d9e3260.
25/04/09 11:05:16 INFO FileFormatWriter: Write Job b9b08f05-b42a-438a-83ba-f4fd3d9e3260 committed. Elapsed time: 36 ms.
25/04/09 11:05:16 INFO FileFormatWriter: Finished processing stats for write job b9b08f05-b42a-438a-83ba-f4fd3d9e3260.
25/04/09 11:05:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 11:05:16 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 11:05:16 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 11:05:16 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 11:05:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 11:05:16 INFO MemoryStore: MemoryStore cleared
25/04/09 11:05:16 INFO BlockManager: BlockManager stopped
25/04/09 11:05:16 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 11:05:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 11:05:16 INFO SparkContext: Successfully stopped SparkContext
25/04/09 11:05:16 INFO ShutdownHookManager: Shutdown hook called
25/04/09 11:05:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b8aaa662-0478-4948-972a-d27d0488dde2
25/04/09 11:05:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-06aa6ece-fce0-40f5-85ea-804094ac434b/pyspark-349ed8a4-4887-45fe-aded-8de4b580672d
25/04/09 11:05:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-06aa6ece-fce0-40f5-85ea-804094ac434b
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 11:06:41 INFO SparkContext: Running Spark version 3.2.2
25/04/09 11:06:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 11:06:41 INFO ResourceUtils: ==============================================================
25/04/09 11:06:41 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 11:06:41 INFO ResourceUtils: ==============================================================
25/04/09 11:06:41 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 11:06:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 11:06:41 INFO ResourceProfile: Limiting resource is cpu
25/04/09 11:06:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 11:06:41 INFO SecurityManager: Changing view acls to: root
25/04/09 11:06:41 INFO SecurityManager: Changing modify acls to: root
25/04/09 11:06:41 INFO SecurityManager: Changing view acls groups to: 
25/04/09 11:06:41 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 11:06:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 11:06:41 INFO Utils: Successfully started service 'sparkDriver' on port 36849.
25/04/09 11:06:41 INFO SparkEnv: Registering MapOutputTracker
25/04/09 11:06:41 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 11:06:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 11:06:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 11:06:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 11:06:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bfec8e1c-bb75-465e-9e43-f43288549154
25/04/09 11:06:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 11:06:41 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 11:06:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 11:06:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 11:06:42 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 11:06:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409110642-0078
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110642-0078/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 11:06:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110642-0078/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110642-0078/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 11:06:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110642-0078/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409110642-0078/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 11:06:42 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409110642-0078/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:06:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39217.
25/04/09 11:06:42 INFO NettyBlockTransferService: Server created on 3fada93ce917:39217
25/04/09 11:06:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 11:06:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39217, None)
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110642-0078/2 is now RUNNING
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110642-0078/0 is now RUNNING
25/04/09 11:06:42 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409110642-0078/1 is now RUNNING
25/04/09 11:06:42 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39217 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39217, None)
25/04/09 11:06:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39217, None)
25/04/09 11:06:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39217, None)
25/04/09 11:06:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 11:06:42 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 11:06:42 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 11:06:43 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/09 11:06:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:59194) with ID 0,  ResourceProfileId 0
25/04/09 11:06:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58656) with ID 1,  ResourceProfileId 0
25/04/09 11:06:44 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:36784) with ID 2,  ResourceProfileId 0
25/04/09 11:06:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:36459 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 36459, None)
25/04/09 11:06:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33653 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33653, None)
25/04/09 11:06:44 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:45631 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 45631, None)
25/04/09 11:06:44 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 11:06:44 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:06:44 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 11:06:44 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:06:44 INFO DAGScheduler: Missing parents: List()
25/04/09 11:06:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:06:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 11:06:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 11:06:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39217 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 11:06:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 11:06:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:06:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 11:06:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 11:06:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:33653 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 11:06:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1143 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 11:06:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 11:06:45 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.235 s
25/04/09 11:06:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:06:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 11:06:45 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.266632 s
25/04/09 11:06:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39217 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 11:06:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:33653 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 11:06:46 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:06:46 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 11:06:46 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:06:46 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 11:06:46 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:06:46 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:06:46 INFO metastore: Connected to metastore.
25/04/09 11:06:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 11:06:47 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=eab70810-d4e9-4e42-87e7-6e1274ec5cad, clientType=HIVECLI]
25/04/09 11:06:47 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 11:06:47 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 11:06:47 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 11:06:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:06:47 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:06:47 INFO metastore: Connected to metastore.
25/04/09 11:06:47 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:06:47 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 11:06:47 INFO metastore: Connected to metastore.
25/04/09 11:06:47 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 11:06:47 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 11:06:47 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 11:06:47 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 11:06:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:06:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:06:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:06:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:06:48 INFO CodeGenerator: Code generated in 143.767493 ms
25/04/09 11:06:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 11:06:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 11:06:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39217 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 11:06:48 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:06:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 11:06:48 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:06:48 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:06:48 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 11:06:48 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:06:48 INFO DAGScheduler: Missing parents: List()
25/04/09 11:06:48 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:06:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 11:06:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 11:06:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39217 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:06:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 11:06:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:06:48 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 11:06:48 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 11:06:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:33653 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:06:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:33653 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 11:06:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 968 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 11:06:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 11:06:49 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.994 s
25/04/09 11:06:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:06:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 11:06:49 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.001583 s
25/04/09 11:06:49 INFO FileFormatWriter: Start to commit write Job 910239a3-55ce-47ee-b3db-97d3ffc9e16c.
25/04/09 11:06:49 INFO FileFormatWriter: Write Job 910239a3-55ce-47ee-b3db-97d3ffc9e16c committed. Elapsed time: 34 ms.
25/04/09 11:06:49 INFO FileFormatWriter: Finished processing stats for write job 910239a3-55ce-47ee-b3db-97d3ffc9e16c.
25/04/09 11:06:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 11:06:49 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 11:06:49 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 11:06:49 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 11:06:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 11:06:49 INFO MemoryStore: MemoryStore cleared
25/04/09 11:06:49 INFO BlockManager: BlockManager stopped
25/04/09 11:06:49 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 11:06:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 11:06:49 INFO SparkContext: Successfully stopped SparkContext
25/04/09 11:06:49 INFO ShutdownHookManager: Shutdown hook called
25/04/09 11:06:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-cfe0e023-22fa-4bd9-b1e2-9594b3e69d63/pyspark-efa095e4-5426-4059-942d-e4234131867b
25/04/09 11:06:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-cfe0e023-22fa-4bd9-b1e2-9594b3e69d63
25/04/09 11:06:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-4478721d-cdbb-43a6-aa67-834a33fee602
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 11:23:30 INFO SparkContext: Running Spark version 3.2.2
25/04/09 11:23:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 11:23:30 INFO ResourceUtils: ==============================================================
25/04/09 11:23:30 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 11:23:30 INFO ResourceUtils: ==============================================================
25/04/09 11:23:30 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 11:23:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 11:23:30 INFO ResourceProfile: Limiting resource is cpu
25/04/09 11:23:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 11:23:30 INFO SecurityManager: Changing view acls to: root
25/04/09 11:23:30 INFO SecurityManager: Changing modify acls to: root
25/04/09 11:23:30 INFO SecurityManager: Changing view acls groups to: 
25/04/09 11:23:30 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 11:23:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 11:23:30 INFO Utils: Successfully started service 'sparkDriver' on port 40423.
25/04/09 11:23:30 INFO SparkEnv: Registering MapOutputTracker
25/04/09 11:23:30 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 11:23:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 11:23:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 11:23:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 11:23:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5ec8b395-1676-417a-b905-a37089ee4ca9
25/04/09 11:23:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 11:23:30 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 11:23:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 11:23:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 11:23:31 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 11:23:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409112331-0081
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409112331-0081/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 11:23:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409112331-0081/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409112331-0081/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 11:23:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409112331-0081/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409112331-0081/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 11:23:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409112331-0081/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 11:23:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39303.
25/04/09 11:23:31 INFO NettyBlockTransferService: Server created on 3fada93ce917:39303
25/04/09 11:23:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409112331-0081/0 is now RUNNING
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409112331-0081/2 is now RUNNING
25/04/09 11:23:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 39303, None)
25/04/09 11:23:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409112331-0081/1 is now RUNNING
25/04/09 11:23:31 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:39303 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 39303, None)
25/04/09 11:23:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 39303, None)
25/04/09 11:23:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 39303, None)
25/04/09 11:23:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 11:23:31 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 11:23:31 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 11:23:33 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/09 11:23:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:39140) with ID 2,  ResourceProfileId 0
25/04/09 11:23:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:34446) with ID 0,  ResourceProfileId 0
25/04/09 11:23:33 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58228) with ID 1,  ResourceProfileId 0
25/04/09 11:23:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:39513 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 39513, None)
25/04/09 11:23:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:42645 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 42645, None)
25/04/09 11:23:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41701 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 41701, None)
25/04/09 11:23:33 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 11:23:33 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:23:33 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 11:23:33 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:23:33 INFO DAGScheduler: Missing parents: List()
25/04/09 11:23:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:23:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 11:23:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 11:23:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:39303 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:23:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 11:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:23:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 11:23:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.12, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 11:23:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.12:42645 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:23:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1224 ms on 172.18.0.12 (executor 2) (1/1)
25/04/09 11:23:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 11:23:34 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.326 s
25/04/09 11:23:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:23:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 11:23:34 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.361692 s
25/04/09 11:23:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:39303 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:23:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.12:42645 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 11:23:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:23:36 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 11:23:36 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 11:23:36 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 11:23:36 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:23:36 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:23:36 INFO metastore: Connected to metastore.
25/04/09 11:23:36 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 11:23:37 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=293f5179-e5c1-4ac9-9ba7-f8cba61b85c7, clientType=HIVECLI]
25/04/09 11:23:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 11:23:37 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 11:23:37 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 11:23:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:23:37 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 11:23:37 INFO metastore: Connected to metastore.
25/04/09 11:23:37 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 11:23:37 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 11:23:37 INFO metastore: Connected to metastore.
25/04/09 11:23:37 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 11:23:37 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 11:23:37 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 11:23:37 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 11:23:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:23:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 11:23:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 11:23:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 11:23:37 INFO CodeGenerator: Code generated in 155.164928 ms
25/04/09 11:23:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 11:23:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 11:23:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:39303 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 11:23:37 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:23:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 11:23:37 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 11:23:37 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 11:23:37 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 11:23:37 INFO DAGScheduler: Parents of final stage: List()
25/04/09 11:23:37 INFO DAGScheduler: Missing parents: List()
25/04/09 11:23:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 11:23:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 11:23:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 11:23:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:39303 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:23:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 11:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 11:23:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 11:23:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 11:23:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:39513 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 11:23:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:39513 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 11:23:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2056 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 11:23:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 11:23:39 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.084 s
25/04/09 11:23:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 11:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 11:23:39 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.090951 s
25/04/09 11:23:39 INFO FileFormatWriter: Start to commit write Job c2068016-cccd-4878-a45e-917bc1482f56.
25/04/09 11:23:39 INFO FileFormatWriter: Write Job c2068016-cccd-4878-a45e-917bc1482f56 committed. Elapsed time: 41 ms.
25/04/09 11:23:39 INFO FileFormatWriter: Finished processing stats for write job c2068016-cccd-4878-a45e-917bc1482f56.
25/04/09 11:23:39 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 11:23:39 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 11:23:39 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 11:23:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 11:23:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 11:23:39 INFO MemoryStore: MemoryStore cleared
25/04/09 11:23:39 INFO BlockManager: BlockManager stopped
25/04/09 11:23:39 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 11:23:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 11:23:39 INFO SparkContext: Successfully stopped SparkContext
25/04/09 11:23:40 INFO ShutdownHookManager: Shutdown hook called
25/04/09 11:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-085a2fa0-d78e-41bd-b395-624a310822bb/pyspark-6c5c9403-50ef-4688-a69f-0ac5fed7fdc9
25/04/09 11:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-085a2fa0-d78e-41bd-b395-624a310822bb
25/04/09 11:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7fda7ac-ae54-4647-969e-d1fc81d26e57
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 12:00:54 INFO SparkContext: Running Spark version 3.2.2
25/04/09 12:00:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 12:00:54 INFO ResourceUtils: ==============================================================
25/04/09 12:00:54 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 12:00:54 INFO ResourceUtils: ==============================================================
25/04/09 12:00:54 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 12:00:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 12:00:54 INFO ResourceProfile: Limiting resource is cpu
25/04/09 12:00:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 12:00:54 INFO SecurityManager: Changing view acls to: root
25/04/09 12:00:54 INFO SecurityManager: Changing modify acls to: root
25/04/09 12:00:54 INFO SecurityManager: Changing view acls groups to: 
25/04/09 12:00:54 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 12:00:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 12:00:54 INFO Utils: Successfully started service 'sparkDriver' on port 40711.
25/04/09 12:00:54 INFO SparkEnv: Registering MapOutputTracker
25/04/09 12:00:54 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 12:00:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 12:00:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 12:00:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 12:00:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1a8d87b4-aea3-4f24-b11e-bed64d230bb8
25/04/09 12:00:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 12:00:54 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 12:00:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 12:00:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 12:00:55 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 12:00:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409120055-0088
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120055-0088/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 12:00:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120055-0088/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120055-0088/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 12:00:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120055-0088/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120055-0088/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 12:00:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120055-0088/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:00:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43911.
25/04/09 12:00:55 INFO NettyBlockTransferService: Server created on 3fada93ce917:43911
25/04/09 12:00:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 12:00:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 43911, None)
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120055-0088/2 is now RUNNING
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120055-0088/0 is now RUNNING
25/04/09 12:00:55 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:43911 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 43911, None)
25/04/09 12:00:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120055-0088/1 is now RUNNING
25/04/09 12:00:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 43911, None)
25/04/09 12:00:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 43911, None)
25/04/09 12:00:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 12:00:55 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 12:00:55 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 12:00:56 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/09 12:00:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:53500) with ID 2,  ResourceProfileId 0
25/04/09 12:00:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:44736) with ID 0,  ResourceProfileId 0
25/04/09 12:00:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:49002) with ID 1,  ResourceProfileId 0
25/04/09 12:00:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:45857 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 45857, None)
25/04/09 12:00:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33721 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 33721, None)
25/04/09 12:00:57 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:33807 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 33807, None)
25/04/09 12:00:57 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 12:00:57 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:00:57 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 12:00:57 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:00:57 INFO DAGScheduler: Missing parents: List()
25/04/09 12:00:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:00:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 12:00:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 12:00:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:43911 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:00:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 12:00:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:00:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 12:00:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 12:00:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.3:33807 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:00:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1150 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 12:00:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 12:00:58 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.248 s
25/04/09 12:00:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:00:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 12:00:58 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.278106 s
25/04/09 12:00:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:43911 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:00:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.3:33807 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:00:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:00:59 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 12:00:59 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:00:59 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 12:00:59 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:00:59 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:00:59 INFO metastore: Connected to metastore.
25/04/09 12:01:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:01:00 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=59b59605-91fa-4941-88d4-7344c5adb05b, clientType=HIVECLI]
25/04/09 12:01:00 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 12:01:00 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 12:01:00 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 12:01:00 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:01:00 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:01:00 INFO metastore: Connected to metastore.
25/04/09 12:01:00 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:01:00 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 12:01:00 INFO metastore: Connected to metastore.
25/04/09 12:01:00 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 12:01:00 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 12:01:00 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 12:01:00 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 12:01:00 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:01:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:01:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:01:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:01:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:01:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:01:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:01:00 INFO CodeGenerator: Code generated in 145.689018 ms
25/04/09 12:01:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 12:01:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 12:01:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:43911 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 12:01:00 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:01:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 12:01:00 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:01:00 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:01:00 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 12:01:00 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:01:00 INFO DAGScheduler: Missing parents: List()
25/04/09 12:01:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:01:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 12:01:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 12:01:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:43911 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:01:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 12:01:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:01:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 12:01:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 12:01:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:33721 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:01:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:33721 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 12:01:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1980 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 12:01:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 12:01:03 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.007 s
25/04/09 12:01:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:01:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 12:01:03 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.015664 s
25/04/09 12:01:03 INFO FileFormatWriter: Start to commit write Job 66b91d99-ab90-4318-b3ca-345dc5b24669.
25/04/09 12:01:03 INFO FileFormatWriter: Write Job 66b91d99-ab90-4318-b3ca-345dc5b24669 committed. Elapsed time: 35 ms.
25/04/09 12:01:03 INFO FileFormatWriter: Finished processing stats for write job 66b91d99-ab90-4318-b3ca-345dc5b24669.
25/04/09 12:01:03 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:01:03 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 12:01:03 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 12:01:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 12:01:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 12:01:03 INFO MemoryStore: MemoryStore cleared
25/04/09 12:01:03 INFO BlockManager: BlockManager stopped
25/04/09 12:01:03 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 12:01:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 12:01:03 INFO SparkContext: Successfully stopped SparkContext
25/04/09 12:01:03 INFO ShutdownHookManager: Shutdown hook called
25/04/09 12:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-89db6175-6bbc-4da2-9e1c-826741635f25/pyspark-4dc4b79c-095f-40bd-a00f-531e5d7e172b
25/04/09 12:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e333e99-66ed-4375-b74e-82eb0bcbf3d5
25/04/09 12:01:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-89db6175-6bbc-4da2-9e1c-826741635f25
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 12:04:49 INFO SparkContext: Running Spark version 3.2.2
25/04/09 12:04:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 12:04:49 INFO ResourceUtils: ==============================================================
25/04/09 12:04:49 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 12:04:49 INFO ResourceUtils: ==============================================================
25/04/09 12:04:49 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 12:04:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 12:04:49 INFO ResourceProfile: Limiting resource is cpu
25/04/09 12:04:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 12:04:49 INFO SecurityManager: Changing view acls to: root
25/04/09 12:04:49 INFO SecurityManager: Changing modify acls to: root
25/04/09 12:04:49 INFO SecurityManager: Changing view acls groups to: 
25/04/09 12:04:49 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 12:04:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 12:04:49 INFO Utils: Successfully started service 'sparkDriver' on port 39069.
25/04/09 12:04:49 INFO SparkEnv: Registering MapOutputTracker
25/04/09 12:04:49 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 12:04:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 12:04:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 12:04:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 12:04:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6439a8b8-7c8c-47a1-adbf-41bd8405fc81
25/04/09 12:04:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 12:04:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 12:04:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 12:04:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 12:04:49 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 12:04:49 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 22 ms (0 ms spent in bootstraps)
25/04/09 12:04:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409120450-0090
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120450-0090/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 12:04:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120450-0090/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120450-0090/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 12:04:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120450-0090/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409120450-0090/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 12:04:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409120450-0090/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:04:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37419.
25/04/09 12:04:50 INFO NettyBlockTransferService: Server created on 3fada93ce917:37419
25/04/09 12:04:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 12:04:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 37419, None)
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120450-0090/2 is now RUNNING
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120450-0090/1 is now RUNNING
25/04/09 12:04:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409120450-0090/0 is now RUNNING
25/04/09 12:04:50 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:37419 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 37419, None)
25/04/09 12:04:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 37419, None)
25/04/09 12:04:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 37419, None)
25/04/09 12:04:50 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 12:04:50 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 12:04:50 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 12:04:51 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 12:04:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:33310) with ID 2,  ResourceProfileId 0
25/04/09 12:04:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:44536) with ID 1,  ResourceProfileId 0
25/04/09 12:04:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:33592) with ID 0,  ResourceProfileId 0
25/04/09 12:04:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:37485 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 37485, None)
25/04/09 12:04:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:44971 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 44971, None)
25/04/09 12:04:52 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:45777 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 45777, None)
25/04/09 12:04:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 12:04:52 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:04:52 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 12:04:52 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:04:52 INFO DAGScheduler: Missing parents: List()
25/04/09 12:04:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:04:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 12:04:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 12:04:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:37419 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:04:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 12:04:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:04:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 12:04:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 12:04:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:45777 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:04:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1190 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 12:04:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 12:04:53 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.283 s
25/04/09 12:04:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:04:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 12:04:53 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.315760 s
25/04/09 12:04:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:37419 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:04:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:45777 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:04:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:04:54 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 12:04:54 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:04:54 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 12:04:54 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:04:54 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:04:54 INFO metastore: Connected to metastore.
25/04/09 12:04:55 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/09 12:04:55 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=282f51f7-2a94-4ba5-9d65-d36d0786e551, clientType=HIVECLI]
25/04/09 12:04:55 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 12:04:55 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 12:04:55 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 12:04:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:04:55 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:04:55 INFO metastore: Connected to metastore.
25/04/09 12:04:55 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:04:55 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 12:04:55 INFO metastore: Connected to metastore.
25/04/09 12:04:55 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 12:04:55 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 12:04:55 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 12:04:55 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 12:04:55 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:04:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:04:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:04:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:04:56 INFO CodeGenerator: Code generated in 159.844347 ms
25/04/09 12:04:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 12:04:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 12:04:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:37419 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 12:04:56 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:04:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 12:04:56 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:04:56 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:04:56 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 12:04:56 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:04:56 INFO DAGScheduler: Missing parents: List()
25/04/09 12:04:56 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:04:56 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 12:04:56 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 12:04:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:37419 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:04:56 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 12:04:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:04:56 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 12:04:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 12:04:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.3:37485 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:04:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.3:37485 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 12:04:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2062 ms on 172.18.0.3 (executor 1) (1/1)
25/04/09 12:04:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 12:04:58 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.090 s
25/04/09 12:04:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:04:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 12:04:58 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.097461 s
25/04/09 12:04:58 INFO FileFormatWriter: Start to commit write Job 4fe566fa-d275-4662-ad10-935f13d6994b.
25/04/09 12:04:58 INFO FileFormatWriter: Write Job 4fe566fa-d275-4662-ad10-935f13d6994b committed. Elapsed time: 37 ms.
25/04/09 12:04:58 INFO FileFormatWriter: Finished processing stats for write job 4fe566fa-d275-4662-ad10-935f13d6994b.
25/04/09 12:04:58 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:04:58 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 12:04:58 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 12:04:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 12:04:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 12:04:58 INFO MemoryStore: MemoryStore cleared
25/04/09 12:04:58 INFO BlockManager: BlockManager stopped
25/04/09 12:04:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 12:04:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 12:04:58 INFO SparkContext: Successfully stopped SparkContext
25/04/09 12:04:58 INFO ShutdownHookManager: Shutdown hook called
25/04/09 12:04:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6ba3d8d-6337-466f-a131-e8de280341db/pyspark-592fae7e-7135-420c-ba08-17f8b8251bb2
25/04/09 12:04:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-64607059-8f24-43c8-95f2-1394c2d30301
25/04/09 12:04:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6ba3d8d-6337-466f-a131-e8de280341db
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 12:11:23 INFO SparkContext: Running Spark version 3.2.2
25/04/09 12:11:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 12:11:23 INFO ResourceUtils: ==============================================================
25/04/09 12:11:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 12:11:23 INFO ResourceUtils: ==============================================================
25/04/09 12:11:23 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 12:11:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 12:11:23 INFO ResourceProfile: Limiting resource is cpu
25/04/09 12:11:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 12:11:23 INFO SecurityManager: Changing view acls to: root
25/04/09 12:11:23 INFO SecurityManager: Changing modify acls to: root
25/04/09 12:11:23 INFO SecurityManager: Changing view acls groups to: 
25/04/09 12:11:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 12:11:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 12:11:23 INFO Utils: Successfully started service 'sparkDriver' on port 40647.
25/04/09 12:11:23 INFO SparkEnv: Registering MapOutputTracker
25/04/09 12:11:23 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 12:11:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 12:11:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 12:11:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 12:11:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-925f7e31-d4d1-4a8c-8c45-6d6ea2f4dd9a
25/04/09 12:11:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 12:11:23 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 12:11:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 12:11:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://3fada93ce917:4040
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 12:11:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 23 ms (0 ms spent in bootstraps)
25/04/09 12:11:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409121124-0093
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409121124-0093/0 on worker-20250408075749-172.18.0.9-36639 (172.18.0.9:36639) with 4 core(s)
25/04/09 12:11:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409121124-0093/0 on hostPort 172.18.0.9:36639 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409121124-0093/1 on worker-20250408075749-172.18.0.3-40933 (172.18.0.3:40933) with 4 core(s)
25/04/09 12:11:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409121124-0093/1 on hostPort 172.18.0.3:40933 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409121124-0093/2 on worker-20250408075749-172.18.0.12-42009 (172.18.0.12:42009) with 4 core(s)
25/04/09 12:11:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409121124-0093/2 on hostPort 172.18.0.12:42009 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:11:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44063.
25/04/09 12:11:24 INFO NettyBlockTransferService: Server created on 3fada93ce917:44063
25/04/09 12:11:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 12:11:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3fada93ce917, 44063, None)
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409121124-0093/2 is now RUNNING
25/04/09 12:11:24 INFO BlockManagerMasterEndpoint: Registering block manager 3fada93ce917:44063 with 366.3 MiB RAM, BlockManagerId(driver, 3fada93ce917, 44063, None)
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409121124-0093/0 is now RUNNING
25/04/09 12:11:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3fada93ce917, 44063, None)
25/04/09 12:11:24 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409121124-0093/1 is now RUNNING
25/04/09 12:11:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3fada93ce917, 44063, None)
25/04/09 12:11:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 12:11:24 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 12:11:24 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 12:11:25 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/09 12:11:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40390) with ID 1,  ResourceProfileId 0
25/04/09 12:11:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:39256) with ID 0,  ResourceProfileId 0
25/04/09 12:11:25 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:47538) with ID 2,  ResourceProfileId 0
25/04/09 12:11:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:40105 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.3, 40105, None)
25/04/09 12:11:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41127 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.9, 41127, None)
25/04/09 12:11:26 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:40521 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.12, 40521, None)
25/04/09 12:11:26 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 12:11:26 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:11:26 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 12:11:26 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:11:26 INFO DAGScheduler: Missing parents: List()
25/04/09 12:11:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:11:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 12:11:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/09 12:11:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3fada93ce917:44063 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:11:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 12:11:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:11:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 12:11:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 12:11:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:41127 (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:11:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1192 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 12:11:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 12:11:27 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.284 s
25/04/09 12:11:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:11:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 12:11:27 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.315503 s
25/04/09 12:11:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3fada93ce917:44063 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:11:27 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:41127 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/09 12:11:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:11:28 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 12:11:28 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:11:28 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 12:11:28 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:11:28 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:11:28 INFO metastore: Connected to metastore.
25/04/09 12:11:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:11:29 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=18d1523f-ae4d-48b3-89b2-cba2d3c7e988, clientType=HIVECLI]
25/04/09 12:11:29 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 12:11:29 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 12:11:29 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 12:11:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:11:29 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:11:29 INFO metastore: Connected to metastore.
25/04/09 12:11:29 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:11:29 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 12:11:29 INFO metastore: Connected to metastore.
25/04/09 12:11:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 12:11:29 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 12:11:29 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 12:11:29 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 12:11:29 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:11:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:11:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:11:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:11:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:11:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:11:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:11:30 INFO CodeGenerator: Code generated in 152.90488 ms
25/04/09 12:11:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 12:11:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 12:11:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3fada93ce917:44063 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 12:11:30 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:11:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 12:11:30 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:11:30 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:11:30 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 12:11:30 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:11:30 INFO DAGScheduler: Missing parents: List()
25/04/09 12:11:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:11:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 12:11:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 12:11:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3fada93ce917:44063 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:11:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 12:11:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:11:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 12:11:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 12:11:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:41127 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:11:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:41127 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 12:11:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1044 ms on 172.18.0.9 (executor 0) (1/1)
25/04/09 12:11:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 12:11:31 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.070 s
25/04/09 12:11:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:11:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 12:11:31 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.077196 s
25/04/09 12:11:31 INFO FileFormatWriter: Start to commit write Job ce0fd80b-44c9-456c-8fa2-31d83730e788.
25/04/09 12:11:31 INFO FileFormatWriter: Write Job ce0fd80b-44c9-456c-8fa2-31d83730e788 committed. Elapsed time: 37 ms.
25/04/09 12:11:31 INFO FileFormatWriter: Finished processing stats for write job ce0fd80b-44c9-456c-8fa2-31d83730e788.
25/04/09 12:11:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:11:31 INFO SparkUI: Stopped Spark web UI at http://3fada93ce917:4040
25/04/09 12:11:31 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 12:11:31 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 12:11:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 12:11:31 INFO MemoryStore: MemoryStore cleared
25/04/09 12:11:31 INFO BlockManager: BlockManager stopped
25/04/09 12:11:31 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 12:11:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 12:11:31 INFO SparkContext: Successfully stopped SparkContext
25/04/09 12:11:31 INFO ShutdownHookManager: Shutdown hook called
25/04/09 12:11:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e2b131-de59-46bf-851a-730fddd01541
25/04/09 12:11:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-54e2b131-de59-46bf-851a-730fddd01541/pyspark-d68e20da-2864-4b6b-ba16-446aaef667b4
25/04/09 12:11:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-5259ad63-373f-465d-bd10-7f537a4d0bc3
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/09 12:34:08 INFO SparkContext: Running Spark version 3.2.2
25/04/09 12:34:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/09 12:34:08 INFO ResourceUtils: ==============================================================
25/04/09 12:34:08 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/09 12:34:08 INFO ResourceUtils: ==============================================================
25/04/09 12:34:08 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/09 12:34:08 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/09 12:34:08 INFO ResourceProfile: Limiting resource is cpu
25/04/09 12:34:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/09 12:34:08 INFO SecurityManager: Changing view acls to: root
25/04/09 12:34:08 INFO SecurityManager: Changing modify acls to: root
25/04/09 12:34:08 INFO SecurityManager: Changing view acls groups to: 
25/04/09 12:34:08 INFO SecurityManager: Changing modify acls groups to: 
25/04/09 12:34:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/09 12:34:08 INFO Utils: Successfully started service 'sparkDriver' on port 37043.
25/04/09 12:34:08 INFO SparkEnv: Registering MapOutputTracker
25/04/09 12:34:08 INFO SparkEnv: Registering BlockManagerMaster
25/04/09 12:34:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/09 12:34:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/09 12:34:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/09 12:34:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-28381d44-fc3e-4a3e-9ed3-10563303bba0
25/04/09 12:34:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/09 12:34:08 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/09 12:34:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/09 12:34:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://e0709172c692:4040
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/09 12:34:09 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 26 ms (0 ms spent in bootstraps)
25/04/09 12:34:09 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250409123409-0001
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409123409-0001/0 on worker-20250409123233-172.18.0.2-43421 (172.18.0.2:43421) with 4 core(s)
25/04/09 12:34:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409123409-0001/0 on hostPort 172.18.0.2:43421 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409123409-0001/1 on worker-20250409123233-172.18.0.5-32837 (172.18.0.5:32837) with 4 core(s)
25/04/09 12:34:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409123409-0001/1 on hostPort 172.18.0.5:32837 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250409123409-0001/2 on worker-20250409123233-172.18.0.7-34901 (172.18.0.7:34901) with 4 core(s)
25/04/09 12:34:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20250409123409-0001/2 on hostPort 172.18.0.7:34901 with 4 core(s), 1024.0 MiB RAM
25/04/09 12:34:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39549.
25/04/09 12:34:09 INFO NettyBlockTransferService: Server created on e0709172c692:39549
25/04/09 12:34:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/09 12:34:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e0709172c692, 39549, None)
25/04/09 12:34:09 INFO BlockManagerMasterEndpoint: Registering block manager e0709172c692:39549 with 366.3 MiB RAM, BlockManagerId(driver, e0709172c692, 39549, None)
25/04/09 12:34:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e0709172c692, 39549, None)
25/04/09 12:34:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e0709172c692, 39549, None)
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409123409-0001/0 is now RUNNING
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409123409-0001/2 is now RUNNING
25/04/09 12:34:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250409123409-0001/1 is now RUNNING
25/04/09 12:34:09 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/09 12:34:09 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/09 12:34:09 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/09 12:34:11 INFO InMemoryFileIndex: It took 78 ms to list leaf files for 1 paths.
25/04/09 12:34:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.2:50548) with ID 0,  ResourceProfileId 0
25/04/09 12:34:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:54374) with ID 1,  ResourceProfileId 0
25/04/09 12:34:11 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:37976) with ID 2,  ResourceProfileId 0
25/04/09 12:34:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:41755 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.5, 41755, None)
25/04/09 12:34:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.2:34535 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.2, 34535, None)
25/04/09 12:34:11 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:36113 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.7, 36113, None)
25/04/09 12:34:12 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/09 12:34:12 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:34:12 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/09 12:34:12 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:34:12 INFO DAGScheduler: Missing parents: List()
25/04/09 12:34:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:34:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/09 12:34:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/09 12:34:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e0709172c692:39549 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 12:34:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/09 12:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:34:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/09 12:34:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.2, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/09 12:34:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.2:34535 (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 12:34:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1355 ms on 172.18.0.2 (executor 0) (1/1)
25/04/09 12:34:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/09 12:34:13 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.463 s
25/04/09 12:34:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:34:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/09 12:34:13 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.501931 s
25/04/09 12:34:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e0709172c692:39549 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 12:34:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.2:34535 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/09 12:34:14 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:34:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/09 12:34:14 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/09 12:34:15 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/09 12:34:15 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:34:15 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:34:15 INFO metastore: Connected to metastore.
25/04/09 12:34:15 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
25/04/09 12:34:17 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=6bf0937b-7627-4a6f-ac79-ef4af5955195, clientType=HIVECLI]
25/04/09 12:34:17 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/09 12:34:17 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/09 12:34:17 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/09 12:34:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:34:17 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/09 12:34:17 INFO metastore: Connected to metastore.
25/04/09 12:34:17 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/09 12:34:17 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/09 12:34:17 INFO metastore: Connected to metastore.
25/04/09 12:34:17 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/09 12:34:17 INFO FileSourceStrategy: Pushed Filters: 
25/04/09 12:34:17 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/09 12:34:17 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/09 12:34:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:34:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:34:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:34:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:34:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/09 12:34:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/09 12:34:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/09 12:34:17 INFO CodeGenerator: Code generated in 173.849312 ms
25/04/09 12:34:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/09 12:34:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/09 12:34:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e0709172c692:39549 (size: 33.8 KiB, free: 366.3 MiB)
25/04/09 12:34:17 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:34:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/09 12:34:17 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/09 12:34:17 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/09 12:34:17 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/09 12:34:17 INFO DAGScheduler: Parents of final stage: List()
25/04/09 12:34:17 INFO DAGScheduler: Missing parents: List()
25/04/09 12:34:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/09 12:34:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/09 12:34:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/09 12:34:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e0709172c692:39549 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:34:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/09 12:34:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/09 12:34:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/09 12:34:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.7, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/09 12:34:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.7:36113 (size: 74.7 KiB, free: 366.2 MiB)
25/04/09 12:34:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.7:36113 (size: 33.8 KiB, free: 366.2 MiB)
25/04/09 12:34:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2492 ms on 172.18.0.7 (executor 2) (1/1)
25/04/09 12:34:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/09 12:34:20 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.532 s
25/04/09 12:34:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/09 12:34:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/09 12:34:20 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.540707 s
25/04/09 12:34:20 INFO FileFormatWriter: Start to commit write Job be38c620-e14f-4a29-b65a-500a365cf30f.
25/04/09 12:34:20 INFO FileFormatWriter: Write Job be38c620-e14f-4a29-b65a-500a365cf30f committed. Elapsed time: 44 ms.
25/04/09 12:34:20 INFO FileFormatWriter: Finished processing stats for write job be38c620-e14f-4a29-b65a-500a365cf30f.
25/04/09 12:34:20 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/09 12:34:20 INFO SparkUI: Stopped Spark web UI at http://e0709172c692:4040
25/04/09 12:34:20 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/09 12:34:20 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/09 12:34:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/09 12:34:20 INFO MemoryStore: MemoryStore cleared
25/04/09 12:34:20 INFO BlockManager: BlockManager stopped
25/04/09 12:34:20 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/09 12:34:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/09 12:34:20 INFO SparkContext: Successfully stopped SparkContext
25/04/09 12:34:21 INFO ShutdownHookManager: Shutdown hook called
25/04/09 12:34:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-90fe4833-5b69-4d06-8fe8-b90fcbd3db29
25/04/09 12:34:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c91a03d8-035c-4377-aac9-5356da18d722/pyspark-53a97ecd-022d-4ac3-af8b-01eb0552ea33
25/04/09 12:34:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c91a03d8-035c-4377-aac9-5356da18d722
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/14 12:10:36 INFO SparkContext: Running Spark version 3.2.2
25/04/14 12:10:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/14 12:10:36 INFO ResourceUtils: ==============================================================
25/04/14 12:10:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/14 12:10:36 INFO ResourceUtils: ==============================================================
25/04/14 12:10:36 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/14 12:10:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/14 12:10:36 INFO ResourceProfile: Limiting resource is cpu
25/04/14 12:10:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/14 12:10:36 INFO SecurityManager: Changing view acls to: root
25/04/14 12:10:36 INFO SecurityManager: Changing modify acls to: root
25/04/14 12:10:36 INFO SecurityManager: Changing view acls groups to: 
25/04/14 12:10:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/14 12:10:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/14 12:10:36 INFO Utils: Successfully started service 'sparkDriver' on port 44703.
25/04/14 12:10:36 INFO SparkEnv: Registering MapOutputTracker
25/04/14 12:10:36 INFO SparkEnv: Registering BlockManagerMaster
25/04/14 12:10:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/14 12:10:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/14 12:10:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/14 12:10:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-de508559-5e6b-4220-81f3-95d44710bd0b
25/04/14 12:10:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/14 12:10:37 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/14 12:10:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/14 12:10:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/14 12:10:37 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/14 12:10:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250414121037-0009
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414121037-0009/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/14 12:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414121037-0009/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414121037-0009/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/14 12:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414121037-0009/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414121037-0009/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/14 12:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414121037-0009/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/14 12:10:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36723.
25/04/14 12:10:37 INFO NettyBlockTransferService: Server created on 32988ccd198e:36723
25/04/14 12:10:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/14 12:10:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 36723, None)
25/04/14 12:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:36723 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 36723, None)
25/04/14 12:10:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 36723, None)
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414121037-0009/0 is now RUNNING
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414121037-0009/1 is now RUNNING
25/04/14 12:10:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 36723, None)
25/04/14 12:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414121037-0009/2 is now RUNNING
25/04/14 12:10:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/14 12:10:37 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/14 12:10:37 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/14 12:10:39 INFO InMemoryFileIndex: It took 63 ms to list leaf files for 1 paths.
25/04/14 12:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:34514) with ID 0,  ResourceProfileId 0
25/04/14 12:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:56758) with ID 2,  ResourceProfileId 0
25/04/14 12:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:60264) with ID 1,  ResourceProfileId 0
25/04/14 12:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:42307 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 42307, None)
25/04/14 12:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38561 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 38561, None)
25/04/14 12:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:36241 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 36241, None)
25/04/14 12:10:39 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/14 12:10:39 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 12:10:39 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/14 12:10:39 INFO DAGScheduler: Parents of final stage: List()
25/04/14 12:10:39 INFO DAGScheduler: Missing parents: List()
25/04/14 12:10:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 12:10:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/14 12:10:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/14 12:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:36723 (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 12:10:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/14 12:10:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 12:10:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/14 12:10:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/14 12:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:36241 (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 12:10:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1163 ms on 172.18.0.9 (executor 1) (1/1)
25/04/14 12:10:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/14 12:10:40 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.257 s
25/04/14 12:10:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 12:10:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/14 12:10:40 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.290423 s
25/04/14 12:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:36723 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 12:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:36241 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 12:10:41 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 12:10:41 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/14 12:10:42 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 12:10:42 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/14 12:10:42 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 12:10:42 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 12:10:42 INFO metastore: Connected to metastore.
25/04/14 12:10:42 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/14 12:10:43 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=0a075c1f-12ec-421a-82db-d5f77d1cd464, clientType=HIVECLI]
25/04/14 12:10:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/14 12:10:43 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/14 12:10:43 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/14 12:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 12:10:43 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 12:10:43 INFO metastore: Connected to metastore.
25/04/14 12:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 12:10:43 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/14 12:10:43 INFO metastore: Connected to metastore.
25/04/14 12:10:43 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/14 12:10:43 INFO FileSourceStrategy: Pushed Filters: 
25/04/14 12:10:43 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/14 12:10:43 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/14 12:10:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 12:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 12:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 12:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 12:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 12:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 12:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 12:10:44 INFO CodeGenerator: Code generated in 158.908174 ms
25/04/14 12:10:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/14 12:10:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/14 12:10:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:36723 (size: 33.8 KiB, free: 366.3 MiB)
25/04/14 12:10:44 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/14 12:10:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/14 12:10:44 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/14 12:10:44 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 12:10:44 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/14 12:10:44 INFO DAGScheduler: Parents of final stage: List()
25/04/14 12:10:44 INFO DAGScheduler: Missing parents: List()
25/04/14 12:10:44 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 12:10:44 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/14 12:10:44 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/14 12:10:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:36723 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 12:10:44 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/14 12:10:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 12:10:44 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/14 12:10:44 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/14 12:10:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:42307 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 12:10:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:42307 (size: 33.8 KiB, free: 366.2 MiB)
25/04/14 12:10:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2012 ms on 172.18.0.6 (executor 0) (1/1)
25/04/14 12:10:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/14 12:10:46 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.037 s
25/04/14 12:10:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 12:10:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/14 12:10:46 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.043493 s
25/04/14 12:10:46 INFO FileFormatWriter: Start to commit write Job 7111b210-698b-46b0-9dd2-1645316251f3.
25/04/14 12:10:46 INFO FileFormatWriter: Write Job 7111b210-698b-46b0-9dd2-1645316251f3 committed. Elapsed time: 40 ms.
25/04/14 12:10:46 INFO FileFormatWriter: Finished processing stats for write job 7111b210-698b-46b0-9dd2-1645316251f3.
25/04/14 12:10:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/14 12:10:46 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/14 12:10:46 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/14 12:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/14 12:10:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/14 12:10:46 INFO MemoryStore: MemoryStore cleared
25/04/14 12:10:46 INFO BlockManager: BlockManager stopped
25/04/14 12:10:46 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/14 12:10:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/14 12:10:46 INFO SparkContext: Successfully stopped SparkContext
25/04/14 12:10:46 INFO ShutdownHookManager: Shutdown hook called
25/04/14 12:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-e421ee21-ef34-4753-b2aa-cf11cb1efecf
25/04/14 12:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-e421ee21-ef34-4753-b2aa-cf11cb1efecf/pyspark-78c243db-d682-490f-b44f-97ed67eab9e8
25/04/14 12:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd212469-ea5c-497c-9b79-50bf20101f97
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/14 16:10:36 INFO SparkContext: Running Spark version 3.2.2
25/04/14 16:10:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/14 16:10:36 INFO ResourceUtils: ==============================================================
25/04/14 16:10:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/14 16:10:36 INFO ResourceUtils: ==============================================================
25/04/14 16:10:36 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/14 16:10:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/14 16:10:36 INFO ResourceProfile: Limiting resource is cpu
25/04/14 16:10:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/14 16:10:36 INFO SecurityManager: Changing view acls to: root
25/04/14 16:10:36 INFO SecurityManager: Changing modify acls to: root
25/04/14 16:10:36 INFO SecurityManager: Changing view acls groups to: 
25/04/14 16:10:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/14 16:10:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/14 16:10:37 INFO Utils: Successfully started service 'sparkDriver' on port 45931.
25/04/14 16:10:37 INFO SparkEnv: Registering MapOutputTracker
25/04/14 16:10:37 INFO SparkEnv: Registering BlockManagerMaster
25/04/14 16:10:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/14 16:10:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/14 16:10:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/14 16:10:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4cf19eec-8dfa-4528-b577-934b7635d68d
25/04/14 16:10:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/14 16:10:37 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/14 16:10:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/14 16:10:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/14 16:10:37 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/14 16:10:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250414161037-0012
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414161037-0012/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/14 16:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414161037-0012/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414161037-0012/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/14 16:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414161037-0012/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414161037-0012/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/14 16:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414161037-0012/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/14 16:10:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39435.
25/04/14 16:10:37 INFO NettyBlockTransferService: Server created on 32988ccd198e:39435
25/04/14 16:10:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/14 16:10:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 39435, None)
25/04/14 16:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:39435 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 39435, None)
25/04/14 16:10:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 39435, None)
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414161037-0012/2 is now RUNNING
25/04/14 16:10:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 39435, None)
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414161037-0012/0 is now RUNNING
25/04/14 16:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414161037-0012/1 is now RUNNING
25/04/14 16:10:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/14 16:10:37 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/14 16:10:38 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/14 16:10:39 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
25/04/14 16:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:51236) with ID 0,  ResourceProfileId 0
25/04/14 16:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:51432) with ID 1,  ResourceProfileId 0
25/04/14 16:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:51704) with ID 2,  ResourceProfileId 0
25/04/14 16:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:34943 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 34943, None)
25/04/14 16:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:35805 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 35805, None)
25/04/14 16:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45445 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 45445, None)
25/04/14 16:10:39 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/14 16:10:39 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 16:10:39 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/14 16:10:39 INFO DAGScheduler: Parents of final stage: List()
25/04/14 16:10:39 INFO DAGScheduler: Missing parents: List()
25/04/14 16:10:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 16:10:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/14 16:10:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/14 16:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:39435 (size: 35.6 KiB, free: 366.3 MiB)
25/04/14 16:10:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/14 16:10:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 16:10:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/14 16:10:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/14 16:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:45445 (size: 35.6 KiB, free: 366.3 MiB)
25/04/14 16:10:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1166 ms on 172.18.0.10 (executor 2) (1/1)
25/04/14 16:10:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/14 16:10:40 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.260 s
25/04/14 16:10:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 16:10:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/14 16:10:40 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.293206 s
25/04/14 16:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:39435 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/14 16:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:45445 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/14 16:10:42 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 16:10:42 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/14 16:10:42 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 16:10:42 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/14 16:10:42 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 16:10:42 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 16:10:42 INFO metastore: Connected to metastore.
25/04/14 16:10:42 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/14 16:10:43 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=4a35be62-dd7e-4c08-a6e7-341c2bf7239d, clientType=HIVECLI]
25/04/14 16:10:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/14 16:10:43 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/14 16:10:43 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/14 16:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 16:10:43 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 16:10:43 INFO metastore: Connected to metastore.
25/04/14 16:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 16:10:43 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/14 16:10:43 INFO metastore: Connected to metastore.
25/04/14 16:10:43 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/14 16:10:43 INFO FileSourceStrategy: Pushed Filters: 
25/04/14 16:10:43 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/14 16:10:43 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/14 16:10:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 16:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 16:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 16:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 16:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 16:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 16:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 16:10:43 INFO CodeGenerator: Code generated in 160.872862 ms
25/04/14 16:10:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/14 16:10:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/14 16:10:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:39435 (size: 33.8 KiB, free: 366.3 MiB)
25/04/14 16:10:43 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/14 16:10:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/14 16:10:43 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/14 16:10:43 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 16:10:43 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/14 16:10:43 INFO DAGScheduler: Parents of final stage: List()
25/04/14 16:10:43 INFO DAGScheduler: Missing parents: List()
25/04/14 16:10:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 16:10:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/14 16:10:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/14 16:10:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:39435 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 16:10:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/14 16:10:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 16:10:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/14 16:10:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/14 16:10:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:35805 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 16:10:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:35805 (size: 33.8 KiB, free: 366.2 MiB)
25/04/14 16:10:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2030 ms on 172.18.0.9 (executor 1) (1/1)
25/04/14 16:10:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/14 16:10:45 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.057 s
25/04/14 16:10:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 16:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/14 16:10:45 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.064159 s
25/04/14 16:10:45 INFO FileFormatWriter: Start to commit write Job 56da84b8-fd7f-481c-903f-45e4685eda1a.
25/04/14 16:10:45 INFO FileFormatWriter: Write Job 56da84b8-fd7f-481c-903f-45e4685eda1a committed. Elapsed time: 38 ms.
25/04/14 16:10:45 INFO FileFormatWriter: Finished processing stats for write job 56da84b8-fd7f-481c-903f-45e4685eda1a.
25/04/14 16:10:45 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/14 16:10:45 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/14 16:10:45 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/14 16:10:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/14 16:10:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/14 16:10:45 INFO MemoryStore: MemoryStore cleared
25/04/14 16:10:45 INFO BlockManager: BlockManager stopped
25/04/14 16:10:45 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/14 16:10:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/14 16:10:45 INFO SparkContext: Successfully stopped SparkContext
25/04/14 16:10:46 INFO ShutdownHookManager: Shutdown hook called
25/04/14 16:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-464cd243-d7a9-415a-88fd-7dea29dd243b
25/04/14 16:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-63601eff-755b-4868-8a28-5366c4f49cfb
25/04/14 16:10:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-464cd243-d7a9-415a-88fd-7dea29dd243b/pyspark-9e811cda-815d-4d02-a0e8-25ca42f437c9
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/14 20:10:36 INFO SparkContext: Running Spark version 3.2.2
25/04/14 20:10:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/14 20:10:36 INFO ResourceUtils: ==============================================================
25/04/14 20:10:36 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/14 20:10:36 INFO ResourceUtils: ==============================================================
25/04/14 20:10:36 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/14 20:10:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/14 20:10:36 INFO ResourceProfile: Limiting resource is cpu
25/04/14 20:10:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/14 20:10:36 INFO SecurityManager: Changing view acls to: root
25/04/14 20:10:36 INFO SecurityManager: Changing modify acls to: root
25/04/14 20:10:36 INFO SecurityManager: Changing view acls groups to: 
25/04/14 20:10:36 INFO SecurityManager: Changing modify acls groups to: 
25/04/14 20:10:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/14 20:10:37 INFO Utils: Successfully started service 'sparkDriver' on port 38909.
25/04/14 20:10:37 INFO SparkEnv: Registering MapOutputTracker
25/04/14 20:10:37 INFO SparkEnv: Registering BlockManagerMaster
25/04/14 20:10:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/14 20:10:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/14 20:10:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/14 20:10:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-580203b7-9d82-4b47-9bc4-c05b8fe7ef25
25/04/14 20:10:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/14 20:10:37 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/14 20:10:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/14 20:10:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/14 20:10:37 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/14 20:10:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250414201037-0015
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414201037-0015/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/14 20:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414201037-0015/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414201037-0015/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/14 20:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414201037-0015/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250414201037-0015/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/14 20:10:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20250414201037-0015/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/14 20:10:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46793.
25/04/14 20:10:37 INFO NettyBlockTransferService: Server created on 32988ccd198e:46793
25/04/14 20:10:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/14 20:10:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 46793, None)
25/04/14 20:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:46793 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 46793, None)
25/04/14 20:10:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 46793, None)
25/04/14 20:10:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 46793, None)
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414201037-0015/1 is now RUNNING
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414201037-0015/2 is now RUNNING
25/04/14 20:10:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250414201037-0015/0 is now RUNNING
25/04/14 20:10:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/14 20:10:38 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/14 20:10:38 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/14 20:10:39 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/14 20:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:46588) with ID 1,  ResourceProfileId 0
25/04/14 20:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:34884) with ID 0,  ResourceProfileId 0
25/04/14 20:10:39 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:44080) with ID 2,  ResourceProfileId 0
25/04/14 20:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:45219 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 45219, None)
25/04/14 20:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:33991 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 33991, None)
25/04/14 20:10:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:39915 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 39915, None)
25/04/14 20:10:39 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/14 20:10:39 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 20:10:39 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/14 20:10:39 INFO DAGScheduler: Parents of final stage: List()
25/04/14 20:10:39 INFO DAGScheduler: Missing parents: List()
25/04/14 20:10:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 20:10:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/14 20:10:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/14 20:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:46793 (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 20:10:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/14 20:10:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 20:10:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/14 20:10:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/14 20:10:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:33991 (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 20:10:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1168 ms on 172.18.0.6 (executor 0) (1/1)
25/04/14 20:10:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/14 20:10:40 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.261 s
25/04/14 20:10:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 20:10:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/14 20:10:40 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.292897 s
25/04/14 20:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:46793 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 20:10:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:33991 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/14 20:10:42 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 20:10:42 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/14 20:10:42 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/14 20:10:42 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/14 20:10:42 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 20:10:42 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 20:10:42 INFO metastore: Connected to metastore.
25/04/14 20:10:42 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/14 20:10:43 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=b0860cb0-6676-4bc9-a23f-5884317b58e9, clientType=HIVECLI]
25/04/14 20:10:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/14 20:10:43 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/14 20:10:43 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/14 20:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 20:10:43 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/14 20:10:43 INFO metastore: Connected to metastore.
25/04/14 20:10:43 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/14 20:10:43 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/14 20:10:43 INFO metastore: Connected to metastore.
25/04/14 20:10:43 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/14 20:10:43 INFO FileSourceStrategy: Pushed Filters: 
25/04/14 20:10:43 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/14 20:10:43 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/14 20:10:43 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 20:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 20:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 20:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 20:10:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/14 20:10:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/14 20:10:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/14 20:10:43 INFO CodeGenerator: Code generated in 155.558884 ms
25/04/14 20:10:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/14 20:10:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/14 20:10:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:46793 (size: 33.8 KiB, free: 366.3 MiB)
25/04/14 20:10:43 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/14 20:10:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/14 20:10:43 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/14 20:10:43 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/14 20:10:43 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/14 20:10:43 INFO DAGScheduler: Parents of final stage: List()
25/04/14 20:10:43 INFO DAGScheduler: Missing parents: List()
25/04/14 20:10:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/14 20:10:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/14 20:10:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/14 20:10:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:46793 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 20:10:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/14 20:10:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/14 20:10:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/14 20:10:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/14 20:10:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:39915 (size: 74.7 KiB, free: 366.2 MiB)
25/04/14 20:10:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:39915 (size: 33.8 KiB, free: 366.2 MiB)
25/04/14 20:10:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2054 ms on 172.18.0.10 (executor 2) (1/1)
25/04/14 20:10:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/14 20:10:45 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.079 s
25/04/14 20:10:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/14 20:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/14 20:10:45 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.085860 s
25/04/14 20:10:45 INFO FileFormatWriter: Start to commit write Job 01f96131-b9df-4cdd-a3d7-eda804bc8f6e.
25/04/14 20:10:45 INFO FileFormatWriter: Write Job 01f96131-b9df-4cdd-a3d7-eda804bc8f6e committed. Elapsed time: 37 ms.
25/04/14 20:10:45 INFO FileFormatWriter: Finished processing stats for write job 01f96131-b9df-4cdd-a3d7-eda804bc8f6e.
25/04/14 20:10:45 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/14 20:10:45 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/14 20:10:45 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/14 20:10:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/14 20:10:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/14 20:10:45 INFO MemoryStore: MemoryStore cleared
25/04/14 20:10:45 INFO BlockManager: BlockManager stopped
25/04/14 20:10:45 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/14 20:10:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/14 20:10:45 INFO SparkContext: Successfully stopped SparkContext
25/04/14 20:10:45 INFO ShutdownHookManager: Shutdown hook called
25/04/14 20:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a021e9e-b983-4c88-bbe2-3a2e391c32ee
25/04/14 20:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-68a58f85-d1fb-4ea4-94fb-047b83f94000/pyspark-1372cccb-3c72-4cd8-9807-dd5867799312
25/04/14 20:10:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-68a58f85-d1fb-4ea4-94fb-047b83f94000
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 00:10:34 INFO SparkContext: Running Spark version 3.2.2
25/04/15 00:10:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 00:10:34 INFO ResourceUtils: ==============================================================
25/04/15 00:10:34 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 00:10:34 INFO ResourceUtils: ==============================================================
25/04/15 00:10:34 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 00:10:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 00:10:34 INFO ResourceProfile: Limiting resource is cpu
25/04/15 00:10:34 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 00:10:34 INFO SecurityManager: Changing view acls to: root
25/04/15 00:10:34 INFO SecurityManager: Changing modify acls to: root
25/04/15 00:10:34 INFO SecurityManager: Changing view acls groups to: 
25/04/15 00:10:34 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 00:10:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 00:10:34 INFO Utils: Successfully started service 'sparkDriver' on port 37613.
25/04/15 00:10:34 INFO SparkEnv: Registering MapOutputTracker
25/04/15 00:10:34 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 00:10:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 00:10:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 00:10:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 00:10:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c568fcab-b5ad-4416-b073-f3f457c2c22e
25/04/15 00:10:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 00:10:35 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 00:10:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 00:10:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 00:10:35 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/15 00:10:35 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415001035-0020
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415001035-0020/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 00:10:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415001035-0020/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415001035-0020/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 00:10:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415001035-0020/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415001035-0020/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 00:10:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415001035-0020/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 00:10:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44131.
25/04/15 00:10:35 INFO NettyBlockTransferService: Server created on 32988ccd198e:44131
25/04/15 00:10:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 00:10:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 44131, None)
25/04/15 00:10:35 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:44131 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 44131, None)
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415001035-0020/0 is now RUNNING
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415001035-0020/2 is now RUNNING
25/04/15 00:10:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 44131, None)
25/04/15 00:10:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 44131, None)
25/04/15 00:10:35 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415001035-0020/1 is now RUNNING
25/04/15 00:10:35 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 00:10:35 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 00:10:35 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 00:10:37 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/15 00:10:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:40884) with ID 2,  ResourceProfileId 0
25/04/15 00:10:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:56098) with ID 0,  ResourceProfileId 0
25/04/15 00:10:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:51000) with ID 1,  ResourceProfileId 0
25/04/15 00:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:44817 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 44817, None)
25/04/15 00:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:37569 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 37569, None)
25/04/15 00:10:37 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:38399 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 38399, None)
25/04/15 00:10:37 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 00:10:37 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 00:10:37 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 00:10:37 INFO DAGScheduler: Parents of final stage: List()
25/04/15 00:10:37 INFO DAGScheduler: Missing parents: List()
25/04/15 00:10:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 00:10:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 00:10:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/15 00:10:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:44131 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 00:10:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 00:10:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 00:10:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 00:10:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 00:10:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:38399 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 00:10:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1154 ms on 172.18.0.9 (executor 1) (1/1)
25/04/15 00:10:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 00:10:38 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.247 s
25/04/15 00:10:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 00:10:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 00:10:38 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.279529 s
25/04/15 00:10:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:44131 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 00:10:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:38399 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 00:10:39 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 00:10:39 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 00:10:40 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 00:10:40 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 00:10:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 00:10:40 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 00:10:40 INFO metastore: Connected to metastore.
25/04/15 00:10:40 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/15 00:10:40 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=50f24fe6-f022-4c4c-b1e1-0df665cf41d8, clientType=HIVECLI]
25/04/15 00:10:40 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 00:10:40 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 00:10:40 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 00:10:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 00:10:40 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 00:10:40 INFO metastore: Connected to metastore.
25/04/15 00:10:40 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 00:10:40 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 00:10:40 INFO metastore: Connected to metastore.
25/04/15 00:10:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 00:10:41 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 00:10:41 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 00:10:41 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 00:10:41 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 00:10:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 00:10:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 00:10:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 00:10:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 00:10:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 00:10:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 00:10:41 INFO CodeGenerator: Code generated in 155.84557 ms
25/04/15 00:10:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 00:10:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 00:10:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:44131 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 00:10:41 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 00:10:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 00:10:41 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 00:10:41 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 00:10:41 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 00:10:41 INFO DAGScheduler: Parents of final stage: List()
25/04/15 00:10:41 INFO DAGScheduler: Missing parents: List()
25/04/15 00:10:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 00:10:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 00:10:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 00:10:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:44131 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 00:10:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 00:10:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 00:10:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 00:10:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 00:10:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:37569 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 00:10:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:37569 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 00:10:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2009 ms on 172.18.0.10 (executor 2) (1/1)
25/04/15 00:10:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 00:10:43 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.034 s
25/04/15 00:10:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 00:10:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 00:10:43 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.040877 s
25/04/15 00:10:43 INFO FileFormatWriter: Start to commit write Job 35800879-036f-4170-ae2b-533ebccad039.
25/04/15 00:10:43 INFO FileFormatWriter: Write Job 35800879-036f-4170-ae2b-533ebccad039 committed. Elapsed time: 38 ms.
25/04/15 00:10:43 INFO FileFormatWriter: Finished processing stats for write job 35800879-036f-4170-ae2b-533ebccad039.
25/04/15 00:10:43 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 00:10:43 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 00:10:43 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 00:10:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 00:10:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 00:10:43 INFO MemoryStore: MemoryStore cleared
25/04/15 00:10:43 INFO BlockManager: BlockManager stopped
25/04/15 00:10:43 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 00:10:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 00:10:43 INFO SparkContext: Successfully stopped SparkContext
25/04/15 00:10:44 INFO ShutdownHookManager: Shutdown hook called
25/04/15 00:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-0eec90ac-a5e3-4dd1-a664-52b5c377f8e3
25/04/15 00:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-0eec90ac-a5e3-4dd1-a664-52b5c377f8e3/pyspark-6e4ad874-5691-4ada-aa9d-eb0f196e800b
25/04/15 00:10:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-7197127a-3078-4270-a91a-7b83d6800bc2
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 04:10:43 INFO SparkContext: Running Spark version 3.2.2
25/04/15 04:10:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 04:10:43 INFO ResourceUtils: ==============================================================
25/04/15 04:10:43 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 04:10:43 INFO ResourceUtils: ==============================================================
25/04/15 04:10:43 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 04:10:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 04:10:43 INFO ResourceProfile: Limiting resource is cpu
25/04/15 04:10:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 04:10:43 INFO SecurityManager: Changing view acls to: root
25/04/15 04:10:43 INFO SecurityManager: Changing modify acls to: root
25/04/15 04:10:43 INFO SecurityManager: Changing view acls groups to: 
25/04/15 04:10:43 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 04:10:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 04:10:44 INFO Utils: Successfully started service 'sparkDriver' on port 43435.
25/04/15 04:10:44 INFO SparkEnv: Registering MapOutputTracker
25/04/15 04:10:44 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 04:10:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 04:10:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 04:10:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 04:10:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a0c95feb-ae23-447d-826f-09eedce2e1c2
25/04/15 04:10:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 04:10:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 04:10:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 04:10:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 04:10:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 21 ms (0 ms spent in bootstraps)
25/04/15 04:10:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415041044-0023
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415041044-0023/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 04:10:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415041044-0023/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415041044-0023/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 04:10:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415041044-0023/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415041044-0023/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 04:10:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415041044-0023/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 04:10:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33613.
25/04/15 04:10:44 INFO NettyBlockTransferService: Server created on 32988ccd198e:33613
25/04/15 04:10:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 04:10:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 33613, None)
25/04/15 04:10:44 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:33613 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 33613, None)
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415041044-0023/1 is now RUNNING
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415041044-0023/0 is now RUNNING
25/04/15 04:10:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 33613, None)
25/04/15 04:10:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415041044-0023/2 is now RUNNING
25/04/15 04:10:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 33613, None)
25/04/15 04:10:44 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 04:10:45 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 04:10:45 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 04:10:46 INFO InMemoryFileIndex: It took 61 ms to list leaf files for 1 paths.
25/04/15 04:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:45330) with ID 1,  ResourceProfileId 0
25/04/15 04:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:45642) with ID 2,  ResourceProfileId 0
25/04/15 04:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:59320) with ID 0,  ResourceProfileId 0
25/04/15 04:10:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:46767 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 46767, None)
25/04/15 04:10:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:42317 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 42317, None)
25/04/15 04:10:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:36525 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 36525, None)
25/04/15 04:10:46 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 04:10:46 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 04:10:46 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 04:10:46 INFO DAGScheduler: Parents of final stage: List()
25/04/15 04:10:46 INFO DAGScheduler: Missing parents: List()
25/04/15 04:10:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 04:10:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 04:10:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/15 04:10:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:33613 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 04:10:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 04:10:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 04:10:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 04:10:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 04:10:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:42317 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 04:10:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1156 ms on 172.18.0.6 (executor 0) (1/1)
25/04/15 04:10:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 04:10:47 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.250 s
25/04/15 04:10:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 04:10:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 04:10:47 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.283650 s
25/04/15 04:10:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:33613 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 04:10:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:42317 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 04:10:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 04:10:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 04:10:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 04:10:49 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 04:10:49 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 04:10:49 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 04:10:49 INFO metastore: Connected to metastore.
25/04/15 04:10:49 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/15 04:10:49 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=64d343e7-7f5d-4f60-9f9b-20ff8b3b7108, clientType=HIVECLI]
25/04/15 04:10:49 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 04:10:49 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 04:10:49 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 04:10:49 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 04:10:49 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 04:10:49 INFO metastore: Connected to metastore.
25/04/15 04:10:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 04:10:50 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 04:10:50 INFO metastore: Connected to metastore.
25/04/15 04:10:50 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 04:10:50 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 04:10:50 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 04:10:50 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 04:10:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 04:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 04:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 04:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 04:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 04:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 04:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 04:10:50 INFO CodeGenerator: Code generated in 152.311063 ms
25/04/15 04:10:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 04:10:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 04:10:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:33613 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 04:10:50 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 04:10:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 04:10:50 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 04:10:50 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 04:10:50 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 04:10:50 INFO DAGScheduler: Parents of final stage: List()
25/04/15 04:10:50 INFO DAGScheduler: Missing parents: List()
25/04/15 04:10:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 04:10:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 04:10:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 04:10:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:33613 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 04:10:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 04:10:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 04:10:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 04:10:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 04:10:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:42317 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 04:10:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:42317 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 04:10:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 966 ms on 172.18.0.6 (executor 0) (1/1)
25/04/15 04:10:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 04:10:51 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 0.993 s
25/04/15 04:10:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 04:10:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 04:10:51 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.001315 s
25/04/15 04:10:51 INFO FileFormatWriter: Start to commit write Job 66647162-9f10-42f4-9eb9-6c95a1ac6f8e.
25/04/15 04:10:51 INFO FileFormatWriter: Write Job 66647162-9f10-42f4-9eb9-6c95a1ac6f8e committed. Elapsed time: 38 ms.
25/04/15 04:10:51 INFO FileFormatWriter: Finished processing stats for write job 66647162-9f10-42f4-9eb9-6c95a1ac6f8e.
25/04/15 04:10:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 04:10:51 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 04:10:51 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 04:10:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 04:10:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 04:10:51 INFO MemoryStore: MemoryStore cleared
25/04/15 04:10:51 INFO BlockManager: BlockManager stopped
25/04/15 04:10:51 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 04:10:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 04:10:51 INFO SparkContext: Successfully stopped SparkContext
25/04/15 04:10:51 INFO ShutdownHookManager: Shutdown hook called
25/04/15 04:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-c748f042-8b14-4ed2-aca0-d67468a2c7bf/pyspark-6a7b2c1f-e83d-493a-8cdb-d1c804c922a5
25/04/15 04:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-c748f042-8b14-4ed2-aca0-d67468a2c7bf
25/04/15 04:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-3b133b97-6efd-4fe5-908f-9490cdb6151b
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 08:10:44 INFO SparkContext: Running Spark version 3.2.2
25/04/15 08:10:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 08:10:44 INFO ResourceUtils: ==============================================================
25/04/15 08:10:44 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 08:10:44 INFO ResourceUtils: ==============================================================
25/04/15 08:10:44 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 08:10:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 08:10:44 INFO ResourceProfile: Limiting resource is cpu
25/04/15 08:10:44 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 08:10:44 INFO SecurityManager: Changing view acls to: root
25/04/15 08:10:44 INFO SecurityManager: Changing modify acls to: root
25/04/15 08:10:44 INFO SecurityManager: Changing view acls groups to: 
25/04/15 08:10:44 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 08:10:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 08:10:44 INFO Utils: Successfully started service 'sparkDriver' on port 41301.
25/04/15 08:10:44 INFO SparkEnv: Registering MapOutputTracker
25/04/15 08:10:44 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 08:10:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 08:10:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 08:10:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 08:10:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-49b6df68-dc26-4077-9518-a0ebf89958ba
25/04/15 08:10:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 08:10:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 08:10:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 08:10:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 08:10:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/15 08:10:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415081045-0026
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415081045-0026/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 08:10:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415081045-0026/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415081045-0026/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 08:10:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415081045-0026/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415081045-0026/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 08:10:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415081045-0026/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 08:10:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34571.
25/04/15 08:10:45 INFO NettyBlockTransferService: Server created on 32988ccd198e:34571
25/04/15 08:10:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 08:10:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 34571, None)
25/04/15 08:10:45 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:34571 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 34571, None)
25/04/15 08:10:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 34571, None)
25/04/15 08:10:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 34571, None)
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415081045-0026/2 is now RUNNING
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415081045-0026/1 is now RUNNING
25/04/15 08:10:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415081045-0026/0 is now RUNNING
25/04/15 08:10:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 08:10:45 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 08:10:45 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 08:10:46 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
25/04/15 08:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:45560) with ID 2,  ResourceProfileId 0
25/04/15 08:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:49132) with ID 0,  ResourceProfileId 0
25/04/15 08:10:46 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:33960) with ID 1,  ResourceProfileId 0
25/04/15 08:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:43681 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 43681, None)
25/04/15 08:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:38451 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 38451, None)
25/04/15 08:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:39753 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 39753, None)
25/04/15 08:10:47 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 08:10:47 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 08:10:47 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 08:10:47 INFO DAGScheduler: Parents of final stage: List()
25/04/15 08:10:47 INFO DAGScheduler: Missing parents: List()
25/04/15 08:10:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 08:10:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 08:10:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 366.2 MiB)
25/04/15 08:10:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:34571 (size: 35.6 KiB, free: 366.3 MiB)
25/04/15 08:10:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 08:10:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 08:10:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 08:10:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 08:10:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:43681 (size: 35.6 KiB, free: 366.3 MiB)
25/04/15 08:10:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1155 ms on 172.18.0.10 (executor 2) (1/1)
25/04/15 08:10:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 08:10:48 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.251 s
25/04/15 08:10:48 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 08:10:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 08:10:48 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.284841 s
25/04/15 08:10:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:34571 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/15 08:10:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:43681 in memory (size: 35.6 KiB, free: 366.3 MiB)
25/04/15 08:10:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 08:10:49 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 08:10:49 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 08:10:49 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 08:10:49 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 08:10:49 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 08:10:49 INFO metastore: Connected to metastore.
25/04/15 08:10:50 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/15 08:10:50 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=935f9591-3d9e-4e8f-88cb-3ddfe4eb8101, clientType=HIVECLI]
25/04/15 08:10:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 08:10:50 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 08:10:50 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 08:10:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 08:10:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 08:10:50 INFO metastore: Connected to metastore.
25/04/15 08:10:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 08:10:50 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 08:10:50 INFO metastore: Connected to metastore.
25/04/15 08:10:50 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 08:10:50 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 08:10:50 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 08:10:50 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 08:10:50 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 08:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 08:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 08:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 08:10:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 08:10:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 08:10:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 08:10:51 INFO CodeGenerator: Code generated in 151.901669 ms
25/04/15 08:10:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 08:10:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 08:10:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:34571 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 08:10:51 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 08:10:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 08:10:51 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 08:10:51 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 08:10:51 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 08:10:51 INFO DAGScheduler: Parents of final stage: List()
25/04/15 08:10:51 INFO DAGScheduler: Missing parents: List()
25/04/15 08:10:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 08:10:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 08:10:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 08:10:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:34571 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 08:10:51 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 08:10:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 08:10:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 08:10:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 08:10:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:38451 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 08:10:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:38451 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 08:10:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2028 ms on 172.18.0.6 (executor 0) (1/1)
25/04/15 08:10:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 08:10:53 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.053 s
25/04/15 08:10:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 08:10:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 08:10:53 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.059816 s
25/04/15 08:10:53 INFO FileFormatWriter: Start to commit write Job 197e3789-5f73-495f-895b-a5fe6ccb5f0a.
25/04/15 08:10:53 INFO FileFormatWriter: Write Job 197e3789-5f73-495f-895b-a5fe6ccb5f0a committed. Elapsed time: 38 ms.
25/04/15 08:10:53 INFO FileFormatWriter: Finished processing stats for write job 197e3789-5f73-495f-895b-a5fe6ccb5f0a.
25/04/15 08:10:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 08:10:53 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 08:10:53 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 08:10:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 08:10:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 08:10:53 INFO MemoryStore: MemoryStore cleared
25/04/15 08:10:53 INFO BlockManager: BlockManager stopped
25/04/15 08:10:53 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 08:10:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 08:10:53 INFO SparkContext: Successfully stopped SparkContext
25/04/15 08:10:53 INFO ShutdownHookManager: Shutdown hook called
25/04/15 08:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-81998aab-b24e-4bbd-8198-8c622f661b69/pyspark-38dbe6d9-d278-49e7-80b1-c184fccec9e2
25/04/15 08:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-e816c5fe-d36e-45f7-8586-47cd62f9db9d
25/04/15 08:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-81998aab-b24e-4bbd-8198-8c622f661b69
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 12:10:45 INFO SparkContext: Running Spark version 3.2.2
25/04/15 12:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 12:10:45 INFO ResourceUtils: ==============================================================
25/04/15 12:10:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 12:10:45 INFO ResourceUtils: ==============================================================
25/04/15 12:10:45 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 12:10:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 12:10:45 INFO ResourceProfile: Limiting resource is cpu
25/04/15 12:10:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 12:10:45 INFO SecurityManager: Changing view acls to: root
25/04/15 12:10:45 INFO SecurityManager: Changing modify acls to: root
25/04/15 12:10:45 INFO SecurityManager: Changing view acls groups to: 
25/04/15 12:10:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 12:10:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 12:10:45 INFO Utils: Successfully started service 'sparkDriver' on port 41085.
25/04/15 12:10:45 INFO SparkEnv: Registering MapOutputTracker
25/04/15 12:10:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 12:10:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 12:10:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 12:10:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 12:10:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5c8ed087-329c-4133-a689-d33c4bd88601
25/04/15 12:10:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 12:10:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 12:10:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 12:10:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 12:10:46 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/15 12:10:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415121046-0029
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415121046-0029/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 12:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415121046-0029/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415121046-0029/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 12:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415121046-0029/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415121046-0029/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 12:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415121046-0029/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 12:10:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36571.
25/04/15 12:10:46 INFO NettyBlockTransferService: Server created on 32988ccd198e:36571
25/04/15 12:10:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 12:10:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 36571, None)
25/04/15 12:10:46 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:36571 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 36571, None)
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415121046-0029/1 is now RUNNING
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415121046-0029/0 is now RUNNING
25/04/15 12:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415121046-0029/2 is now RUNNING
25/04/15 12:10:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 36571, None)
25/04/15 12:10:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 36571, None)
25/04/15 12:10:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 12:10:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 12:10:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 12:10:47 INFO InMemoryFileIndex: It took 62 ms to list leaf files for 1 paths.
25/04/15 12:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:53010) with ID 2,  ResourceProfileId 0
25/04/15 12:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:38062) with ID 1,  ResourceProfileId 0
25/04/15 12:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:41844) with ID 0,  ResourceProfileId 0
25/04/15 12:10:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:33263 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 33263, None)
25/04/15 12:10:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:44457 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 44457, None)
25/04/15 12:10:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:33919 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 33919, None)
25/04/15 12:10:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 12:10:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 12:10:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 12:10:48 INFO DAGScheduler: Parents of final stage: List()
25/04/15 12:10:48 INFO DAGScheduler: Missing parents: List()
25/04/15 12:10:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 12:10:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 12:10:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/15 12:10:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:36571 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 12:10:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 12:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 12:10:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 12:10:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 12:10:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:33919 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 12:10:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1175 ms on 172.18.0.6 (executor 0) (1/1)
25/04/15 12:10:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 12:10:49 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.271 s
25/04/15 12:10:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 12:10:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 12:10:49 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.304528 s
25/04/15 12:10:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:36571 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 12:10:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:33919 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 12:10:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 12:10:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 12:10:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 12:10:50 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 12:10:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 12:10:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 12:10:50 INFO metastore: Connected to metastore.
25/04/15 12:10:51 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/15 12:10:51 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=872f89d3-82ee-48ef-a52b-8041fdaeefbf, clientType=HIVECLI]
25/04/15 12:10:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 12:10:51 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 12:10:51 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 12:10:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 12:10:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 12:10:51 INFO metastore: Connected to metastore.
25/04/15 12:10:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 12:10:51 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 12:10:51 INFO metastore: Connected to metastore.
25/04/15 12:10:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 12:10:51 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 12:10:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 12:10:51 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 12:10:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 12:10:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 12:10:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 12:10:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 12:10:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 12:10:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 12:10:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 12:10:52 INFO CodeGenerator: Code generated in 156.28463 ms
25/04/15 12:10:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 12:10:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 12:10:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:36571 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 12:10:52 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 12:10:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 12:10:52 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 12:10:52 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 12:10:52 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 12:10:52 INFO DAGScheduler: Parents of final stage: List()
25/04/15 12:10:52 INFO DAGScheduler: Missing parents: List()
25/04/15 12:10:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 12:10:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 12:10:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 12:10:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:36571 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 12:10:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 12:10:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 12:10:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 12:10:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 12:10:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:44457 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 12:10:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:44457 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 12:10:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1982 ms on 172.18.0.10 (executor 2) (1/1)
25/04/15 12:10:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 12:10:54 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.009 s
25/04/15 12:10:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 12:10:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 12:10:54 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.016888 s
25/04/15 12:10:54 INFO FileFormatWriter: Start to commit write Job 0f6bd5c3-ad8d-4bbf-83af-a14f03fa65fe.
25/04/15 12:10:54 INFO FileFormatWriter: Write Job 0f6bd5c3-ad8d-4bbf-83af-a14f03fa65fe committed. Elapsed time: 40 ms.
25/04/15 12:10:54 INFO FileFormatWriter: Finished processing stats for write job 0f6bd5c3-ad8d-4bbf-83af-a14f03fa65fe.
25/04/15 12:10:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 12:10:54 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 12:10:54 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 12:10:54 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 12:10:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 12:10:54 INFO MemoryStore: MemoryStore cleared
25/04/15 12:10:54 INFO BlockManager: BlockManager stopped
25/04/15 12:10:54 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 12:10:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 12:10:54 INFO SparkContext: Successfully stopped SparkContext
25/04/15 12:10:54 INFO ShutdownHookManager: Shutdown hook called
25/04/15 12:10:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-636e9b1b-1800-4284-b902-8178339f4bff
25/04/15 12:10:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec9575ee-4a8c-44b5-9a2b-ec3e17a3f647
25/04/15 12:10:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec9575ee-4a8c-44b5-9a2b-ec3e17a3f647/pyspark-8a07f5b4-7aee-4d48-b5a5-bae17837a917
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 16:10:42 INFO SparkContext: Running Spark version 3.2.2
25/04/15 16:10:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 16:10:42 INFO ResourceUtils: ==============================================================
25/04/15 16:10:42 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 16:10:42 INFO ResourceUtils: ==============================================================
25/04/15 16:10:42 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 16:10:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 16:10:42 INFO ResourceProfile: Limiting resource is cpu
25/04/15 16:10:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 16:10:42 INFO SecurityManager: Changing view acls to: root
25/04/15 16:10:42 INFO SecurityManager: Changing modify acls to: root
25/04/15 16:10:42 INFO SecurityManager: Changing view acls groups to: 
25/04/15 16:10:42 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 16:10:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 16:10:43 INFO Utils: Successfully started service 'sparkDriver' on port 39045.
25/04/15 16:10:43 INFO SparkEnv: Registering MapOutputTracker
25/04/15 16:10:43 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 16:10:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 16:10:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 16:10:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 16:10:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ca3826aa-9d64-42aa-94fe-6bc3c1f644ac
25/04/15 16:10:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 16:10:43 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 16:10:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 16:10:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 16:10:43 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 22 ms (0 ms spent in bootstraps)
25/04/15 16:10:43 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415161043-0032
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415161043-0032/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 16:10:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415161043-0032/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415161043-0032/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 16:10:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415161043-0032/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415161043-0032/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 16:10:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415161043-0032/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 16:10:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35007.
25/04/15 16:10:43 INFO NettyBlockTransferService: Server created on 32988ccd198e:35007
25/04/15 16:10:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 16:10:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 35007, None)
25/04/15 16:10:43 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:35007 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 35007, None)
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415161043-0032/2 is now RUNNING
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415161043-0032/0 is now RUNNING
25/04/15 16:10:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 35007, None)
25/04/15 16:10:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415161043-0032/1 is now RUNNING
25/04/15 16:10:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 35007, None)
25/04/15 16:10:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 16:10:43 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 16:10:43 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 16:10:45 INFO InMemoryFileIndex: It took 60 ms to list leaf files for 1 paths.
25/04/15 16:10:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:47966) with ID 2,  ResourceProfileId 0
25/04/15 16:10:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:47818) with ID 0,  ResourceProfileId 0
25/04/15 16:10:45 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:57974) with ID 1,  ResourceProfileId 0
25/04/15 16:10:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:38239 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 38239, None)
25/04/15 16:10:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45461 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 45461, None)
25/04/15 16:10:45 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:46557 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 46557, None)
25/04/15 16:10:45 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 16:10:45 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 16:10:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 16:10:45 INFO DAGScheduler: Parents of final stage: List()
25/04/15 16:10:45 INFO DAGScheduler: Missing parents: List()
25/04/15 16:10:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 16:10:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 16:10:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/15 16:10:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:35007 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 16:10:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 16:10:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 16:10:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 16:10:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6, executor 0, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 16:10:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.6:38239 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 16:10:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1152 ms on 172.18.0.6 (executor 0) (1/1)
25/04/15 16:10:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 16:10:46 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.241 s
25/04/15 16:10:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 16:10:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 16:10:46 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.273195 s
25/04/15 16:10:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:35007 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 16:10:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.6:38239 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 16:10:48 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 16:10:48 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 16:10:48 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 16:10:48 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 16:10:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 16:10:48 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 16:10:48 INFO metastore: Connected to metastore.
25/04/15 16:10:48 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 16:10:48 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=82546371-ed6a-4294-8dc0-477148cb94a9, clientType=HIVECLI]
25/04/15 16:10:48 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 16:10:48 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 16:10:48 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 16:10:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 16:10:48 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 16:10:48 INFO metastore: Connected to metastore.
25/04/15 16:10:48 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 16:10:48 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 16:10:48 INFO metastore: Connected to metastore.
25/04/15 16:10:49 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 16:10:49 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 16:10:49 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 16:10:49 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 16:10:49 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 16:10:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 16:10:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 16:10:49 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 16:10:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 16:10:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 16:10:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 16:10:49 INFO CodeGenerator: Code generated in 156.270139 ms
25/04/15 16:10:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 16:10:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 16:10:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:35007 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 16:10:49 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 16:10:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 16:10:49 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 16:10:49 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 16:10:49 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 16:10:49 INFO DAGScheduler: Parents of final stage: List()
25/04/15 16:10:49 INFO DAGScheduler: Missing parents: List()
25/04/15 16:10:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 16:10:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 16:10:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 16:10:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:35007 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 16:10:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 16:10:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 16:10:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 16:10:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.10, executor 2, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 16:10:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.10:45461 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 16:10:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.10:45461 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 16:10:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1960 ms on 172.18.0.10 (executor 2) (1/1)
25/04/15 16:10:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 16:10:51 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.985 s
25/04/15 16:10:51 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 16:10:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 16:10:51 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.991344 s
25/04/15 16:10:51 INFO FileFormatWriter: Start to commit write Job ead94f26-1f05-43fc-8dc4-5f414a52a688.
25/04/15 16:10:51 INFO FileFormatWriter: Write Job ead94f26-1f05-43fc-8dc4-5f414a52a688 committed. Elapsed time: 35 ms.
25/04/15 16:10:51 INFO FileFormatWriter: Finished processing stats for write job ead94f26-1f05-43fc-8dc4-5f414a52a688.
25/04/15 16:10:51 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/15 16:10:51 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 16:10:51 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 16:10:51 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 16:10:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 16:10:51 INFO MemoryStore: MemoryStore cleared
25/04/15 16:10:51 INFO BlockManager: BlockManager stopped
25/04/15 16:10:51 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 16:10:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 16:10:51 INFO SparkContext: Successfully stopped SparkContext
25/04/15 16:10:51 INFO ShutdownHookManager: Shutdown hook called
25/04/15 16:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a096974-3bab-43c0-ba8f-ff40ad06fdd7/pyspark-b34a5085-c779-46c7-9c3c-c167e3aad4ad
25/04/15 16:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a096974-3bab-43c0-ba8f-ff40ad06fdd7
25/04/15 16:10:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-e96cf453-0fc7-4805-8a43-c8da448ebac0
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/15 20:10:45 INFO SparkContext: Running Spark version 3.2.2
25/04/15 20:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/15 20:10:45 INFO ResourceUtils: ==============================================================
25/04/15 20:10:45 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/15 20:10:45 INFO ResourceUtils: ==============================================================
25/04/15 20:10:45 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/15 20:10:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/15 20:10:45 INFO ResourceProfile: Limiting resource is cpu
25/04/15 20:10:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/15 20:10:45 INFO SecurityManager: Changing view acls to: root
25/04/15 20:10:45 INFO SecurityManager: Changing modify acls to: root
25/04/15 20:10:45 INFO SecurityManager: Changing view acls groups to: 
25/04/15 20:10:45 INFO SecurityManager: Changing modify acls groups to: 
25/04/15 20:10:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/15 20:10:45 INFO Utils: Successfully started service 'sparkDriver' on port 45725.
25/04/15 20:10:45 INFO SparkEnv: Registering MapOutputTracker
25/04/15 20:10:45 INFO SparkEnv: Registering BlockManagerMaster
25/04/15 20:10:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/15 20:10:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/15 20:10:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/15 20:10:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c155ad06-5270-4622-92ea-e26306b34d77
25/04/15 20:10:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/15 20:10:45 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/15 20:10:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/15 20:10:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/15 20:10:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/15 20:10:45 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 24 ms (0 ms spent in bootstraps)
25/04/15 20:10:46 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250415201046-0035
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415201046-0035/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/15 20:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415201046-0035/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415201046-0035/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/15 20:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415201046-0035/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250415201046-0035/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/15 20:10:46 INFO StandaloneSchedulerBackend: Granted executor ID app-20250415201046-0035/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/15 20:10:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32865.
25/04/15 20:10:46 INFO NettyBlockTransferService: Server created on 32988ccd198e:32865
25/04/15 20:10:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/15 20:10:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 32865, None)
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415201046-0035/1 is now RUNNING
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415201046-0035/0 is now RUNNING
25/04/15 20:10:46 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:32865 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 32865, None)
25/04/15 20:10:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250415201046-0035/2 is now RUNNING
25/04/15 20:10:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 32865, None)
25/04/15 20:10:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 32865, None)
25/04/15 20:10:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/15 20:10:46 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/15 20:10:46 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/15 20:10:47 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/15 20:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:52024) with ID 2,  ResourceProfileId 0
25/04/15 20:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:36096) with ID 1,  ResourceProfileId 0
25/04/15 20:10:47 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:39618) with ID 0,  ResourceProfileId 0
25/04/15 20:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:34393 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 34393, None)
25/04/15 20:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:38537 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 38537, None)
25/04/15 20:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:41923 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 41923, None)
25/04/15 20:10:48 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/15 20:10:48 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 20:10:48 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/15 20:10:48 INFO DAGScheduler: Parents of final stage: List()
25/04/15 20:10:48 INFO DAGScheduler: Missing parents: List()
25/04/15 20:10:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 20:10:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/15 20:10:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/15 20:10:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:32865 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 20:10:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/15 20:10:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 20:10:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/15 20:10:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 1, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/15 20:10:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:34393 (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 20:10:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1170 ms on 172.18.0.9 (executor 1) (1/1)
25/04/15 20:10:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/15 20:10:49 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.274 s
25/04/15 20:10:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 20:10:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/15 20:10:49 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.308568 s
25/04/15 20:10:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:32865 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 20:10:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:34393 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/15 20:10:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 20:10:50 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/15 20:10:50 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/15 20:10:50 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/15 20:10:50 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 20:10:50 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 20:10:50 INFO metastore: Connected to metastore.
25/04/15 20:10:51 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
25/04/15 20:10:51 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1c4f281c-cf1c-49f1-80ac-0f1cc88c69b1, clientType=HIVECLI]
25/04/15 20:10:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/15 20:10:51 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/15 20:10:51 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/15 20:10:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 20:10:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/15 20:10:51 INFO metastore: Connected to metastore.
25/04/15 20:10:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/15 20:10:51 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/15 20:10:51 INFO metastore: Connected to metastore.
25/04/15 20:10:51 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/15 20:10:51 INFO FileSourceStrategy: Pushed Filters: 
25/04/15 20:10:51 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/15 20:10:51 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/15 20:10:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 20:10:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 20:10:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 20:10:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 20:10:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/15 20:10:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/15 20:10:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/15 20:10:52 INFO CodeGenerator: Code generated in 154.482988 ms
25/04/15 20:10:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/15 20:10:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/15 20:10:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:32865 (size: 33.8 KiB, free: 366.3 MiB)
25/04/15 20:10:52 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/15 20:10:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/15 20:10:52 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/15 20:10:52 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/15 20:10:52 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/15 20:10:52 INFO DAGScheduler: Parents of final stage: List()
25/04/15 20:10:52 INFO DAGScheduler: Missing parents: List()
25/04/15 20:10:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/15 20:10:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/15 20:10:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/15 20:10:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:32865 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 20:10:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/15 20:10:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/15 20:10:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/15 20:10:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 1, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/15 20:10:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.9:34393 (size: 74.7 KiB, free: 366.2 MiB)
25/04/15 20:10:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:34393 (size: 33.8 KiB, free: 366.2 MiB)
25/04/15 20:10:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 992 ms on 172.18.0.9 (executor 1) (1/1)
25/04/15 20:10:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/15 20:10:53 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 1.020 s
25/04/15 20:10:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/15 20:10:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/15 20:10:53 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 1.026061 s
25/04/15 20:10:53 INFO FileFormatWriter: Start to commit write Job d352279a-0dfe-42dd-9cd6-428c4a8967a9.
25/04/15 20:10:53 INFO FileFormatWriter: Write Job d352279a-0dfe-42dd-9cd6-428c4a8967a9 committed. Elapsed time: 38 ms.
25/04/15 20:10:53 INFO FileFormatWriter: Finished processing stats for write job d352279a-0dfe-42dd-9cd6-428c4a8967a9.
25/04/15 20:10:53 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/04/15 20:10:53 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/15 20:10:53 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/15 20:10:53 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/15 20:10:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/15 20:10:53 INFO MemoryStore: MemoryStore cleared
25/04/15 20:10:53 INFO BlockManager: BlockManager stopped
25/04/15 20:10:53 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/15 20:10:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/15 20:10:53 INFO SparkContext: Successfully stopped SparkContext
25/04/15 20:10:53 INFO ShutdownHookManager: Shutdown hook called
25/04/15 20:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a3dd9a2-1827-4eed-adb6-a90d4aa449d1/pyspark-464483d8-5ea7-4326-8a58-c56cb3f241a3
25/04/15 20:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a3dd9a2-1827-4eed-adb6-a90d4aa449d1
25/04/15 20:10:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-75e6f7df-a4b9-4c9f-b990-4f6eb7467353
Spark job completed successfully.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/16 00:10:46 INFO SparkContext: Running Spark version 3.2.2
25/04/16 00:10:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/16 00:10:46 INFO ResourceUtils: ==============================================================
25/04/16 00:10:46 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/16 00:10:46 INFO ResourceUtils: ==============================================================
25/04/16 00:10:46 INFO SparkContext: Submitted application: Load unprocessed logs data into Hive
25/04/16 00:10:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/16 00:10:46 INFO ResourceProfile: Limiting resource is cpu
25/04/16 00:10:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/16 00:10:46 INFO SecurityManager: Changing view acls to: root
25/04/16 00:10:46 INFO SecurityManager: Changing modify acls to: root
25/04/16 00:10:46 INFO SecurityManager: Changing view acls groups to: 
25/04/16 00:10:46 INFO SecurityManager: Changing modify acls groups to: 
25/04/16 00:10:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/16 00:10:46 INFO Utils: Successfully started service 'sparkDriver' on port 40433.
25/04/16 00:10:46 INFO SparkEnv: Registering MapOutputTracker
25/04/16 00:10:46 INFO SparkEnv: Registering BlockManagerMaster
25/04/16 00:10:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/16 00:10:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/16 00:10:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/16 00:10:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c13100bb-858b-4cec-abe1-76e709f0b230
25/04/16 00:10:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/16 00:10:46 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/16 00:10:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/16 00:10:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://32988ccd198e:4040
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/16 00:10:47 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.13:7077 after 23 ms (0 ms spent in bootstraps)
25/04/16 00:10:47 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250416001047-0040
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250416001047-0040/0 on worker-20250414100905-172.18.0.6-44685 (172.18.0.6:44685) with 4 core(s)
25/04/16 00:10:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250416001047-0040/0 on hostPort 172.18.0.6:44685 with 4 core(s), 1024.0 MiB RAM
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250416001047-0040/1 on worker-20250414100905-172.18.0.9-43923 (172.18.0.9:43923) with 4 core(s)
25/04/16 00:10:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250416001047-0040/1 on hostPort 172.18.0.9:43923 with 4 core(s), 1024.0 MiB RAM
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250416001047-0040/2 on worker-20250414100905-172.18.0.10-33907 (172.18.0.10:33907) with 4 core(s)
25/04/16 00:10:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20250416001047-0040/2 on hostPort 172.18.0.10:33907 with 4 core(s), 1024.0 MiB RAM
25/04/16 00:10:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34739.
25/04/16 00:10:47 INFO NettyBlockTransferService: Server created on 32988ccd198e:34739
25/04/16 00:10:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/16 00:10:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32988ccd198e, 34739, None)
25/04/16 00:10:47 INFO BlockManagerMasterEndpoint: Registering block manager 32988ccd198e:34739 with 366.3 MiB RAM, BlockManagerId(driver, 32988ccd198e, 34739, None)
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250416001047-0040/0 is now RUNNING
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250416001047-0040/1 is now RUNNING
25/04/16 00:10:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32988ccd198e, 34739, None)
25/04/16 00:10:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250416001047-0040/2 is now RUNNING
25/04/16 00:10:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32988ccd198e, 34739, None)
25/04/16 00:10:47 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/16 00:10:47 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/16 00:10:47 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/16 00:10:48 INFO InMemoryFileIndex: It took 66 ms to list leaf files for 1 paths.
25/04/16 00:10:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:54522) with ID 0,  ResourceProfileId 0
25/04/16 00:10:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.10:36936) with ID 2,  ResourceProfileId 0
25/04/16 00:10:48 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:56606) with ID 1,  ResourceProfileId 0
25/04/16 00:10:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:34317 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.6, 34317, None)
25/04/16 00:10:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:46323 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.9, 46323, None)
25/04/16 00:10:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.10:45993 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.10, 45993, None)
25/04/16 00:10:49 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
25/04/16 00:10:49 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/16 00:10:49 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
25/04/16 00:10:49 INFO DAGScheduler: Parents of final stage: List()
25/04/16 00:10:49 INFO DAGScheduler: Missing parents: List()
25/04/16 00:10:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/16 00:10:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 100.3 KiB, free 366.2 MiB)
25/04/16 00:10:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.7 KiB, free 366.2 MiB)
25/04/16 00:10:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32988ccd198e:34739 (size: 35.7 KiB, free: 366.3 MiB)
25/04/16 00:10:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
25/04/16 00:10:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/16 00:10:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/16 00:10:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.10, executor 2, partition 0, PROCESS_LOCAL, 4655 bytes) taskResourceAssignments Map()
25/04/16 00:10:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.10:45993 (size: 35.7 KiB, free: 366.3 MiB)
25/04/16 00:10:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1145 ms on 172.18.0.10 (executor 2) (1/1)
25/04/16 00:10:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/16 00:10:50 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.240 s
25/04/16 00:10:50 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/16 00:10:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/16 00:10:50 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.275470 s
25/04/16 00:10:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32988ccd198e:34739 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/16 00:10:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.10:45993 in memory (size: 35.7 KiB, free: 366.3 MiB)
25/04/16 00:10:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/16 00:10:51 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/16 00:10:51 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/16 00:10:51 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/16 00:10:51 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/16 00:10:51 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/16 00:10:51 INFO metastore: Connected to metastore.
25/04/16 00:10:52 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
25/04/16 00:10:52 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=cde916ba-3b81-4cbd-94c5-93a9865121a6, clientType=HIVECLI]
25/04/16 00:10:52 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/16 00:10:52 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/16 00:10:52 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/16 00:10:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/16 00:10:52 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/16 00:10:52 INFO metastore: Connected to metastore.
25/04/16 00:10:52 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/16 00:10:52 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/16 00:10:52 INFO metastore: Connected to metastore.
25/04/16 00:10:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
25/04/16 00:10:52 INFO FileSourceStrategy: Pushed Filters: 
25/04/16 00:10:52 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/16 00:10:52 INFO FileSourceStrategy: Output Data Schema: struct<InvoiceNo: string, StockCode: string, Quantity: int, CustomerID: string, Country: string ... 1 more field>
25/04/16 00:10:52 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/16 00:10:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/16 00:10:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/16 00:10:52 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/16 00:10:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/16 00:10:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/16 00:10:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/16 00:10:53 INFO CodeGenerator: Code generated in 154.513144 ms
25/04/16 00:10:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 350.7 KiB, free 366.0 MiB)
25/04/16 00:10:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.9 MiB)
25/04/16 00:10:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32988ccd198e:34739 (size: 33.8 KiB, free: 366.3 MiB)
25/04/16 00:10:53 INFO SparkContext: Created broadcast 1 from insertInto at NativeMethodAccessorImpl.java:0
25/04/16 00:10:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/16 00:10:53 INFO SparkContext: Starting job: insertInto at NativeMethodAccessorImpl.java:0
25/04/16 00:10:53 INFO DAGScheduler: Got job 1 (insertInto at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/16 00:10:53 INFO DAGScheduler: Final stage: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0)
25/04/16 00:10:53 INFO DAGScheduler: Parents of final stage: List()
25/04/16 00:10:53 INFO DAGScheduler: Missing parents: List()
25/04/16 00:10:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/16 00:10:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.3 KiB, free 365.7 MiB)
25/04/16 00:10:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 74.7 KiB, free 365.6 MiB)
25/04/16 00:10:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 32988ccd198e:34739 (size: 74.7 KiB, free: 366.2 MiB)
25/04/16 00:10:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478
25/04/16 00:10:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at insertInto at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/16 00:10:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/04/16 00:10:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6, executor 0, partition 0, ANY, 4932 bytes) taskResourceAssignments Map()
25/04/16 00:10:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.6:34317 (size: 74.7 KiB, free: 366.2 MiB)
25/04/16 00:10:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.6:34317 (size: 33.8 KiB, free: 366.2 MiB)
25/04/16 00:10:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1976 ms on 172.18.0.6 (executor 0) (1/1)
25/04/16 00:10:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/16 00:10:55 INFO DAGScheduler: ResultStage 1 (insertInto at NativeMethodAccessorImpl.java:0) finished in 2.002 s
25/04/16 00:10:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/16 00:10:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/16 00:10:55 INFO DAGScheduler: Job 1 finished: insertInto at NativeMethodAccessorImpl.java:0, took 2.009556 s
25/04/16 00:10:55 INFO FileFormatWriter: Start to commit write Job 8d4e2e1c-5c19-4af6-8907-248ae5fc0273.
25/04/16 00:10:55 INFO FileFormatWriter: Write Job 8d4e2e1c-5c19-4af6-8907-248ae5fc0273 committed. Elapsed time: 37 ms.
25/04/16 00:10:55 INFO FileFormatWriter: Finished processing stats for write job 8d4e2e1c-5c19-4af6-8907-248ae5fc0273.
25/04/16 00:10:55 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
25/04/16 00:10:55 INFO SparkUI: Stopped Spark web UI at http://32988ccd198e:4040
25/04/16 00:10:55 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/16 00:10:55 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/16 00:10:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/16 00:10:55 INFO MemoryStore: MemoryStore cleared
25/04/16 00:10:55 INFO BlockManager: BlockManager stopped
25/04/16 00:10:55 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/16 00:10:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/16 00:10:55 INFO SparkContext: Successfully stopped SparkContext
25/04/16 00:10:55 INFO ShutdownHookManager: Shutdown hook called
25/04/16 00:10:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-967196ce-a726-4587-b6d5-4079fa862028/pyspark-924a4d59-0236-4796-ae63-c25747023159
25/04/16 00:10:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-3c7d2257-4d8f-45ee-8a10-4158b3ceaed9
25/04/16 00:10:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-967196ce-a726-4587-b6d5-4079fa862028
Spark job completed successfully.
