Started by timer
Running as SYSTEM
Building in workspace /var/jenkins_home/workspace/LoadCountriesIntoHive
[SSH] script:
PATH="/opt/java/openjdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
JAVA_HOME="/opt/java/openjdk"
HOME="/root"

echo "Starting Spark job..." | tee -a spark_job.log

LOG_FILE=/opt/bitnami/spark/logs/load_countries_into_hdfs.log

export JAVA_HOME=/opt/bitnami/java
export PATH=$JAVA_HOME/bin:$PATH
export PATH=$PATH:/opt/bitnami/python/bin
export PYSPARK_PYTHON=/opt/bitnami/python/bin/python3
export CORE_CONF_fs_defaultFS=hdfs://namenode:9000

/opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 /opt/bitnami/spark/jobs/load_countries_into_hive.py 2>&1 | tee -a "$LOG_FILE"

exit_code=${PIPESTATUS[0]}

if [ $exit_code -ne 0 ]; then
  echo "Spark job FAILED!" | tee -a "$LOG_FILE"
  exit 1
else
  echo "Spark job completed successfully." | tee -a "$LOG_FILE"
fi


[SSH] executing...
Starting Spark job...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/04/11 07:04:23 INFO SparkContext: Running Spark version 3.2.2
25/04/11 07:04:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/11 07:04:23 INFO ResourceUtils: ==============================================================
25/04/11 07:04:23 INFO ResourceUtils: No custom resources configured for spark.driver.
25/04/11 07:04:23 INFO ResourceUtils: ==============================================================
25/04/11 07:04:23 INFO SparkContext: Submitted application: Load countries data into Hive
25/04/11 07:04:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/04/11 07:04:23 INFO ResourceProfile: Limiting resource is cpu
25/04/11 07:04:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/04/11 07:04:23 INFO SecurityManager: Changing view acls to: root
25/04/11 07:04:23 INFO SecurityManager: Changing modify acls to: root
25/04/11 07:04:23 INFO SecurityManager: Changing view acls groups to: 
25/04/11 07:04:23 INFO SecurityManager: Changing modify acls groups to: 
25/04/11 07:04:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
25/04/11 07:04:23 INFO Utils: Successfully started service 'sparkDriver' on port 46579.
25/04/11 07:04:23 INFO SparkEnv: Registering MapOutputTracker
25/04/11 07:04:23 INFO SparkEnv: Registering BlockManagerMaster
25/04/11 07:04:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/04/11 07:04:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/04/11 07:04:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/04/11 07:04:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f1228f2-43a1-42eb-836e-9c14d227177e
25/04/11 07:04:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
25/04/11 07:04:24 INFO SparkEnv: Registering OutputCommitCoordinator
25/04/11 07:04:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/04/11 07:04:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://0c970a188c9a:4040
25/04/11 07:04:24 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/04/11 07:04:24 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.14:7077 after 32 ms (0 ms spent in bootstraps)
25/04/11 07:04:24 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250411070424-0000
25/04/11 07:04:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42863.
25/04/11 07:04:24 INFO NettyBlockTransferService: Server created on 0c970a188c9a:42863
25/04/11 07:04:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/04/11 07:04:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0c970a188c9a, 42863, None)
25/04/11 07:04:24 INFO BlockManagerMasterEndpoint: Registering block manager 0c970a188c9a:42863 with 366.3 MiB RAM, BlockManagerId(driver, 0c970a188c9a, 42863, None)
25/04/11 07:04:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0c970a188c9a, 42863, None)
25/04/11 07:04:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0c970a188c9a, 42863, None)
25/04/11 07:04:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250411070424-0000/0 on worker-20250411070409-172.18.0.11-34239 (172.18.0.11:34239) with 4 core(s)
25/04/11 07:04:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250411070424-0000/0 on hostPort 172.18.0.11:34239 with 4 core(s), 1024.0 MiB RAM
25/04/11 07:04:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250411070424-0000/1 on worker-20250411070409-172.18.0.12-43825 (172.18.0.12:43825) with 4 core(s)
25/04/11 07:04:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250411070424-0000/1 on hostPort 172.18.0.12:43825 with 4 core(s), 1024.0 MiB RAM
25/04/11 07:04:24 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250411070424-0000/2 on worker-20250411070409-172.18.0.9-38161 (172.18.0.9:38161) with 4 core(s)
25/04/11 07:04:24 INFO StandaloneSchedulerBackend: Granted executor ID app-20250411070424-0000/2 on hostPort 172.18.0.9:38161 with 4 core(s), 1024.0 MiB RAM
25/04/11 07:04:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250411070424-0000/0 is now RUNNING
25/04/11 07:04:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250411070424-0000/1 is now RUNNING
25/04/11 07:04:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250411070424-0000/2 is now RUNNING
25/04/11 07:04:25 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/04/11 07:04:25 INFO SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir.
25/04/11 07:04:25 INFO SharedState: Warehouse path is 'file:/user/hive/warehouse'.
25/04/11 07:04:28 INFO InMemoryFileIndex: It took 145 ms to list leaf files for 1 paths.
25/04/11 07:04:28 INFO InMemoryFileIndex: It took 18 ms to list leaf files for 4 paths.
25/04/11 07:04:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.12:46930) with ID 1,  ResourceProfileId 0
25/04/11 07:04:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.11:40816) with ID 0,  ResourceProfileId 0
25/04/11 07:04:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.12:34117 with 366.3 MiB RAM, BlockManagerId(1, 172.18.0.12, 34117, None)
25/04/11 07:04:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.11:39247 with 366.3 MiB RAM, BlockManagerId(0, 172.18.0.11, 39247, None)
25/04/11 07:04:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:45968) with ID 2,  ResourceProfileId 0
25/04/11 07:04:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:41401 with 366.3 MiB RAM, BlockManagerId(2, 172.18.0.9, 41401, None)
25/04/11 07:04:30 INFO FileSourceStrategy: Pushed Filters: 
25/04/11 07:04:30 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
25/04/11 07:04:30 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/04/11 07:04:31 INFO CodeGenerator: Code generated in 239.812074 ms
25/04/11 07:04:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.4 KiB, free 366.0 MiB)
25/04/11 07:04:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
25/04/11 07:04:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0c970a188c9a:42863 (size: 32.6 KiB, free: 366.3 MiB)
25/04/11 07:04:31 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
25/04/11 07:04:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/11 07:04:31 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
25/04/11 07:04:31 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/11 07:04:31 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
25/04/11 07:04:31 INFO DAGScheduler: Parents of final stage: List()
25/04/11 07:04:31 INFO DAGScheduler: Missing parents: List()
25/04/11 07:04:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/11 07:04:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.6 KiB, free 365.9 MiB)
25/04/11 07:04:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 365.9 MiB)
25/04/11 07:04:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0c970a188c9a:42863 (size: 5.8 KiB, free: 366.3 MiB)
25/04/11 07:04:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478
25/04/11 07:04:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/11 07:04:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
25/04/11 07:04:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.11, executor 0, partition 0, ANY, 4898 bytes) taskResourceAssignments Map()
25/04/11 07:04:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.11:39247 (size: 5.8 KiB, free: 366.3 MiB)
25/04/11 07:04:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.11:39247 (size: 32.6 KiB, free: 366.3 MiB)
25/04/11 07:04:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2330 ms on 172.18.0.11 (executor 0) (1/1)
25/04/11 07:04:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/04/11 07:04:34 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.490 s
25/04/11 07:04:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/11 07:04:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/04/11 07:04:34 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.549542 s
25/04/11 07:04:34 INFO CodeGenerator: Code generated in 24.89814 ms
25/04/11 07:04:34 INFO FileSourceStrategy: Pushed Filters: 
25/04/11 07:04:34 INFO FileSourceStrategy: Post-Scan Filters: 
25/04/11 07:04:34 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
25/04/11 07:04:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 345.4 KiB, free 365.6 MiB)
25/04/11 07:04:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.5 MiB)
25/04/11 07:04:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0c970a188c9a:42863 (size: 32.6 KiB, free: 366.2 MiB)
25/04/11 07:04:34 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
25/04/11 07:04:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/11 07:04:34 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/11 07:04:34 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/04/11 07:04:35 INFO HiveConf: Found configuration file file:/opt/bitnami/spark/conf/hive-site.xml
25/04/11 07:04:35 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/user/hive/warehouse
25/04/11 07:04:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/11 07:04:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/11 07:04:35 INFO metastore: Connected to metastore.
25/04/11 07:04:35 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=133e1d9a-69e9-4a6a-ab42-d006a2039d93, clientType=HIVECLI]
25/04/11 07:04:35 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
25/04/11 07:04:35 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
25/04/11 07:04:35 INFO metastore: Closed a connection to metastore, current connections: 0
25/04/11 07:04:35 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/11 07:04:35 INFO metastore: Opened a connection to metastore, current connections: 1
25/04/11 07:04:35 INFO metastore: Connected to metastore.
25/04/11 07:04:36 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
25/04/11 07:04:36 INFO metastore: Opened a connection to metastore, current connections: 2
25/04/11 07:04:36 INFO metastore: Connected to metastore.
25/04/11 07:04:36 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
25/04/11 07:04:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID)
25/04/11 07:04:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#16),isnotnull(cast(CountryID#16 as int))
25/04/11 07:04:36 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/04/11 07:04:36 INFO FileSourceStrategy: Pushed Filters: IsNotNull(countryid),IsNotNull(countryname)
25/04/11 07:04:36 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(countryid#24),isnotnull(countryname#25)
25/04/11 07:04:36 INFO FileSourceStrategy: Output Data Schema: struct<countryid: int, countryname: string>
25/04/11 07:04:36 INFO CodeGenerator: Code generated in 41.657747 ms
25/04/11 07:04:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 349.9 KiB, free 365.2 MiB)
25/04/11 07:04:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 33.5 KiB, free 365.2 MiB)
25/04/11 07:04:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0c970a188c9a:42863 (size: 33.5 KiB, free: 366.2 MiB)
25/04/11 07:04:37 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/04/11 07:04:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/11 07:04:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0c970a188c9a:42863 in memory (size: 5.8 KiB, free: 366.2 MiB)
25/04/11 07:04:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.11:39247 in memory (size: 5.8 KiB, free: 366.3 MiB)
25/04/11 07:04:37 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/04/11 07:04:37 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 2 output partitions
25/04/11 07:04:37 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)
25/04/11 07:04:37 INFO DAGScheduler: Parents of final stage: List()
25/04/11 07:04:37 INFO DAGScheduler: Missing parents: List()
25/04/11 07:04:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents
25/04/11 07:04:37 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.5 KiB, free 365.2 MiB)
25/04/11 07:04:37 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.2 MiB)
25/04/11 07:04:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 0c970a188c9a:42863 (size: 6.0 KiB, free: 366.2 MiB)
25/04/11 07:04:37 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1478
25/04/11 07:04:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1))
25/04/11 07:04:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
25/04/11 07:04:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 2, partition 0, ANY, 4947 bytes) taskResourceAssignments Map()
25/04/11 07:04:37 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.11, executor 0, partition 1, ANY, 4947 bytes) taskResourceAssignments Map()
25/04/11 07:04:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.11:39247 (size: 6.0 KiB, free: 366.3 MiB)
25/04/11 07:04:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.11:39247 (size: 33.5 KiB, free: 366.2 MiB)
25/04/11 07:04:37 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.9:41401 (size: 6.0 KiB, free: 366.3 MiB)
25/04/11 07:04:38 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 995 ms on 172.18.0.11 (executor 0) (1/2)
25/04/11 07:04:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.9:41401 (size: 33.5 KiB, free: 366.3 MiB)
25/04/11 07:04:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2565 ms on 172.18.0.9 (executor 2) (2/2)
25/04/11 07:04:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/04/11 07:04:39 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) finished in 2.576 s
25/04/11 07:04:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/11 07:04:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/04/11 07:04:39 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266, took 2.584383 s
25/04/11 07:04:39 INFO CodeGenerator: Code generated in 8.309927 ms
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 1026.0 KiB, free 364.2 MiB)
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1697.0 B, free 364.2 MiB)
25/04/11 07:04:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 0c970a188c9a:42863 (size: 1697.0 B, free: 366.2 MiB)
25/04/11 07:04:39 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266
25/04/11 07:04:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID)
25/04/11 07:04:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#16),isnotnull(cast(CountryID#16 as int))
25/04/11 07:04:39 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/04/11 07:04:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/04/11 07:04:39 INFO CodeGenerator: Code generated in 26.006113 ms
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 345.3 KiB, free 363.8 MiB)
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 363.8 MiB)
25/04/11 07:04:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 0c970a188c9a:42863 (size: 32.6 KiB, free: 366.2 MiB)
25/04/11 07:04:39 INFO SparkContext: Created broadcast 6 from count at NativeMethodAccessorImpl.java:0
25/04/11 07:04:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/04/11 07:04:39 INFO DAGScheduler: Registering RDD 17 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
25/04/11 07:04:39 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
25/04/11 07:04:39 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
25/04/11 07:04:39 INFO DAGScheduler: Parents of final stage: List()
25/04/11 07:04:39 INFO DAGScheduler: Missing parents: List()
25/04/11 07:04:39 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 40.8 KiB, free 363.8 MiB)
25/04/11 07:04:39 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 363.7 MiB)
25/04/11 07:04:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 0c970a188c9a:42863 (size: 19.2 KiB, free: 366.1 MiB)
25/04/11 07:04:39 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478
25/04/11 07:04:39 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[17] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
25/04/11 07:04:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0
25/04/11 07:04:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (172.18.0.11, executor 0, partition 0, ANY, 4887 bytes) taskResourceAssignments Map()
25/04/11 07:04:39 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4) (172.18.0.12, executor 1, partition 1, ANY, 4887 bytes) taskResourceAssignments Map()
25/04/11 07:04:39 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5) (172.18.0.9, executor 2, partition 2, ANY, 4887 bytes) taskResourceAssignments Map()
25/04/11 07:04:39 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6) (172.18.0.11, executor 0, partition 3, ANY, 4887 bytes) taskResourceAssignments Map()
25/04/11 07:04:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.11:39247 (size: 19.2 KiB, free: 366.2 MiB)
25/04/11 07:04:39 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.9:41401 (size: 19.2 KiB, free: 366.2 MiB)
25/04/11 07:04:40 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.12:34117 (size: 19.2 KiB, free: 366.3 MiB)
25/04/11 07:04:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.11:39247 (size: 1697.0 B, free: 366.2 MiB)
25/04/11 07:04:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.9:41401 (size: 1697.0 B, free: 366.2 MiB)
25/04/11 07:04:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.11:39247 (size: 32.6 KiB, free: 366.2 MiB)
25/04/11 07:04:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.9:41401 (size: 32.6 KiB, free: 366.2 MiB)
25/04/11 07:04:40 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 733 ms on 172.18.0.11 (executor 0) (1/4)
25/04/11 07:04:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 776 ms on 172.18.0.11 (executor 0) (2/4)
25/04/11 07:04:40 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 845 ms on 172.18.0.9 (executor 2) (3/4)
25/04/11 07:04:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.12:34117 (size: 1697.0 B, free: 366.3 MiB)
25/04/11 07:04:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.12:34117 (size: 32.6 KiB, free: 366.2 MiB)
25/04/11 07:04:42 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 2570 ms on 172.18.0.12 (executor 1) (4/4)
25/04/11 07:04:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/04/11 07:04:42 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 2.585 s
25/04/11 07:04:42 INFO DAGScheduler: looking for newly runnable stages
25/04/11 07:04:42 INFO DAGScheduler: running: Set()
25/04/11 07:04:42 INFO DAGScheduler: waiting: Set()
25/04/11 07:04:42 INFO DAGScheduler: failed: Set()
25/04/11 07:04:42 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/04/11 07:04:42 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/04/11 07:04:42 INFO CodeGenerator: Code generated in 14.282804 ms
25/04/11 07:04:42 INFO DAGScheduler: Registering RDD 20 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
25/04/11 07:04:42 INFO DAGScheduler: Got map stage job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/11 07:04:42 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
25/04/11 07:04:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
25/04/11 07:04:42 INFO DAGScheduler: Missing parents: List()
25/04/11 07:04:42 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/11 07:04:42 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 45.6 KiB, free 363.7 MiB)
25/04/11 07:04:42 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 21.5 KiB, free 363.7 MiB)
25/04/11 07:04:42 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 0c970a188c9a:42863 (size: 21.5 KiB, free: 366.1 MiB)
25/04/11 07:04:42 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478
25/04/11 07:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/11 07:04:42 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
25/04/11 07:04:42 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (172.18.0.9, executor 2, partition 0, NODE_LOCAL, 4446 bytes) taskResourceAssignments Map()
25/04/11 07:04:42 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.9:41401 (size: 21.5 KiB, free: 366.2 MiB)
25/04/11 07:04:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.9:45968
25/04/11 07:04:42 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 169 ms on 172.18.0.9 (executor 2) (1/1)
25/04/11 07:04:42 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
25/04/11 07:04:42 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.178 s
25/04/11 07:04:42 INFO DAGScheduler: looking for newly runnable stages
25/04/11 07:04:42 INFO DAGScheduler: running: Set()
25/04/11 07:04:42 INFO DAGScheduler: waiting: Set()
25/04/11 07:04:42 INFO DAGScheduler: failed: Set()
25/04/11 07:04:42 INFO CodeGenerator: Code generated in 7.60166 ms
25/04/11 07:04:42 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
25/04/11 07:04:42 INFO DAGScheduler: Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
25/04/11 07:04:42 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
25/04/11 07:04:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
25/04/11 07:04:42 INFO DAGScheduler: Missing parents: List()
25/04/11 07:04:42 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
25/04/11 07:04:42 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.0 KiB, free 363.7 MiB)
25/04/11 07:04:42 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 363.7 MiB)
25/04/11 07:04:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 0c970a188c9a:42863 (size: 5.5 KiB, free: 366.1 MiB)
25/04/11 07:04:42 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1478
25/04/11 07:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
25/04/11 07:04:42 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
25/04/11 07:04:42 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8) (172.18.0.9, executor 2, partition 0, NODE_LOCAL, 4457 bytes) taskResourceAssignments Map()
25/04/11 07:04:42 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.9:41401 (size: 5.5 KiB, free: 366.2 MiB)
25/04/11 07:04:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.9:45968
25/04/11 07:04:42 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 28 ms on 172.18.0.9 (executor 2) (1/1)
25/04/11 07:04:42 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
25/04/11 07:04:42 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.035 s
25/04/11 07:04:42 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
25/04/11 07:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
25/04/11 07:04:42 INFO DAGScheduler: Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.038519 s
25/04/11 07:04:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CountryID)
25/04/11 07:04:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CountryID#16),isnotnull(cast(CountryID#16 as int))
25/04/11 07:04:42 INFO FileSourceStrategy: Output Data Schema: struct<CountryID: string, CountryName: string>
25/04/11 07:04:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(countryid),IsNotNull(countryname)
25/04/11 07:04:42 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(countryid#24),isnotnull(countryname#25)
25/04/11 07:04:42 INFO FileSourceStrategy: Output Data Schema: struct<countryid: int, countryname: string>
25/04/11 07:04:42 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/11 07:04:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/11 07:04:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/11 07:04:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
25/04/11 07:04:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
25/04/11 07:04:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
25/04/11 07:04:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
Traceback (most recent call last):
  File "/opt/bitnami/spark/jobs/load_countries_into_hive.py", line 48, in <module>
    df.write.mode("append").insertInto(HIVE_TABLE)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 762, in insertInto
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o56.insertInto.
: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /user/hive/warehouse/countries/_temporary/0. Name node is in safe mode.
The reported blocks 2706 has reached the threshold 0.9990 of total blocks 2706. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:namenode
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2501)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2475)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1484)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1481)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1498)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1473)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2388)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:182)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:481)
	at org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:436)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /user/hive/warehouse/countries/_temporary/0. Name node is in safe mode.
The reported blocks 2706 has reached the threshold 0.9990 of total blocks 2706. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 1 seconds. NamenodeHostName:namenode
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1476)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1463)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3232)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1145)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:720)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1573)
	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
	at com.sun.proxy.$Proxy19.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:674)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy20.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2499)
	... 50 more

25/04/11 07:04:43 INFO SparkContext: Invoking stop() from shutdown hook
25/04/11 07:04:43 INFO SparkUI: Stopped Spark web UI at http://0c970a188c9a:4040
25/04/11 07:04:43 INFO StandaloneSchedulerBackend: Shutting down all executors
25/04/11 07:04:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
25/04/11 07:04:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/04/11 07:04:43 INFO MemoryStore: MemoryStore cleared
25/04/11 07:04:43 INFO BlockManager: BlockManager stopped
25/04/11 07:04:43 INFO BlockManagerMaster: BlockManagerMaster stopped
25/04/11 07:04:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/04/11 07:04:43 INFO SparkContext: Successfully stopped SparkContext
25/04/11 07:04:43 INFO ShutdownHookManager: Shutdown hook called
25/04/11 07:04:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ea2ac7a-bfa7-4ee2-93bf-df3ef685e934
25/04/11 07:04:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-929c7252-cec9-4dd4-9db7-1c602a0fe283
25/04/11 07:04:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-929c7252-cec9-4dd4-9db7-1c602a0fe283/pyspark-76543bac-13bf-41bb-86f7-64ce47ababe7
Spark job FAILED!

[SSH] completed
[SSH] exit-status: 1

Build step 'Execute shell script on remote host using ssh' marked build as failure
Sending e-mails to: nikolicmarko1243@gmail.com
Finished: FAILURE
